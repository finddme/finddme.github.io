---
title: Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020)
category: Natural Language Processing and Linguistics
tag: NLP & Linguistics
---

## 1. Introduction

본 연구에서는 영어의 장거리 의존문장을 여러 학습 모델에서 어떻게 처리하는지에 대해 실험한다. 장거리 의존 구문이란 자연어가 가지고 있는 sequential 한 속성과 더불어 hierarchical한 속성을 가지고 있다는 대표적인 예시이다. 영어에서 나타나는 대표적인 장거리 의존 구문은 WH-question, Relative clause, Topicalization이다. 장거리 의존 구문에 대한 자세한 설명은 [Long-Distance Dependency](https://finddme.github.io/linguistics(english)/2021/04/04/Longdistance/) 여기에 있다.

장거리 의존 문장과 같이 복잡한 구조를 가진 문장을 잘 파악하지 못하면 아래와 같은 오류를 낼 수 있기 때문에 자연어처리 분야에서의 장거리 의존 구문 처리는 매우 중요하다.

It was the lawyer(s) who I think you said (GAP) was/were upset.  
-> It was the lawyer who I think you said (GAP) was/*were upset.  
-> It was the lawyer(s) who I think you said (GAP) *was/were upset.  

따라서 본 연구에서는 딥러닝 모델들이 다양한 형태의 장거리 의존 구문을 잘 처리하는지 알아보고자 한다.

## 2. LSTM RNNs 

여기에서 사용되는 모델은 Gulordava model과 Google model이다. Gulordava model은 English Wikipedia corpus 9000만 token으로 학습되었고 650 unit hidden layer 두 개를 가진다. Google model은 10억개의 Word Benchmark(Chelba et al., 2013)로 학습되었고 8196 unit을 가진 두 개의 hidden layer로 이루어져 있다. 그리고 character level CNN의 output을 LSTM의 input으로 사용한다. 이 두 모델은 아래와 같은 두 가지 조건 하에 사용된다. 기본적인 실험 방식은 Wilcox et al.(2019b)를 기반으로 한다.

### 2.1 Experiment 1: agreement in clefts
첫 번째 조건은 cleft sentence(분열문)이다. 분열문은 한 문장이 두 부분으로 구성되어 있는 문장으로, it-that 강조구문을 생각하면 쉽다. 

<center><img width="251" alt="2021-04-04 (2)" src="https://user-images.githubusercontent.com/53667002/113701234-dabda800-9712-11eb-9f51-759d46d03e33.png"></center>

위 예문 a는 wh-관계절로 이루어진 문장으로, it was the lawyer부분과 삽입절 I think로 나뉠 수 있다. 위 예문 a, b, c, d는 순서대로 삽입절의 개수가 점점 추가되는 것을 확인할 수 있다. 이에 따라 내포 수준(LEVEL)도 증가하는 것도 볼 수 있다. 

본 실험에서는 2x2x4 factorial design(삽입절을 통해 강조되는 the lawyer(s)는 단수일 경우와 복수일 경우가 존재하며 이에 따라 동사의 경우도 2가지(단수/복수), 위 예문처럼 삽입절이 추가되는 4가지 문장)을 이용한 20 cleft item을 만들어 총 320문장을 가지고 실험을 진행한다. 

### 2.2 Experiment 2: agreement in indirect interrogatives

두 번째 조건은 indirect interrogative(간접의문문)이다. 이는 embedded question이라고도 하는데, 종속절로 기능하는 의문문이다.

<center><img width="218" alt="2021-04-04 (3)" src="https://user-images.githubusercontent.com/53667002/113701334-fd4fc100-9712-11eb-8291-294edcbaddc5.png"></center>

여기에서도 마찬가지로 2x2x4 factorial design, 20 cleft items, 320 sentences로 실험이 진행된다.

### 2.3 Result

두 조건에 대한 결과는 Surprisal로 제시된다. 본 실험에서는 수일치를 확인하고자 하기 때문에 RNN이 수일치에 성공했다면 Surprisal 수치가 낮을 것이고 틀렸다면 높을 것이다.

> **Surprisal(의외성/놀람)**<br>
Wilcox et al. (2018)에서는 log inverse probability라고 하며 아래와 같은 공식을 제시하였다. 
위 수식에서 x_i는 예측하고자 하는 단어이고 h_{i-1}은 x_i 이전의 hidden state 값이다. 따라서 맢선 맥락을 바탕으로 해당 단어에 대해 기대하는 정도를 surprisal로 나타낸다. 값이 낮을수록 의외가 아닌 것이고 높을수록 의외인 것이다. 
(공식)<br>
CoNLL에서 주로 사용되는 개념으로, Surprisal(의외성)은 사건이 일어나지 말아야하는 환경에서 나타날 경우에 나타나는 놀라움의 정도를 나타낸다. 예를 들어 초등학교 동창을 같은 동네에서 마주친 것보다 휴가지로 떠난 하와이에서 마주친 경우가 의외성의 수준이 더 높은 것이다. 

###  2.3.1 Experiment 1 result: agreement in clefts


<center><img width="250" alt="2021-04-05" src="https://user-images.githubusercontent.com/53667002/113701571-4acc2e00-9713-11eb-94e2-7d14aa7e7750.png"></center>

위 표에서 npl(빨간 박스)은 복수명사, nsg(초록 박스)는 단수명사를 나타낸다. 그리고 표의 x축에는 vpl(복수 동사)와 vsg(단수동사)가 나와있고 위에 제시된 숫자는 삽입절 내포 수준을 나타낸다. RNN이 제대로 학습했다면 명사가 복수일 때 동사가 복수여야 하기 때문에 초록색 박스보다 빨간색 박스의 수치가 낮게 나타나야 한다. 하지만 표를 보면 두 박스가 나타내는 수치의 차이가 거의 없다. google model에서는 삽입절이 많을수록 surprisal값이 높아지는 것을 볼 수 있다(예측을 잘 하지 못한다).

### 2.3.2 Experiment 2 result: agreement in indirect interrogatives

<center><img width="256" alt="2021-04-05 (1)" src="https://user-images.githubusercontent.com/53667002/113701674-69cac000-9713-11eb-8ca7-15d8b1ab9136.png"></center>

두 번째 실험의 결과도 첫 번째 실험처럼 전반적으로 겹침이 많은 것을 볼 수 있다. 따라서 Gulordava와 Google model은 장거리 의존 구문의 형태 통사적인 특징을 잘 학습하지 못한다는 것을 알 수 있다.

## 3. Transformer-XL

Transformer-XL은 기존 Transformer의 self-attention network에 ‘recurrence’개념을 도입한 모델이다. 기존 Transformer는 현재의 정보를 처리할 때 이전 정보가 단절되는데 Transformer-XL은  recurrence를 도입하여 이전 정보가 현재 정보에도 전파되도록 한다. Dai et al.(2019)에 따르면 RNN보다 80%더 긴 dependency를 학습할 수 있다고 한다. 이를 증명하기 위해 본 논문에서는 RNN으로 진행한 실험을 동일하게 진행하였다.

### 3.1 Experiment 1, 2 result: agreement in clefts, agreement in indirect interrogatives

<center><img width="230" alt="2021-04-05 (8)" src="https://user-images.githubusercontent.com/53667002/113701749-8666f800-9713-11eb-9e09-1b745455d276.png"></center>

위 그래프는 실험 1의 결과를 시각화한 것이다. 내포수준 1과 3에서만 주-동 수 일치 surprisal이 불일치 surprisal보다 낮게 나왔다. 즉 내포수준 1과 3에서만 제대로 구분을 하고 나머지는 못하는 것이다. 따라서 Transformer-XL의 성능이 RNN보다 아주 조금 더 좋다는 것을 알 수 있다.

<center><img width="222" alt="2021-04-05 (9)" src="https://user-images.githubusercontent.com/53667002/113701829-9c74b880-9713-11eb-9972-f1c264a1ffe4.png"></center>

실험 2에서도 결과가 좋지 않았다.


### 3.2 Experiment 3: Filler-gap surprisal in subject-inverted interrogatives

실험3은 주어가 도치된 의문문의 dependency에 대한 실험이다. 아래 예시 문장의 경우, talk about의 목적어가 what으로 대체되고 what이 절의 맨앞으로 이동한 것이다. 이때 what이 원래 있던 자리에 GAP이 생기는데 이 GAP과 what 사이에 dependency가 발생한다. GAP자리에 목적어 it을 삽입하면 talk about의 목적어가 두개(what과 it)가 되어버리기 때문에 비문이 된다. 따라서 실험3은 what과 GAP사이의 dependency를 모델이 잘 처리할 수 있는지 알아보는 것이다.

<center><img width="188" alt="2021-04-06" src="https://user-images.githubusercontent.com/53667002/113702016-e9f12580-9713-11eb-8baf-ebcfb138f7de.png"></center>

20개의 item 대해 WH-question이 있는 문장과 없는 문장 그리고 GAP이 있는 것과 없는 것 그리고 내포수준 1-4까지 해서 20x2x2x4, 총 320문장으로 실험을 진행한다. 

그리고 GAP뒤에 나오는 item, 위 예시에서는 at뒤에 나오는 item의 softmax값(확률값)을 추출한다.

> 본 논문의 저자인 Rui P. Chaves는 island effect를 전공한 사람이다. 그래서 experiment syntax 연구 모델을 활용하여 본 실험을 설계한 것이다. a, b, c, d에서 b와 c는 정문이고 a와 d는 비문이다. 이와 같은 2x2 design은 a-b, a-c, c-d 각각이 syntactic constraint를 기준으로 하나의 minimal pair가 된다. 이런 식으로 2x2 conditional design으로 정석적인 experiment syntactic method를 사용하였다. 20개의 item은 각 항목마다 lexicalization을 해서 20개씩 분포했다는 말이다. 이러한 metric은 human evaluation에서도 매우 좋은 설계이다.


### 3.2.1 Experiment 3 result: Filler-gap surprisal in subject-inverted interrogatives

<center><img width="228" alt="2021-04-06 (1)" src="https://user-images.githubusercontent.com/53667002/113702113-0ab97b00-9714-11eb-9b8c-c4bb3175c863.png"></center>

위 boxplot은 내포수준 1에 대한 결과를 시각화 한 것이다. 그래프 상에서 두 번째와 세 번째 박스가 정문이고 첫 번째와 네 번째 박스가 비문에 해당한다(wh가 없을 때는 gap도 없어야 정문이고 wh가 있을 때는 gap도 있어야 정문이 된다.) 따라서 모델이 예측을 잘 했다면 두 번째와 세 번째 surprisal이 첫 번째와 네 번째보다 낮아야 한다. 하지만 실험 결과, 정문에 해당하는 surprisal이 비문에 해당하는 surprisal보다 높게 나타났다. 나머지 내포수준에서도 전반적으로 결과가 좋지 않아 Transformer-XL이 주어 도치 의문문 처리 성능이 좋지 않음이 확인되었다.

> 최근 정략적 기반의 empirical linguistic study에서 boxplot을 이용한 시각화가 많이 보이고 있다.

### 3.3 Experiment 4: Filler-gap surprisal in uninverted indirect interrogatives

<center><img width="200" alt="2021-04-06 (2)" src="https://user-images.githubusercontent.com/53667002/113702191-21f86880-9714-11eb-9b52-5d11bba70634.png"></center>

실험 4에서는 모델이 도치가 안 된 간접 의문문에서의 dependency를 파악할 수 있는지를 확인한다. 일반적으로 의문문은 주어와 동사가 도치되지만 의문문이 모절 안에 내포될 경우에는 도치 없이 실현된다. What과 GAP사이에 dependency가 발생한다는 점은 실험 3과 동일하지만 We talked about과 같이 주어와 동사가 도치되지 않는 점이 차이이다.

### 3.3.1 Experiment 4 result: Filler-gap surprisal in uninverted indirect interrogatives

실험 4의 결과는 그냥 처참하다. 모든 embedding level에서 올바른 surprisal pattern이 관찰되지 않았다. Transformer-XL은 RNN보다 dependency처리 능력이 나쁘다고 볼 수 있다.

## 4. BERT

BERT에 대한 자세한 설명은 [BERT](https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/)여기에 있다.

### 4.1 Experiment 1 result: agreement in clefts

<center><img width="223" alt="2021-04-06 (6)" src="https://user-images.githubusercontent.com/53667002/113702288-494f3580-9714-11eb-952f-b37b608d9956.png"></center>

BERT에는 masked token이 사용되는데 본 실험에서는 be동사에 masking을 했다. 

<center><img width="199" alt="2021-04-06 (7)" src="https://user-images.githubusercontent.com/53667002/113702377-6257e680-9714-11eb-9d20-cf96fbdfdb6d.png"></center>

위 예문처럼 was나 were부분을 가려놓고 그 곳의 surprisal값을 구하는 방식으로 실험을 진행하였다. 정문이 비문보다 surprisal이 낮아야 한다. 위 boxplot을 보면 한 박스 내에서 1번과 4번이 낮고 2번과 3번이 높게 나타나 BERT 실험 결과가 통계적으로 유효한 차이를 보이고 있음을 알 수 있다. 

### 4.2 Experiment 2 result: agreement in indirect interrogatives

실험 2는 간접 의문문과 관련된 실험이다. 

<center><img width="225" alt="2021-04-06 (8)" src="https://user-images.githubusercontent.com/53667002/113702441-77347a00-9714-11eb-825b-b7adbfce31e2.png"></center>

여기에서도 실험 1과 마찬가지로 한 박스 내에서 1번과 4번이 나머지보다 surprisal이 낮아야 한다. 내포수준 1, 2, 3에서는 잘 처리가 되고 있고 level4에서는 조금 결과가 안 좋다. 조금 아쉬운 결과지만 다른 모델들보다 BERT의 결과가 가장 잘 나왔다.

### 4.3 Additional Experiment 1 result

## Reference

> Da Costa.J et al."Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies,"Association for Computational Linguistics.2020

