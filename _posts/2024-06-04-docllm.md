---
title: "DocLLM: A layout-aware generative language model for multimodal document understanding"
category: Multimodal
tag: Multimodal
---







* 목차
{:toc}











# 1. Introduction

DocLLM은 특정 양식을 갖춘 자료 혹은 각종 청구서를 처리하기 위 모델로, JPMorgan에서 개발하였다. 해당 모델은 아래 그림과 같이 요약될 수 있다.

<center><img width="600" src="https://github.com/finddme/finddme.github.io/assets/53667002/5d962f31-5931-46dd-af7a-c36264659596"></center>
<center><em style="color:gray;">DocLLM: A layout-aware generative language model for multimodal document understanding</em></center><br>

1. 학습에 사용되는 데이터는 OCR을 통해 text token과 그에 대한  bounding box 정보가 포함된 데이터이다. 해당 모델은 문서의 layout 구조를 이해할 때 기존의 Multimodal Model들이 시각 정보를 처리하는 방식과는 달리 image encoder가 아닌 OCR을 통해 얻어진 bounding box 정보를 사용한다. (image encoder를 사용하지 않기 때문에 모델 크기가 타 Multimodal Model에 비하여 작고, 이에 따라 추론 시간도 단축된다.)

2. 해당 모델은  text semantic과 spatial layout 간의 관계를 포착할 때 확장된 attention mecanism을 도입한다. 본 모델은 문서 내 text 와 공간적(spatial) 정보를 각각 따로 처리하기 위해 두 modality 간의 cross-alignment를 포착할 때 classical transformer의 self-attention mechanism을 분해하여 사용한다. 구체적으로, 각 modality에 대한 attention score 뿐만 아니라 두 modality간의 관계를 포착하는 attention score도 계산한다. 

3. 다양한 layout과 문서 내 시각적 정보를 잘 파악하기 위해 사전 학습 과정에서 infill text segment task를 학습한다. infill text segment task는 다양한 layout을 지닌 문서에서 이전 text 정보가 현재 text와 관련 없을 가능성이 있기 때문에 이러한 경우를 파악하기에 적합한 과제이다. 

4. 사전 학습 후에는 네 가지 핵심 문서에 대해 intelligence task에 대한 fine-tuning(instruction-tuning)을 수행한다.

본 연구의 contribution은 아래와 같다:

- visual document를 이해하기 위해 설계된 light-weight LLM 개발
-  text and layout modality 간의 cross-alignment를 위한 분리된 attention mechanism 제안
- 불규칙적인 layout을 효과적으로 파악하기 위한 infilling pre-training objective 도입
- visual document intelligence task 수행을 위해 특수 생성된 instruction-tuning dataset 구축

