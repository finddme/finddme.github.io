---
title: RAG(Retrieval-Augmented Generation)
category: Natural Language Processing
tag: NLP
---







* 목차
{:toc}








# 1. What is RAG?
RAG(Retrieval-Augmented Generation)은 LLM 기반 QA시스템에서 LLM의 hallucination 문제를 완화하고 정보를 효율적이고 정확하게 반환하기 위한 방법론이다. 
RAG은 Indexing-Retrieval-Generation 단계를 통해 사용자로부터 입력된 질문에 대해 답을 반환한다.

1) Indexing
   - 문서(PDF, docx 등)를 chunk로 분절

    Transformer 계열의 모델은 고정된 입력 길이를 input으로 받는다. 모델의 context window가 커도 문서 전체를 입력으로 넣는 것보다 질문과 의미 유사도가 높은 몇몇 chunk만 입력으로 넣는 것이 정확한 답변을 생성하는 데에 도움이 된다. Chunking 단계에서는 각 chunk들이 의미를 잃지 않는 선에서 일정한 크기로 분절되는 것이 중요하다. 예를 들어 하나의 문장이 chunk size에 걸려 두 부분으로 나뉘어서는 안 된다. 문장이나 단락 단위로 chunk를 나누는 것이 좋다.
    
    chunk size는 사용중인 embedding 모델에 따라 결정하는 것이 좋다. Sentence-Transformer의 경우 max token size가 512이고,  OpenAI ada-002는 8191이다. 

   - 분절된 chunk를 vector로 encode(indexing)

     embedding model 선정도 매우 중요하다. [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)를 통해 적절한 Embedding Model을 선정할 수 있다.
     
   - vector DB에 적재

     
1) Retrieval
   - 입력된 질문과 의미 유사도가 높은 Top k의 chunk를 찾는다.
   - Indexing 단계에서 사용된 embedding model과 동일한 모델을 사용하여 사용자 질문을 encoding해야 한다.
     
2) Generation
   - 입력된 질문과 retrieved chunk를 LLM에 함께 입력하여 답변을 생성한다.
  
     아래와 같이 user_message(사용자 질문)과 context(retrieval 결과)를 함께 LLM에 입력하며, context를 참고하여 user_message에 대한 답변을 생성하라는 prompt를 포함시켜 LLM에 입력한다.
     ```
      prompt=f"""
      <|im_start|>system
      You are a helpful assistant chatbot. 
      Write a response that appropriately completes the request, referring to given Context.
      When generating responses, it is crucial to adhere to the following conditions.
      - Do not generate new question. You must respond only to the given question.
      - You should never repeat the same sentence.
      - Do not repeat the questions in your response. 
      - You must answer in Korean Language. Do not use any other languages except Korean.
      Here is context to help:
      context: {context}<|im_end|>
      <|im_start|>user
      {user_message}<|im_end|>
      <|im_start|>assistant
      """  
     ```

RAG은 내부 플로우 구성에 따라 아래 이미지와 같이 크게 3 종류로 나뉜다.

<center><img width="1000" src="https://github.com/finddme/finddme.github.io/assets/53667002/002dd783-0e47-4e1b-847a-c31bc4898693"></center>

## 1.1 Naive RAG

Naive RAG(standard RAG)은 가장 기본적인 RAG이다. 

<center><img width="1000" src="https://github.com/finddme/finddme.github.io/assets/53667002/7bd535bf-efd6-41d5-9461-b47b28bd4400"></center>

### Naive RAG의 문제점

Naive RAG의 각 단계마다 문제점들이 존재한다.

1. Query:
   
   사용자의 질문이 명확하지 않은 경우, similarity search 과정에서 오류가 발생할 수 있다.
   
2. Indexing
   
   1) Parsing: PDF와 같은 비정형 문서 내 이미지 및 표에 담긴 유용한 정보에 대한 추출이 불완전하다.

   2) Chunking: 파일 특성을 고려하지 않고 일률적인 크기로 chunking함으로써 의미상 포함되어야 할 정보가 고정된 size 문제로 잘려 각 chunk에 불완전한 정보가 담겨있을 가능성이 있다.

   3) Indexing: vector db의 indexing 구조가 파일 유형마다 최적화되어 있지 않아 retreival 과정에 부정적인 영향을 미칠 수 있다.

   4) Embedding Model: 임베딩 모델의 semantic representation 성능이 좋지 않을 경우 retreival 과정에 부정적인 영향을 미칠 수 있다.

3. Retrieval
   
   1) 검색된 chunk들과 질문의 관련성이 낮을 가능성이 있다.
 
   2) 여러 검색 알고리즘을 종합적으로 사용할 수 없어 검색 기능이 제한적이다.
   
   3) 검색된 chunk들에 유사한 정보가 중첩되는 경우, LLM이 생성 시 참고할 정보가 제한적이다.

4. Generation
   
   1) 생성모델 특성 상 동일한 질문에 대해 일관적이지 않은 답변이 생성될 수 있다.

   2) 생성 모델의 성능에 따라 새로운 답변을 생성하지 않고 검색된 chunk에 과도하게 의존하는 경우도 있다. 이 경우 chunk를 그대로 반환하기도 한다.

   3) LLM의 문제점을 완화하기 위해 RAG을 적용했지만 여전히 부정확하거나 관련 없는 답변을 생성할 가능성이 있다. 
  

## 1.2 Advanced RAG

Advanced RAG은 Naive RAG의 문제점을 완화하기 위해 Indexing 과정 혹은 Retrieval 앞뒤에 검색 정확도를 높이기 위한 과정이 추가된 RAG이다. 아래 그림에서 파란색 부분이 이에 해당한다.

<center><img width="1000" src="https://github.com/finddme/finddme.github.io/assets/53667002/74c631bc-d917-43c1-a6f1-372cad685242"></center>

### Pre-Retrieval(Query transformations)

Pre-Retrieval 단계에서는 Retrieval 시 질문과 관련있는 chunk를 더 잘 검색하기 위해 Query를 조작한다.

1) Query reformulation

   사용자의 의도와 더 가깝게 질문을 재작성하는 방법. 
   
2) Query expansion

   동의어나 관련 표현들을 통해 질문을 확장함으로써 관련있는 chunk 검색을 돕는 방법

3) Query normalization
   
   철자 수정, 용어 차이 완화, 특수문자 및 공백 제거 불용어 제거 등 query 정규화 방법

4) Multi/Sub query transformation

   복잡한 질문을 LLM을 통해 단순한 여러 질문으로 수정한 후 Multi Query Retrieval을 수행하는 방법. 

   예를 들어 "John과 Mike 중 이번 중간 평가 평균 점수를 알려줘"

   -> sub query 1) John의 중간 평가 평균 점수를 알려줘.
      sub query 2) Mike의 중간 평가 평균 점수를 알려줘.

   하지만 이 방법은 두 sub query에 대해 검색된 chunk가 상이하여 서로 다른 문서로부터 답을 찾을 가능성이 있다는 문제가 있다. 

### Indexing

1) Hierarchical index retrieval:

검색 데이터(문서)가 많은 경우, 검색 효율을 높이기 위해 문서 요약 index와 문서 chunk index 단계를 만들어 우선 요약을 통해 관련 문서를 filtering한 후 해당하는 문서 chunk 중 질문과 의미 유사도가 높은 chunk들을 뽑는 방법이다.

2) Hypothetical Questions

LLM을 통해 각 chunk들에 대한 질문을 생성하여 이와 같은 가상 질문을 embedding하여 chunk와 함께 vector DB에 저장하는 것이다. 이와 같은 방법은 질문과 가상 질문 간의 의미적 유사도가 질문과 chunk 사이의 유사도보다 높기 때문에 검색 품질을 향상시킬 수 있다. 

3) HyDE

HyDE는 Hypothetical Questions와 반대의 접근 방식이다. LLM에 사용자의 질문을 입력하여 가상 응답을 생성하도록 한 후 생성된 가상 응답과 chunk 사이의 유사도를 통해 검색을 수행하는 방법이다. (비추다.)

4) Context enrichment

Context enrichment는 검색된 chunk에 대해 추가적으로 context를 강화하는 방법이. Context enrichment에는 크게 두 가지 방식이 있다.

4.1) Sentence Window Retrieval

chunk를 문장 단위로 만들어 embedding한 후 query와 문장 사이의 유사도를 검색한 후 해당 문장의 전 후 k개 문장으로 context window를 확장하여 LLM에 입력하는 방법.

4.2) Auto-merging Retriever (Parent Document Retriever)

우선 작은 size의 chunk를 검색한 후 검색된 상위 k개의 chunk 중 n개 이상의 조각이 동일한 상위 node(larger chunk)와 연결된 경우 해당 node를 LLM이 참고할 context로 입력한다.

### Post-Retrieval

1) Fusion retrieval or hybrid search

Fusion retrieval 혹은 hybrid search는 Query와 chunk에 대한 의미 유사도, 키워드 매칭을 모두 고려하기 위해 tf-idf 혹은 BM25와 같은 keyword-based old school search와 최근에 많이 사용되는 semantic/vectore search 를 결합하는 방법이다. 

이 방법론에서 중요한 것은 두 검색 결과를 적절히 결합하는 것이다. 검색 결과 결합에는 일반적으로 Reciprocal Rank Fusion algorithm이 사용된다. 

2) Reranking & filtering

검색 결과에 대해 유사도 점수 등을 기준으로 chunk를 정렬하거나, 키워드나 metadata를 기반으로 검색된 chunk를 filtering하는 단계로, 검색 chunk에 대한 마지막 후처리 단계로 볼 수 있다.




