---
title: RAG(Retrieval-Augmented Generation)
category: Natural Language Processing
tag: NLP
---







* 목차
{:toc}








# 1. What is RAG?
RAG(Retrieval-Augmented Generation)은 LLM 기반 QA시스템에서 LLM의 hallucination 문제를 완화하고 정보를 효율적이고 정확하게 반환하기 위한 방법론이다. 
RAG은 Indexing-Retrieval-Generation 단계를 통해 사용자로부터 입력된 질문에 대해 답을 반환한다.

**1) Indexing**
   - 문서(PDF, docx 등)를 chunk로 분절

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Transformer 계열의 모델은 고정된 입력 길이를 input으로 받는다. 모델의 context window가 커도 문서 전체를 입력으로 넣는 것보다 질문과 의미 유사도가 높은 몇몇 chunk만 입력으로 넣는 것이 정확한 답변을 생성하는 데에 도움이 된다. Chunking 단계에서는 각 chunk들이 의미를 잃지 않는 선에서 일정한 크기로 분절되는 것이 중요하다. 예를 들어 하나의 문장이 chunk size에 걸려 두 부분으로 나뉘어서는 안 된다. 문장이나 단락 단위로 chunk를 나누는 것이 좋다. chunk size는 사용중인 embedding 모델에 따라 결정하는 것이 좋다. Sentence-Transformer의 경우 max token size가 512이고,  OpenAI ada-002는 8191이다. 

   - 분절된 chunk를 vector로 encode(indexing)

     embedding model 선정도 매우 중요하다. [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)를 통해 적절한 Embedding Model을 선정할 수 있다.
     
   - vector DB에 적재

     
**2) Retrieval**
   - 입력된 질문과 의미 유사도가 높은 Top k의 chunk를 찾는다.
   - Indexing 단계에서 사용된 embedding model과 동일한 모델을 사용하여 사용자 질문을 encoding해야 한다.
     
**3) Generation**
   - 입력된 질문과 retrieved chunk를 LLM에 함께 입력하여 답변을 생성한다.
  
     아래와 같이 user_message(사용자 질문)과 context(retrieval 결과)를 함께 LLM에 입력하며, context를 참고하여 user_message에 대한 답변을 생성하라는 prompt를 포함시켜 LLM에 입력한다.
     ```
      prompt=f"""
      <|im_start|>system
      You are a helpful assistant chatbot. 
      Write a response that appropriately completes the request, referring to given Context.
      When generating responses, it is crucial to adhere to the following conditions.
      - Do not generate new question. You must respond only to the given question.
      - You should never repeat the same sentence.
      - Do not repeat the questions in your response. 
      - You must answer in Korean Language. Do not use any other languages except Korean.
      Here is context to help:
      context: {context}<|im_end|>
      <|im_start|>user
      {user_message}<|im_end|>
      <|im_start|>assistant
      """  
     ```

## 1.1 Naive RAG

  &nbsp;&nbsp;&nbsp;&nbsp;Naive RAG(standard RAG)은 가장 기본적인 RAG이다. 

<center><img width="1000" src="https://github.com/finddme/finddme.github.io/assets/53667002/7bd535bf-efd6-41d5-9461-b47b28bd4400"></center>

### 1.1.1 Naive RAG의 문제점

  &nbsp;&nbsp;&nbsp;&nbsp;Naive RAG의 각 단계마다 문제점들이 존재한다.

&nbsp;&nbsp;&nbsp;&nbsp;**1. Query:**
   
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;사용자의 질문이 명확하지 않은 경우, similarity search 과정에서 오류가 발생할 수 있다.
   
&nbsp;&nbsp;&nbsp;&nbsp;**2. Indexing**
   
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) Parsing: PDF와 같은 비정형 문서 내 이미지 및 표에 담긴 유용한 정보에 대한 추출이 불완전하다.

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) Chunking: 파일 특성을 고려하지 않고 일률적인 크기로 chunking함으로써 의미상 포함되어야 할 정보가 고정된 size 문제로 잘려 각 chunk에 불완전한 정보가 담겨있을 가능성이 있다.

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) Indexing: vector db의 indexing 구조가 파일 유형마다 최적화되어 있지 않아 retreival 과정에 부정적인 영향을 미칠 수 있다.

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4) Embedding Model: 임베딩 모델의 semantic representation 성능이 좋지 않을 경우 retreival 과정에 부정적인 영향을 미칠 수 있다.

&nbsp;&nbsp;&nbsp;&nbsp;**3. Retrieval**
   
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) 검색된 chunk들과 질문의 관련성이 낮을 가능성이 있다.
 
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) 여러 검색 알고리즘을 종합적으로 사용할 수 없어 검색 기능이 제한적이다.
   
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) 검색된 chunk들에 유사한 정보가 중첩되는 경우, LLM이 생성 시 참고할 정보가 제한적이다.

&nbsp;&nbsp;&nbsp;&nbsp;**4. Generation**
   
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) 생성모델 특성 상 동일한 질문에 대해 일관적이지 않은 답변이 생성될 수 있다.

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) 생성 모델의 성능에 따라 새로운 답변을 생성하지 않고 검색된 chunk에 과도하게 의존하는 경우도 있다. 이 경우 chunk를 그대로 반환하기도 한다.

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) LLM의 문제점을 완화하기 위해 RAG을 적용했지만 여전히 부정확하거나 관련 없는 답변을 생성할 가능성이 있다. 
  

## 1.2 Advanced RAG

  &nbsp;&nbsp;&nbsp;&nbsp;Advanced RAG은 Naive RAG의 문제점을 완화하기 위해 Indexing 과정 혹은 Retrieval 앞뒤에 검색 정확도를 높이기 위한 과정이 추가된 RAG이다. 아래 그림에서 파란색 부분이 이에 해당한다.

<center><img width="1000" src="https://github.com/finddme/finddme.github.io/assets/53667002/5e85f6b0-630f-45a4-959c-49ec2a8a7738"></center>

### 1.2.1 Enhancing Parsing Techniques

<center><img width="700" src="https://github.com/finddme/finddme.github.io/assets/53667002/d4a81b6f-f4b5-409a-9797-e74bcfdca358"></center>

  &nbsp;&nbsp;&nbsp;&nbsp;RAG은 문서를 검색한 후 검색된 문서를 기반으로 답변을 반환하는 구조로 작동되기 때문에 비정형 데이터로부터 정보를 정확히 추출하는 것이 중요하다. RAG을 위한 비정형 데이터의 종류는 매우 다양하지만 그 중 PDF parsing이 매우 까다롭다. PDF parsing 방법은 크게 세 가지이다.
  
  &nbsp;&nbsp;&nbsp;&nbsp;**1) Rule-base parsing**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pypdf, pdfplumber와 같은 규칙 기반 parser를 사용하는 방법이다. 이와 같은 라이브러리는 정해진 규칙에 따라 pdf에서 텍스트를 추출하기 때문에 다양한 형식의 PDF를 다루기에는 범용성이 떨어지는 문제가 있다.
  
  &nbsp;&nbsp;&nbsp;&nbsp;**2) deep learning model base parsing**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;object detection과 OCR 기술을 결합한 deep learning 모델을 기반으로 PDF에서 정보를 추출하는 방법이다. object detection을 통해 문서의 레이아웃을 식별하거나 표 내부 구조를 파악하여 정보를 보다 정확하게 추출할 수 있다. 하지만 모델을 사용하는 만큼 pasing에 소요되는 시간이 rule-base보다 길다는 것이 단점이다. 

  &nbsp;&nbsp;&nbsp;&nbsp;**3) multimodal large model base parsing**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;multimodal LLM을 사용하여 PDF를 parsing하는 방법이다.
    
### 1.2.2 Pre-Retrieval(Query transformations)

<center><img width="700" src="https://github.com/finddme/finddme.github.io/assets/53667002/841e3d5c-ed1f-4251-8345-174f671a18b3"></center>

  &nbsp;&nbsp;&nbsp;&nbsp;Pre-Retrieval 단계에서는 Retrieval 시 질문과 관련있는 chunk를 더 잘 검색하기 위해 Query를 조작한다. 
  
  &nbsp;&nbsp;&nbsp;&nbsp;**1) Query reformulation**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;사용자의 의도와 더 가깝게 질문을 재작성하는 방법. 
     
  &nbsp;&nbsp;&nbsp;&nbsp;**2) Query expansion**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;동의어나 관련 표현들을 통해 질문을 확장함으로써 관련있는 chunk 검색을 돕는 방법
     
  &nbsp;&nbsp;&nbsp;&nbsp;**3) Query Routing**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Query를 LLM에 효과적으로 입력하기 위해 Query부터 LLM 입력까지의 경로를 지정하는 것이다.
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;예를 들어 인덱스에 계층이 있는 경우, 각 chunk에 대해 요약 등의 metadata가 있는 경우 등 query가 LLM까지 도달하기까지 다양한 경로가 존재할 때 Query router를 통해 최적의 옵션을 타고 LLM에 도달하게 된다.
  
  &nbsp;&nbsp;&nbsp;&nbsp;**5) Query normalization**
     
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;철자 수정, 용어 차이 완화, 특수문자 및 공백 제거 불용어 제거 등 query 정규화 방법
  
  &nbsp;&nbsp;&nbsp;&nbsp;**6) Multi/Sub query transformation**
  
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;복잡한 질문을 LLM을 통해 단순한 여러 질문으로 수정한 후 Multi Query Retrieval을 수행하는 방법. 

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;예를 들어 "John과 Mike 중 이번 중간 평가 평균 점수를 알려줘"

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-> sub query 1) John의 중간 평가 평균 점수를 알려줘.
   
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   sub query 2) Mike의 중간 평가 평균 점수를 알려줘.

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;하지만 이 방법은 두 sub query에 대해 검색된 chunk가 상이하여 서로 다른 문서로부터 답을 찾을 가능성이 있다는 문제가 있다. 
   
   &nbsp;&nbsp;&nbsp;&nbsp;**7) Rewrite-Retrieve-Read**
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Query Rewriting for Retrieval-Augmented Large Language Models 논문에서 제안된 방법으로, 사용자의 질문이 LLM 검색에 최적화되지 않은 경우가 많아 LLM을 통해 질문을 재 작성하는 방법을 소개한다. 
  

   &nbsp;&nbsp;&nbsp;&nbsp;**8) Step-Back Prompting**

   LLM이 고수준 개념을 기반으로 추상적 추론 및 검색을 수행하도록 합니다.


   &nbsp;&nbsp;&nbsp;&nbsp;**9) Query2Doc**


LLM의 몇 가지 프롬프트를 사용하여 가상 문서를 만듭니다. 그런 다음 이를 원래 쿼리와 결합하여 새로운 쿼리를 만듭니다.


   &nbsp;&nbsp;&nbsp;&nbsp;**10) ITER-RETGEN**

이전 생성의 결과를 이전 쿼리와 결합하는 방법을 제안합니다. 그런 다음 관련 문서를 검색하고 새로운 결과를 생성합니다. 이 과정을 여러 번 반복하여 최종 결과를 얻습니다.


### 1.2.3 Enhanced indexing strategy

<center><img width="700" src="https://github.com/finddme/finddme.github.io/assets/53667002/db9a8afe-a2cd-4790-95e0-3fa3a9414ee5"></center>

  &nbsp;&nbsp;&nbsp;&nbsp;**1) Hierarchical index retrieval**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;검색 데이터(문서)가 많은 경우, 검색 효율을 높이기 위해 문서 요약 index와 문서 chunk index 단계를 만들어 우선 요약을 통해 관련 문서를 filtering한 후 해당하는 문서 chunk 중 질문과 의미 유사도가 높은 chunk들을 뽑는 방법이다.
  
  &nbsp;&nbsp;&nbsp;&nbsp;**2) Hypothetical Questions**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LLM을 통해 각 chunk들에 대한 질문을 생성하여 이와 같은 가상 질문을 embedding하여 chunk와 함께 vector DB에 저장하는 것이다. 이와 같은 방법은 질문과 가상 질문 간의 의미적 유사도가 질문과 chunk 사이의 유사도보다 높기 때문에 검색 품질을 향상시킬 수 있다. 
  
  &nbsp;&nbsp;&nbsp;&nbsp;**3) HyDE(Hypothetical Document Embeddings)**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HyDE는 Hypothetical Questions와 반대의 접근 방식이다. LLM에 사용자의 질문을 입력하여 가상 응답을 생성하도록 한 후 생성된 가상 응답과 chunk 사이의 유사도를 통해 검색을 수행하는 방법이다. (비추다.)
  
  &nbsp;&nbsp;&nbsp;&nbsp;**4) Context enrichment**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Context enrichment는 검색된 chunk에 대해 추가적으로 context를 강화하는 방법이. Context enrichment에는 크게 두 가지 방식이 있다.
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**4.1) Sentence Window Retrieval**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chunk를 문장 단위로 만들어 embedding한 후 query와 문장 사이의 유사도를 검색한 후 해당 문장의 전 후 k개 문장으로 context window를 확장하여 LLM에 입력하는 방법.
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**4.2) Auto-merging Retriever (Parent Document Retriever)**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;우선 작은 size의 chunk를 검색한 후 검색된 상위 k개의 chunk 중 n개 이상의 조각이 동일한 상위 node(larger chunk)와 연결된 경우 해당 node를 LLM이 참고할 context로 입력한다.

### 1.2.4 Optimizing Embedding Model

<center><img width="700" src="https://github.com/finddme/finddme.github.io/assets/53667002/ce34e6ca-9063-4072-8c27-8297df4ceb2a"></center>

  &nbsp;&nbsp;&nbsp;&nbsp;retrieval 수행 시 가장 관련성 높은 chunk를 추출하는 것이 중요하다. 일반적으로 retrieval은 query와 indexing된 chunk들 간의 벡터 검색을 통해 산출된 의미 유사도를 기반으로 수행된다. 이에 따라 embedding model이 문장 의미 정보를 잘 표현하는 것이 중요하다. Embedding model을 강화하는 방법으로 아래 두 가지가 잘 알려져 있다.
  
  &nbsp;&nbsp;&nbsp;&nbsp;**1) Fine-tuning embedding model**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Embedding model을 domain-specific context에 맞게 fine-tuning하는 방법이다.
  
  &nbsp;&nbsp;&nbsp;&nbsp;**2) Dynamic Embedding**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;각 단어에 하나의 벡터만 사용하는 static embedding(정적 임베딩)과 달리 model이 context에 맞게 vector를 embedding하는 방법으로, OpenAI의 embeddings-ada-02가 대표적인 동적 임베딩 모델이다.

### 1.2.5 Post-Retrieval

<center><img width="700" src="https://github.com/finddme/finddme.github.io/assets/53667002/0e7061b8-6878-4699-bb06-3f0a040c1029"></center>

  &nbsp;&nbsp;&nbsp;&nbsp;Post-Retrieval 단계에서는 검색된 문서들을 정제하여 답변 생성 품질을 향상시키기 위한 처리들을 한다.
  
  &nbsp;&nbsp;&nbsp;&nbsp;**1) Fusion retrieval or hybrid search**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fusion retrieval 혹은 hybrid search는 Query와 chunk에 대한 의미 유사도, 키워드 매칭을 모두 고려하기 위해 tf-idf 혹은 BM25와 같은 keyword-based old school search와 최근에 많이 사용되는 semantic/vectore search 를 결합하는 방법이다. 
    
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;이 방법론에서 중요한 것은 두 검색 결과를 적절히 결합하는 것이다. 검색 결과 결합에는 일반적으로 Reciprocal Rank Fusion algorithm이 사용된다. 
  
  &nbsp;&nbsp;&nbsp;&nbsp;**2) Re-Ranking**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;검색 결과에 대해 유사도 점수 등을 기준으로 chunk를 정렬하여 query와 관련성 높은 문서를 강조하기 위한 처리이다. Re-Ranking model은 query와 context를 input으로 받아 유사도를 측정한다. 
  
  &nbsp;&nbsp;&nbsp;&nbsp;**3) Prompt compression**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;관련 없는 context는 제거하고 중요한 context는 강조하여 전체 prompt 길이를 줄인다. 이 과정을 통해 LLM을 중요한 정보만 입력 받아 정확한 답변을 생성할 가능성이 높아지고, LLM의 입력이 줄어들어 추론 속도가 감소할 수 있다.
  
  &nbsp;&nbsp;&nbsp;&nbsp;**4) Filtering**
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;키워드나 metadata를 기반으로 검색된 chunk를 filtering하는 단계로, 관련 없는 문서가 검색되었을 때 해당 문서를 제거하기 위한 과정이다. 


