---
title: LLaMa 3
category: LLM / Multimodal
tag: NLP
---







* 목차
{:toc}










# LLaMa 3
LLaMA 3는 Meta에서 공개한 Open Source LLM model로, LLaMa2에 이어 공개된 모델이다. 
LLaMA 3는 Instruct Model과 Pre-trained Model에 대해 각각 8B, 70B 두 사이즈가 공개되었다. 

Pretraining과 Post-training 방법의 개선으로 공개된 8B, 70B의 Pretrained, Instruction-fine-tuned model이 2024 4월18일 기준 해당 parameter scale 모델 중 가장 좋은 성능을 보인다고 한다. Post-training과정에서는 false refusal rate를 줄이고, model의 alignment를 개선하고, model response의 다양성을 증가시켰다. 특히 LLaMa 3는 2보다 코드 생성, instruction 수행 능력이 크게 향상되어 모델을 보다 다양하게 활용할 수 있을 것으로 보인다. 

LLaMa 3 개발 시 Meta팀은 benchmark 성능 향상 뿐만 아니라 실제 추론 능력(real-world scenario) 최적화에도 집중하였다고 한다. 실제 추론 능력 검증을 위해 새로운 high-quality human evaluation set을 개발했다고 한다. 해당 데이터는 12가지 use case에 대한 총 1,800개 prompt로 구성되어 있다. (12가지 use case: asking for advice, brainstorming, classification, closed question answering, coding, creative writing, extraction, inhabiting a character/persona, open question answering, reasoning, rewriting, summarization). 아래 표는 해당 evaluation set에 대한 Claude Sonnet, Mistral Medium, GPT-3.5의 추론 결과를 비교한 것이다.

<center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/3a9d8f8f-20ab-4dcd-8ced-5454de03328c"></center>
<center><em style="color:gray;">https://ai.meta.com/blog/meta-llama-3/</em></center><br>


LLaMA 3 개발팀은 모델 개발 시 Model architecture, Training data, Scaling up pretraining, Instruction fine-tuning을 중점적으로 개선했다고 한다.

# Model architecture

- **Base architecture:** LLaMa 3는 standard decoder-only transformer architecture를 기반으로 개발되었다.
- **Tokenizer size:** 128K token 수를 가진 vocab을 사용하여 언어를 보다 효과적으로 encoding하여 모델 성능을 향상시킴. -> LLaMa 2보다 4배 큰 tokenizer size
- **Attention method:** 8B, 70B 모두 GQA를 사용하여 모델 추론 효율성을 높였다. -> LLaMa 2는 70에만 GQA 적용
- **context window:** 8,192 token -> LLaMa 2는 4,096였다.

> **Tokenizer size가 커지면 encoding을 효과적/효율적으로 할 수 있는 이유:**
> tokenzer size가 4배 커지면 동일한 데이터가 약 4배정도 압축되어 모델에 입력될 수 있다.
> 예를 들어 입력한 데이터가 LLaMa 2의 tokenizer로 200 token으로 encoding 될 때, LLaMa 3 tokenizer로는 100개가 되지 않는 token 수로 encoding될 수 있기 때문이다.
> 그리고 tokenizer size가 커진 만큼 모델이 많이 학습하지 않은 한국어 데이터에 대한 tuning을 수행할 때 vocab에 따로 한국어 token을 추가하지 않아도 된다.

# Training data

## size
LLaMa 3는 15T개의 token으로 학습되었다. (LLaMa 2는 2T로, 2에 비해 3는 약 7배 더 큰 학습데이터를 사용하였다.)

수집된 데이터는 모두 publicly available source에서 수집되었다.

- code data: LLaMa 2에 사용된 code data 수보다 4배 더 많음
- multilingual use case를 위해 전체 training data의 5% 이상을 high-quality non-English(30개 이상) data로 구성

## data filtering

data의 품질을 향상시키기 위해 data-filtering pipeline을 개발했다. 

- heuristic filters
- NSFW filters(음란 데이터 filtering)
- semantic deduplication approaches(의미 중복 제거)
- text classifier

filtering model은 LLaMa 2를 기반으로 한다.

# Scaling up pretraining

## scaling law
새로운 scaling law를 발견했다고 한다.

- Chinchilla-optimal은 8B model 학습 시 가장 최적의 학습 token양은 ~200B token이라고 했지만 200B보다 많은 token을 학습해도 model의 성능은 계속 증가한다.
  Llama3 8B, 70B를 15T token data로 학습시켰을 때 수렴점에 근접하지 않으면서 log-linearly하게 모델 성능이 계속 향상되는 것을 확인했다고 한다.
  
## parallelization

70B 모델 학습을 위해 data parallelization, model parallelization, pipeline parallelization을 결합했다. 


# Instruction fine-tuning

LLaMa 3의 post-training은 supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), direct preference optimization (DPO)를 결합한 방식으로 수행되었다. post-training에서는 SFT에 사용되는 prompt와 PPO, DPO에 사용되는 preference ranking이 중요하다. 


# Reference

> https://ai.meta.com/blog/meta-llama-3/
