---
title: Contextual word representation
category: Natural Language Processing and Linguistics
tag: NLP & Linguistics
---

## 1. Contextual representations of language

language model들의 vector representation방식이 변화는 아래와 같다. 

word type들에 대해 pre-trained vector embedding된 word2vec과 Glove가 있다. 아래 수식에서$f_vocab : v->h$에서 f_vocab은 vocabulary가 embedding된다는 것을 나타낸다.

\begin{matrix}
f_vocab: v->h
\end{matrix}

FastText와 같은 Subword-informed 방식은 literal character, 즉 음절 자체가 sequence로 embedding되는 것이다. 그래서 앞에 $v_1$이 단어이고 뒤에 있는 $c_1$부터 $c_t$가 character단위로 쪼개진 것이다.

\begin{matrix}
f_subword : (v,(c_1, …, c_t) ->h
\end{matrix}

ELMo와 BERT는 특정 문맥에서의 해당 단어의 쓰임이 반영한다. w_1부터 w_n은 텍스트에 존재하는 word이고, 이것들이 embedding되어서 $h_1$부터 $h_n$이 되는 것이다.

\begin{matrix}
f_contextual : (w_1, …, w_N) => (h_1, … h_N)
\end{matrix}

## 2. Beyond “words in context”

strong contextual representation을 위한 model들(CoVe, ELMo, ULMFiT, GPT, BERT, GPT2)의 behavior를 보완하기 위해 강력한 contextual model들이 언어 이해에 중요한 task들을 암시적으로 수행하는 것이 제안된다(syntax, coreference, QA 등).

문맥에서의 단어는 파악했지만, 문장 자체가 지닌 중의성들과 구조적인 중의성 표현들은 어떻게 해결할 것인가에 대한 문제가 남아있다. The chef who ran to the stroe was out of food라는 문장은 중의성을 가진다. 이 문장에는 두 가지 가능성이 존재한다. 음식이 없기 때문에 셰프가 다른 store에 간 것인지 혹은 재료가 다 떨어져서 셰프가 식당으로 돌아온 것인지. 직감적으로는 후자가 맞을 것 같다. 그래서 chef가 was에 걸릴 수 있도록 하는 것이 직관에 맞는 구문분석이다.

> 언어학 지식을 고려한 NLP논문들(Tal Linzen과 John Hewitt의 논문 등)은 대부분 모델의agreement와 관계절 반영을 중요 주제로 삼는다. agreement는 grammatical theory에 있어 중요한 역할을 한다. 따라서 agreement가 설명이 되어야 syntax가 어느정도 맞다고 이론적으로 이야기할 수 있다. 자연어는 sequential한 속성과 hierarchical한 속성이 공존한다. 영어의 장거리 의존 구문은 크게 관계절, wh-question, topicalization이 있다. 그 중에서 syntactic complexity(통사적 복잡도)가 가장 높은 것이 관계절이다. 따라서 자연어가 가지는 단순한 sequential한 속성이 아닌 hierarchical한 속성까지 반영하여 구문을 처리했다고 말할 수 있으려면 agreement와 장거리 의존 구문을 처리할 수 있어야 모델이 English syntax의 어떠한 특성을 반영하고 있다고 말할 수 있다. BERT와 ELMo와 같은 large neural network들이 syntax를 제대로 이해한다면 hierarchical한, in-depth한 syntactic information을 반영할 것이다.

## Reference

> https://nlp.stanford.edu/~johnhew/structural-probe.html#the-structural-probe
