---
title: "RAG : Embedding Model, Reranking Model"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}











# Embedding Model

Embedding Model은 bi-encoder model이다. 

- Bi-encoder model은 두 개의 독립적인 encoder로 구성됨. 그래서 "bi" encoder.
  - encoder 1은 입력 query를 encoding
  - encoder 2는 관련/무관 문장을 encoding
  - 두 encoder는 각각 독립적으로 embedding을 생성
- 학습
  - 학습 과정에서는 query와 관련된 documnet들 간의 유사도를 최대화하는 방향으로 학습.
  - query와 관련 없는 documnet들 간의 유사도는 최소화하는 방향으로 학습.
  - 아래와 같은 문장 쌍 데이터를 활용하여 문장 임베딩을 생성하고, 유사한 문장은 벡터 공간에서 가깝게, 다른 문장은 멀게 위치하도록 학습
  - 데이터 예시
    ```json
    [
    {
      "sentence1": "강아지가 공원에서 뛰어놀고 있다.",
      "sentence2": "개가 야외에서 운동하고 있습니다.", 
      "label": 1  # 유사
    },
    {
      "sentence1": "오늘 날씨가 좋습니다.",
      "sentence2": "내일은 비가 올 예정입니다.",
      "label": 0  # 비유사
    }
    ]
    ```
- 유사도 점수 산정 방식(추론)
  - query와 각 문장 간의 유사도를 독립적으로 계산. 


# Summary

대규모 검색에서는 Bi-encoder로 후보군을 추린 후 Cross-encoder로 재순위화하는 방식을 사용하는 것이 일반적이다.이를 통해 속도와 정확도의 균형을 맞출 수 있다.

|             | Bi-encoder                                                             | Cross-encoder                                                 |
| ----------- | ---------------------------------------------------------------------- | ------------------------------------------------------------- |
| encoding 방식 | \- 두 문장을 독립적으로 encoding<br>\- 각 문장에 대해 별도의 임베딩 벡터를 생성                  | \- 두 문장을 동시에 입력으로 받아 함께 encoding<br>\- 두 문장의 관계를 직접적으로 모델링    |
| 성능 및 정확도    | \- 일반적으로 Cross-encoder보다 정확도가 낮음<br>\- 정보 손실이 발생할 수 있어 성능이 다소 떨어질 수 있음 | \- 비교적 높은 정확도<br>\- 두 문장 간의 관계를 더 잘 파악                        |
| 속도 및 확장성    | \- 빠른 처리 속도와 높은 확장성<br>\- 대규모 데이터셋에 적합                                 | \- 처리 속도가 상대적으로 느리고 확장성이 제한적<br>\- 소규모 데이터셋이나 정확도가 중요한 작업에 적합 |
| 응용 분야       | \- 정보 검색, 의미론적 검색, 클러스터링에 적합<br>\- 실시간 처리가 필요한 작업에 유용                  | \- 분류 작업이나 정확한 순위 매기기에 적합<br>\- 소수의 문장 쌍을 비교하는 작업에 효과적        |
