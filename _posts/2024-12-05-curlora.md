---
title: "Key-Value(KV) Caching (작성 중)"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}













Transformer는 2017년 발표된 "Attention Is All You Need" 논문에서 발표되었다. 최근 GPT 계열의 LLM들은 대부분 Transformer의 decoder 구조를 기반으로 하고 있다. LLM들의 복잡성과 규모가 커지며, 생성 응답의 속도를 향상시키기 위한 다양한 시도가 이루어지고 있다. Key-Value(KV) Caching은 그런한 방법론들 중 하나이다. 
