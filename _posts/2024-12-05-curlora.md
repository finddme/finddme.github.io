---
title: "Key-Value(KV) Caching (작성 중)"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}













Transformer는 2017년 발표된 "Attention Is All You Need" 논문에서 발표되었다. 최근 GPT 계열의 LLM들은 대부분 Transformer의 decoder 구조를 기반으로 하고 있다. 

LLM이 널리 사용되며 빠른 응답에 대한 필요성도 커져 LLM의 생성 속도를 향상시키기 위한 다양한 시도가 이루어지고 있다. Key-Value(KV) Caching은 그런한 방법론들 중 하나이다. 

Key-Value(KV) Caching을 간략이 설명하면 transformers모델이 attention연산을 수행할 때 사용하는 key와 value값들을 저장해 주는 기법이다. 이전에 계산했던 결과를 재사용하여 불필요한 연산을 줄이는 것이다. 예를 들어 LLM 기반 채팅에서 사용자가 "안녕"이라고 입력했을 때 모델이 이 입력을 처리할 때 생성한 key-value값을 cache에 저장하여 사용자가 추가 메세지를 보낼 때 이전에 저장해 둔 key-value값을 재활용하여 생성 소요 시간을 줄이는 것이다.

# Transformer and Self-Attention

최근 많이 사용되는 GPT계열 모델들(decoder 기반 모델)은 autoregressive 모델이라고도 불린다. autoregressive 모델이라고 불리는 이유는 이전 time step의 출력이 다음 time step의 입력으로 들어가는 점이 자기 회기적이기 때문이다. 해당 모델은 이런 방식으로 이전 time step의 결과를 다음 time step의 입력으로 받아 다음 token을 예측하는 task를 수행한다. 

## Transformer 기본 구성 요소

- Tokenizer: 입력 text를 token 단위로 나눔
- Embedding Layer: token을 vector로 변환
- dropout, layer normalization, feed-forward: 기본 신경망 layer들
- self attention layer

## Basic Self-Attention

self-attnetion은 모델이 다음 toekn을 생성할 때 입력 sequence의 특정 부분에 주의를 기울일 수 있도록 하는 것이다. 

```
She poured the coffee into the cup
```

위 문장의 경우, 모델이 "into"라는 단어를 예측할 때, "poured"와 "coffee"와 같이 다음 단어를 예측하기에 더 도움이 되는 중요한 문맥을 제공하는 token에 주의를 기울여 다음 단어를 예측하도록 하는 것이 self-attention이다.

이를 위해 self-attnetion은 각 embedding token을 context vector라는 것으로 변환시킨다. 이는 주어진 text의 모든 input으로부터 정보를 결합하는 방식으로 계산된다.

### Context Vector 생성 과정 예시

```
The cat sat on the mat
```

1. 각 단어마다 하나의 context vector가 생성된다. (위 예문의 경우 3개)
2. "cat"이라는 단어의 컨텍스트 벡터를 만들 때 아래와 같은 가중치 행렬들이 필요하
3. Query: "cat"이 다른 단어들과 얼마나 관련있는지 계산
4. Key: 입력 시퀀스 내 모든 token들 특징
5. Value: 입력 시퀀스 내 각 단어의 실제 의미


```python
class Basic_SelfAttention(torch.nn.Module):

    def __init__(self, d_in, d_out, qkv_bias=False):
        super().__init__()
        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias) # 현재 time step의 toeken이 다른 token들과 얼마나 관련 있는지 나타내는 가중치 행렬
        self.W_key   = torch.nn.Linear(d_in, d_out, bias=qkv_bias) # 입력 시퀀스 내 모든 token들을 나타내는 가중치 행렬 
        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias) # 입력 시퀀스 내 각 단어의 실제 의미가 담긴 가중치 행렬

    def forward(self, x):
        # 현재 처리 중인 token x에 대한 가중치 행렬 계산
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)
        
        attn_scores = torch.matmul(queries, keys.T) # 현재 time step token과 다른 token들 간의 관련성 scoring 
        attn_weights = torch.softmax(
          attn_scores / keys.shape[-1]**0.5, dim=-1
        ) # attention 가중치 정규화

        context_vec = torch.matmul(attn_weights, values)  # context vector 생
        return context_vec
```
