{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습 24-1] 시퀀스-투-시퀀스 모델(Seq2Seq Model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리와 시퀀스 모델\n",
    "이번 장(24장)에서는 시퀀스-투-시퀀스 모델에 대한 실습을 진행해 보도록 하겠습니다.\n",
    "\n",
    "시퀀스-투-시퀀스 모델은 시퀀스가 입력으로 들어가면 출력 또한 시퀀스로 출력되는 모델을 말합니다.\n",
    "\n",
    "이번 시간에는 케라스(Keras) 공식 블로그에 나와있는 모델을 바탕으로, 영어-프랑스어 간 번역모델을 간단하게 구현해보고 그 원리를 이해해 보도록 합시다.\n",
    "\n",
    "# 실습\n",
    "1. (입력 데이터 수, 입력 최대 길이, 토큰 수) 크기의 값이 0인 배열을 생성합니다.\n",
    "\n",
    "2. LSTM층을 변수 encoder에 저장합니다.(return_state=True / latent_dim 활용)\n",
    "\n",
    "3. LSTM층을 변수 decoder에 저장합니다.(return_state=True, return_sequence=True / latent_dim 활용)\n",
    "\n",
    "4. inital_state를 인코더의 states로 설정하고 디코더의 입력값을 바탕으로 lstm 연산을 수행하는 층을 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/fra.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a013ca622977>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m## 데이터를 불러옵니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/fra.txt'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim = 128\n",
    "num_samples = 1000\n",
    "data_path = './data/fra.txt'\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_chars = set()\n",
    "target_chars = set()\n",
    "\n",
    "## 데이터를 불러옵니다.\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "## num_samples개수의 데이터에 대해서 \"\\t\" 기호를 바탕으로 토큰화합니다. \n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # TODO: target_text에 대해서 (앞)tap(\\t) 과 (뒤)'\\n' 추가하여 right shifted 되도록 합니다. #전체 하나씩 돌면서 #list를 씌워주면 token단위로 뽑아진다. 즉 분석 단위가 바귐. \n",
    "    target_text = '\\t' + target_text +'\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    #input_text의 글자(char)에 대해서 input_characters에 없다면 추가해줍니다. \n",
    "    for char in input_text:\n",
    "        if char not in input_chars:\n",
    "            input_chars.add(char)\n",
    "    #target_text의 글자(char)에 대해서 target_characters에 없다면 추가해줍니다.\n",
    "    for char in target_text:\n",
    "        if char not in target_chars:\n",
    "            target_chars.add(char)\n",
    "\n",
    "# Characteristic 수준에서의 토큰화\n",
    "\n",
    "input_chars = sorted(list(input_chars))\n",
    "target_chars = sorted(list(target_chars))\n",
    "enc_num_tokens = len(input_chars)  # input의 character별로 모아놓은 것의 token 개수(len)\n",
    "dec_num_tokens = len(target_chars) # target의 character별로 모아놓은 것의 token 개수(len)\n",
    "#(인코더, 디코더) 입력 문장의 최대 길이를 입력 시퀀스의 최대 길이로 설정합니다. \n",
    "enc_max_len = max([len(text) for text in input_texts])\n",
    "dec_max_len = max([len(text) for text in target_texts])\n",
    "\n",
    "print('샘플의 개수:', len(input_texts))\n",
    "print('중복되지 않는 입력의 개수:', enc_num_tokens)\n",
    "print('중복되지 않는 출력의 개수:', dec_num_tokens)\n",
    "print('입력 시퀀스의 최대 길이:', enc_max_len)\n",
    "print('출력 시퀀스의 최대 길이:', dec_max_len)\n",
    "\n",
    "#입력과 타겟 데이터의 {'token':index } 딕셔너리를 생성합니다.\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_chars)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_chars)])\n",
    "\n",
    "#TODO: (입력 데이터 수, 입력 최대 길이, 토큰 수) 크기의 값이 0인 배열을 생성합니다.(초기화) #여기 다 디코더에 관한 것.\n",
    "enc_input_data = np.zeros((len(input_texts),enc_max_len, enc_num_tokens), dtype='float32') #초기값이라서 zero를 넣은 것.\n",
    "dec_input_data = np.zeros((len(input_texts),dec_max_len, dec_num_tokens),dtype='float32')\n",
    "dec_target_data = np.zeros((len(input_texts),dec_max_len,dec_num_tokens),dtype='float32')\n",
    "\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    #enc_input_data > (data length, sequence length, total_word)\n",
    "    for t, char in enumerate(input_text):\n",
    "        enc_input_data[i, t, input_token_index[char]] = 1.\n",
    "    \n",
    "    #dec_input_data > (data length, sequence length, total_word)\n",
    "    for t, char in enumerate(target_text):\n",
    "        dec_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            dec_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "            \n",
    "\n",
    "#인코더의 입력층을 설정하여 줍니다. \n",
    "enc_inputs = Input(shape=(None, enc_num_tokens))\n",
    "# TODO: LSTM층을 변수 encoder에 저장합니다.(return_state=True / latent_dim 활용) #LSTM위에 정의되어있음.\n",
    "encoder = LSTM(latent_dim,return_state=True) # 우리가 정한 max length의 sequence 차원의 vector를 그대로 출력하느냐 아니면 하나의 값만 출력하느냐/ #return_state=True->hidden state값을 출력하느냐 마느냐\n",
    "enc_outputs, state_h, state_c = encoder(enc_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#디코더의 입력층을 설정하여 줍니다.\n",
    "dec_inputs = Input(shape=(None, dec_num_tokens))\n",
    "# TODO: LSTM층을 변수 decoder에 저장합니다.(return_state=True, return_sequences=True / latent_dim 활용) \n",
    "dec_lstm = LSTM(latent_dim,return_state=True, return_sequences=True )\n",
    "\n",
    "# TODO:inital_state를 인코더의 states로 설정하고 디코더의 입력값을 바탕으로 lstm 연산을 수행하는 층을 만들어 줍니다.\n",
    "dec_outputs, _, _ = dec_lstm(dec_inputs, initial_state=encoder_states)\n",
    "dec_dense = Dense(dec_num_tokens, activation='softmax')\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "\n",
    "# 인코더와 디코더를 바탕으로 모델을 선언합니다.\n",
    "model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
    "\n",
    "# 모델의 파라미터를 설정하고 학습을 진행합니다.\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit([enc_input_data, dec_input_data], dec_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "enc_model = Model(enc_inputs, encoder_states)\n",
    "\n",
    "dec_state_input_h = Input(shape=(latent_dim,))\n",
    "dec_state_input_c = Input(shape=(latent_dim,))\n",
    "dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
    "dec_outputs, state_h, state_c = dec_lstm(dec_inputs, initial_state=dec_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "decoder_model = Model(\n",
    "    [dec_inputs] + dec_states_inputs,\n",
    "    [dec_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 인코더 모델을 바탕으로 input_seq에 대한 state_value를 예측합니다.\n",
    "    states_value = enc_model.predict(input_seq)\n",
    "    # (1,1, 디코더 토큰 개수) 크기를 가진 값이 0인 배열을 만들어 줍니다.\n",
    "    seq_target = np.zeros((1, 1, dec_num_tokens))\n",
    "    seq_target[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    _stop = False\n",
    "    _decoded = ''\n",
    "    while not _stop:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [seq_target] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        _decoded += sampled_char\n",
    "        # 디코딩된 글자의 수가 dec_max_len을 넘거나 그 글자가 개행('\\n')이면 _stop을 True로 활성화시킵니다. \n",
    "        if (sampled_char == '\\n' or\n",
    "           len(_decoded) > dec_max_len):\n",
    "            _stop = True\n",
    "        seq_target = np.zeros((1, 1, dec_num_tokens))\n",
    "        seq_target[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return _decoded\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = enc_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습 24-2] 어텐션 매커니즘(Attention Mechanism)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션 매커니즘\n",
    "어텐션 매커니즘(Attention Mechanism)은 기존의 가중치(weight)와 달리 데이터의 특징 또는 형태를 반영하여 도출한 어텐션 가중치(attention weight)를 바탕으로 일련의 연산을 수행하는 것을 의미합니다.\n",
    "\n",
    "어텐션 매커니즘은 그 절차 및 방식에 따라, Additive Attention과 Dot-product Attention으로 나누어 볼 수 있습니다.\n",
    "\n",
    "이번 시간에는 Additive Attention을 코드를 통해 구현해보고 그 특징을 이해해 봅시다.\n",
    "\n",
    "# 실습\n",
    "1. _value, _query에 대해 같은 출력을 내보내는 FC 레이어를 생성합니다.\n",
    "\n",
    "2. _query의 차원을 추가시킵니다.(axis=1)\n",
    "\n",
    "3. softmax함수를 통과한 attention value 값을 attention_weight에 저장합니다.\n",
    "\n",
    "4. attention_weight과 _value를 곱한 값의 요소별 평균(axis=1)을 context_vector에 저장합니다.\n",
    "\n",
    "5. 간단한 차원의 값들을 조정해가며 어텐션 매커니즘을 직관적으로 이해해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------case1--------------------------\n",
      "어텐션 가중치:\n",
      "[[[0.33333334]\n",
      "  [0.33333334]\n",
      "  [0.33333334]]]\n",
      "컨텍스트 벡터:\n",
      "[[0.]]\n",
      "--------------------------case2--------------------------\n",
      "어텐션 가중치:\n",
      "[[[0.33333334]\n",
      "  [0.33333334]\n",
      "  [0.33333334]]]\n",
      "컨텍스트 벡터:\n",
      "[[0.]]\n",
      "--------------------------case3--------------------------\n",
      "어텐션 가중치:\n",
      "[[[0.25081095]\n",
      "  [0.49837807]\n",
      "  [0.25081095]]\n",
      "\n",
      " [[0.25081095]\n",
      "  [0.49837807]\n",
      "  [0.25081095]]]\n",
      "컨텍스트 벡터:\n",
      "[[0.49837807]\n",
      " [0.49837807]]\n",
      "--------------------------case4--------------------------\n",
      "어텐션 가중치:\n",
      "[[[0.2632927 ]\n",
      "  [0.4734145 ]\n",
      "  [0.2632927 ]]\n",
      "\n",
      " [[0.23611811]\n",
      "  [0.5277638 ]\n",
      "  [0.23611811]]]\n",
      "컨텍스트 벡터:\n",
      "[[0.4734145]\n",
      " [0.5277638]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def attention(_query, _value):\n",
    "    ACT='tanh'\n",
    "    weights={\n",
    "        # TODO: _value, _query에 대해 같은 출력을 내보내는 FC 레이어를 생성합니다.\n",
    "        'value' : tf.keras.layers.Dense(10, activation=ACT), #value랑query값이 같게 맞춰. 더해질 때 에러안 나게.\n",
    "        'query' : tf.keras.layers.Dense(10, activation=ACT),\n",
    "        'projection' : tf.keras.layers.Dense(1, activation=ACT)\n",
    "        }\n",
    "    # TODO: _query의 차원을 추가시킵니다.(axis=1)\n",
    "    _query = tf.expand_dims(_query,1)\n",
    "\n",
    "    attention_value = weights['projection'](weights['value'](_value)+weights['query'](_query))\n",
    "\n",
    "    # TODO: softmax함수를 통과한 attention value 값을 attention_weight에 저장합니다.\n",
    "    attention_weight = tf.nn.softmax(attention_value,axis=1)\n",
    "    # TODO: attention_weight과 _value를 곱한 값의 요소별 평균(axis=1)을 context_vector에 저장합니다.\n",
    "    context_vector = tf.reduce_sum(attention_weight*_value,axis=1)\n",
    "    \n",
    "    return attention_weight, context_vector\n",
    "    \n",
    "def print_fn(attention_weight, context_vector):\n",
    "    print(\"어텐션 가중치:\")\n",
    "    print(attention_weight.numpy())\n",
    "    print(\"컨텍스트 벡터:\")\n",
    "    print(context_vector.numpy())\n",
    "\n",
    "#TODO: 간단한 차원의 값들을 조정해가며 어텐션 매커니즘을 직관적으로 이해해봅시다.\n",
    "\n",
    "print(\"--------------------------case1--------------------------\")\n",
    "_value = tf.constant([[[0],[0],[0]]], dtype = tf.float32)\n",
    "_query = tf.constant([[0]], dtype = tf.float32)\n",
    "\n",
    "attention_weight, context_vector = attention(_query, _value) \n",
    "print_fn(attention_weight, context_vector)\n",
    "\n",
    "print(\"--------------------------case2--------------------------\")\n",
    "_value = tf.constant([[[0],[0],[0]]], dtype = tf.float32)\n",
    "_query = tf.constant([[0, 0, 0]], dtype = tf.float32)\n",
    "\n",
    "attention_weight, context_vector = attention(_query, _value) \n",
    "print_fn(attention_weight, context_vector)\n",
    "\n",
    "print(\"--------------------------case3--------------------------\")\n",
    "_value = tf.constant([[[0],[1],[0]]], dtype = tf.float32)\n",
    "_query = tf.constant([[0],[0]], dtype = tf.float32)\n",
    "\n",
    "attention_weight, context_vector = attention(_query, _value) \n",
    "print_fn(attention_weight, context_vector)\n",
    "\n",
    "print(\"--------------------------case4--------------------------\")\n",
    "_value = tf.constant([[[0],[1],[0]]], dtype = tf.float32)\n",
    "_query = tf.constant([[1],[0]], dtype = tf.float32)\n",
    "\n",
    "attention_weight, context_vector = attention(_query, _value) \n",
    "print_fn(attention_weight, context_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습 24-3] 어텐션을 활용한 신경망 기계 번역(Neural Machine Translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 기계 번역\n",
    "  신경망 기계 번역(Neural Machine Translation)은 서로 다른 언어의 문장에 대한 번역을 인공 신경망 알고리즘을 통해 구현한 것을 의미합니다.\n",
    "\n",
    "   이번 시간에는 지난 시간에 알아본 Seq2Seq 모델과 Attention 알고리즘을 이용하여 간단한 신경망 기계번역을 구현하여 봅시다.\n",
    "\n",
    "   데이터 셋은 영어(English)-스페인어(Spanish) 번역 데이터입니다.\n",
    "\n",
    "(실습은 텐서 플로우 공식 튜토리얼을 바탕으로 작성되었습니다.)\n",
    "\n",
    "# 실습\n",
    "1. GRU 신경망을 Keras를 이용하여 설정한 인코더 유닛만큼 self.gru에 선언합니다.(return_sequences=True, return_state=True)\n",
    "\n",
    "2. GRU의 입력을 x로, 초기 상태를 hidden으로 설정합니다.\n",
    "\n",
    "3. values를 W1에 _hidden을 W2에 연산을 적용한 후, 이를 V에 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/input.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6ac949e260e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#데이터와 토크나이저를 불러옵니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0m_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mlan_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlan_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6ac949e260e2>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'input.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0m_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'label.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/input.pickle'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle, os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_PATH = './data/'\n",
    "BATCH_SIZE = 32\n",
    "EMD_DM = 128\n",
    "UNITS = 128\n",
    "EPOCHS = 1\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path + 'input.pickle', 'rb') as handle:\n",
    "        _input = pickle.load(handle)\n",
    "    with open(path + 'label.pickle', 'rb') as handle:\n",
    "        _label = pickle.load(handle)\n",
    "    return _input, _label\n",
    "\n",
    "def load_tokenizer(path):\n",
    "    with open(path + 'tokenizer_input.pickle', 'rb') as handle:\n",
    "        _input = pickle.load(handle)\n",
    "    with open(path + 'tokenizer_target.pickle', 'rb') as handle:\n",
    "        _target = pickle.load(handle)\n",
    "    return _input, _target\n",
    "\n",
    "#데이터와 토크나이저를 불러옵니다.\n",
    "_input, _label = load_data(DATA_PATH)\n",
    "lan_input, lan_target = load_tokenizer(DATA_PATH)\n",
    "\n",
    "input_train, input_test, label_train, label_test = train_test_split(_input, _label, test_size=.2)\n",
    "\n",
    "BUFFER_SIZE = len(input_train)\n",
    "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
    "SIZE_INPUT = len(lan_input.word_index) + 1\n",
    "SIZE_TARGET = len(lan_target.word_index) + 1\n",
    "\n",
    "# tf data를 사용하여 데이터셋을 다루어 봅니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "#인코더를 정의하는 클래스 입니다.\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, EMD_DM, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, EMD_DM)\n",
    "        # TODO: GRU 신경망을 Keras를 이용하여 설정한 인코더 유닛만큼 self.gru에 선언합니다.(return_sequences=True, return_state=True)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        # TODO: GRU의 입력을 x로, 초기 상태를 hidden으로 설정합니다. \n",
    "        output, state = self.gru(x)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "#어텐션을 정의하는 클래스 입니다.\n",
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, UNITS):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(UNITS)\n",
    "        self.W2 = tf.keras.layers.Dense(UNITS)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):       \n",
    "        _hidden = tf.expand_dims(query, 1)\n",
    "        # TODO: values를 W1에 _hidden을 W2에 연산을 적용한 후, 이를 V에 적용합니다. \n",
    "        score = self.V(tf.nn.tanh(self.W1( values) + self.W2(_hidden)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# 디코더를 정의하는 클래스 입니다.\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, EMD_DM, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, EMD_DM)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = Attention(self.dec_units)\n",
    "\n",
    "    def call(self, _x, hidden, out_encoder):\n",
    "        context_vector, attention_weights = self.attention(hidden, out_encoder)\n",
    "        _x = self.embedding(_x)\n",
    "        _x = tf.concat([tf.expand_dims(context_vector, 1), _x], axis=-1)\n",
    "        output, state = self.gru(_x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "#손실함수를 정의하는 함수입니다.\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "def train_step(input_, target_, hidden_encoder):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output_encoder, hidden_encoder = encoder(input_, hidden_encoder)\n",
    "        hidden_decoder = hidden_encoder\n",
    "        input_decoder = tf.expand_dims([lan_target.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        for t in range(1, target_.shape[1]):\n",
    "            predictions, hidden_decoder, _ = decoder(input_decoder, hidden_decoder, output_encoder)\n",
    "            loss += loss_function(target_[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(target_[:, t], 1)\n",
    "\n",
    "    loss_batch = (loss / int(target_.shape[1]))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss_batch\n",
    "\n",
    "\n",
    "encoder = Encoder(SIZE_INPUT, EMD_DM, UNITS, BATCH_SIZE)\n",
    "decoder = Decoder(SIZE_TARGET, EMD_DM, UNITS, BATCH_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# 학습을 진행합니다.\n",
    "for epoch in range(EPOCHS):\n",
    "    hidden_encoder = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    for (batch, (input_data, target_data)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "\n",
    "        batch_loss = train_step(input_data, target_data, hidden_encoder)\n",
    "        total_loss += batch_loss\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1, batch, batch_loss.numpy()))\n",
    "    print(\"[EPOCH - {}] Total Loss: {}\".format(epoch+1, total_loss/steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습 24-4] 간단한 챗봇(Chat-bot) 만들기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 챗봇 만들기\n",
    "이번 시간에는 시퀀스 모델이 가장 많이 활용되는 분야 중 하나인 챗봇(Chat-bot)에 대해 살펴보도록 하겠습니다.\n",
    "\n",
    "영어 자연어처리 관련 라이브러리인 nltk를 활용하여 간단한 응답을 출력하는 모델을 만들어 봅니다.\n",
    "\n",
    "# 실습\n",
    "1. ‘!\"#$%&’()+, -./:;<=>?@[]^{|}~’ 를 딕셔너리 remove_punct_dict로 저장합니다.(string.punctuation 활용)\n",
    "\n",
    "2. TfidfVectorizer를 생성합니다.\n",
    "\n",
    "3. tfidf가 적용된 user_responce (tfidf[-1])과 전체 값을 바탕으로 코사인 유사도를 구합니다.\n",
    "\n",
    "4. GREETING_INPUTS 에 있는 값을 입력했을 때, GREETING_RESPONSES 이 나오는지 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yein4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yein4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/chatbot.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2ab13b8a2113>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0msent_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mword_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-2ab13b8a2113>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/chatbot.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/chatbot.txt'"
     ]
    }
   ],
   "source": [
    "import io, random, string \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True) \n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet') \n",
    "\n",
    "def load_data():\n",
    "    with open('./data/chatbot.txt','r', encoding='utf8', errors ='ignore') as fin:\n",
    "        raw = fin.read().lower()\n",
    "    return raw\n",
    "    \n",
    "raw = load_data()\n",
    "sent_tokens = nltk.sent_tokenize(raw) \n",
    "word_tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "# WordNetLemmatizer를 선언합니다.\n",
    "lemmer = WordNetLemmatizer() #먹었습니다를 받아서 먹었다 라고 즉 기본형으로 바꿔주는 것.\n",
    "LemTokens = lambda tokens: [lemmer.lemmatize(token) for token in tokens]#토큰을 받아서\n",
    "LemNormalize = lambda text: LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict))) #불용어 제거\n",
    "\n",
    "# TODO: '!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~'를 딕셔너리 remove_punct_dict로 저장합니다.(string.punctuation 활용)#string은 내장함수 # string에서 for문이돌면 chararcter가 하나씩 돌기 때문에 character하나씩 나와서 숫자로 바꿔서 dictionary에 저장하고\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation )\n",
    "\n",
    "\n",
    "# 키워드 매칭에 사용될 입력과 응답입니다.(테스트 시, 여기있는 입력값을 입력하면 대응되는 출력값이 출력됩니다.) \n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\",\"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    # 인사말이 입력되면, 그에 맞는 응답을 랜덤하게 반환합니다.\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "\n",
    "# 응답(Response)을 생성하는 함수입니다.\n",
    "def response(user_response):\n",
    "    robot_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    #TODO: TfidfVectorizer를 생성합니다.(LemNormalize 활용)\n",
    "    TfidfVec = TfidfVectorizer(LemNormalize)\n",
    "    # sent_token을 Tfidf 값으로 변환합니다.\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    # TODO: tfidf화 된 user_responce (tfidf[-1])과 전체 값을 바탕으로 코사인 유사도를 구합니다.\n",
    "    vals = cosine_similarity(user_response,dense_output=True)\n",
    "    \n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if(req_tfidf==0):\n",
    "        robot_response = robot_response+\"I am sorry! I don't understand you\"\n",
    "    else:\n",
    "        robot_response = robot_response+sent_tokens[idx]\n",
    "    \n",
    "    return robot_response\n",
    "\n",
    "# 학습을 진행합니다. #실제 실행창\n",
    "flag=True\n",
    "print(\"ROBOT: If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBOT: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBOT: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBOT: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [미션 24] 신경망 기계 번역의 성능 높이기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 미션! 신경망 기계번역 성능 높이기\n",
    "이번 시간에는 지난 [실습 24-3]에서 구현했던 신경망 기계번역 모델을 수정하여 그 성능을 높여보도록 합시다.\n",
    "\n",
    "옆의 코드는 [실습 24-3]의 코드를 그대로 옮겨 놓았습니다. 해당 실습에서 완성하였던 코드를 옮겨 적으신 후, 모델 내부의 수정을 통해 총 손실값(total_loss/steps_per_epoch)가 2.3 이하가 되도록 해보세요.\n",
    "\n",
    "(모델 수정은 지금까지 배웠던 여러 신경망들을 더하거나 파라미터, 하이퍼파라미터를 변경함으로써 달성할 수 있습니다.)\n",
    "\n",
    "# 미션\n",
    "1. [실습 24-3] 에서 빈칸을 채운 모델을 복사하여 옮겨줍니다.\n",
    "\n",
    "2. Encoder, Decoder, Attention 클래스 또는 파라미터를 수정하여 total_loss가 2.3이하가 되도록 합니다.(EPOCHS=1로 고정합니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/input.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4c6c80c63004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#데이터와 토크나이저를 불러옵니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0m_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mlan_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlan_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-4c6c80c63004>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'input.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0m_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'label.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/input.pickle'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle, os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_PATH = './data/'\n",
    "BATCH_SIZE = 32\n",
    "EMD_DM = 128\n",
    "UNITS = 128\n",
    "EPOCHS = 1\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path + 'input.pickle', 'rb') as handle:\n",
    "        _input = pickle.load(handle)\n",
    "    with open(path + 'label.pickle', 'rb') as handle:\n",
    "        _label = pickle.load(handle)\n",
    "    return _input, _label\n",
    "\n",
    "def load_tokenizer(path):\n",
    "    with open(path + 'tokenizer_input.pickle', 'rb') as handle:\n",
    "        _input = pickle.load(handle)\n",
    "    with open(path + 'tokenizer_target.pickle', 'rb') as handle:\n",
    "        _target = pickle.load(handle)\n",
    "    return _input, _target\n",
    "\n",
    "#데이터와 토크나이저를 불러옵니다.\n",
    "_input, _label = load_data(DATA_PATH)\n",
    "lan_input, lan_target = load_tokenizer(DATA_PATH)\n",
    "\n",
    "input_train, input_test, label_train, label_test = train_test_split(_input, _label, test_size=.2)\n",
    "\n",
    "BUFFER_SIZE = len(input_train)\n",
    "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
    "SIZE_INPUT = len(lan_input.word_index) + 1\n",
    "SIZE_TARGET = len(lan_target.word_index) + 1\n",
    "\n",
    "# tf data를 사용하여 데이터셋을 다루어 봅니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "#인코더를 정의하는 클래스 입니다.\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, EMD_DM, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, EMD_DM)\n",
    "        # TODO: GRU 신경망을 Keras를 이용하여 설정한 인코더 유닛만큼 self.gru에 선언합니다.(return_sequences=True, return_state=True)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        # TODO: GRU의 입력을 x로, 초기 상태를 hidden으로 설정합니다. \n",
    "        output, state = self.gru(x)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "#어텐션을 정의하는 클래스 입니다.\n",
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, UNITS):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(UNITS)\n",
    "        self.W2 = tf.keras.layers.Dense(UNITS)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):       \n",
    "        _hidden = tf.expand_dims(query, 1)\n",
    "        # TODO: values를 W1에 _hidden을 W2에 연산을 적용한 후, 이를 V에 적용합니다. \n",
    "        score = self.W1( values)*self.W2(_hidden)\n",
    "        #self.V(self.W1( values)*self.W2(_hidden))\n",
    "        #self.V(self.W1( values) +self.W2(_hidden))\n",
    "        #self.V(tf.nn.tanh(self.W1( values) + self.W2(_hidden)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# 디코더를 정의하는 클래스 입니다.\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, EMD_DM, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, EMD_DM)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = Attention(self.dec_units)\n",
    "\n",
    "    def call(self, _x, hidden, out_encoder):\n",
    "        context_vector, attention_weights = self.attention(hidden, out_encoder)\n",
    "        _x = self.embedding(_x)\n",
    "        _x = tf.concat([tf.expand_dims(context_vector, 1), _x], axis=-1)\n",
    "        output, state = self.gru(_x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "#손실함수를 정의하는 함수입니다.\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "def train_step(input_, target_, hidden_encoder):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output_encoder, hidden_encoder = encoder(input_, hidden_encoder)\n",
    "        hidden_decoder = hidden_encoder\n",
    "        input_decoder = tf.expand_dims([lan_target.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        for t in range(1, target_.shape[1]):\n",
    "            predictions, hidden_decoder, _ = decoder(input_decoder, hidden_decoder, output_encoder)\n",
    "            loss += loss_function(target_[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(target_[:, t], 1)\n",
    "\n",
    "    loss_batch = (loss / int(target_.shape[1]))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss_batch\n",
    "    \n",
    "    \n",
    "encoder = Encoder(SIZE_INPUT, EMD_DM, UNITS, BATCH_SIZE)\n",
    "decoder = Decoder(SIZE_TARGET, EMD_DM, UNITS, BATCH_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "def main():\n",
    "    total_score=0\n",
    "    for epoch in range(EPOCHS):\n",
    "        hidden_encoder = encoder.initialize_hidden_state()\n",
    "        total_loss = 0\n",
    "        for (batch, (input_data, target_data)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "\n",
    "            batch_loss = train_step(input_data, target_data, hidden_encoder)\n",
    "            total_loss += batch_loss\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "        print(\"[EPOCH - {}] Total Loss: {}\".format(epoch + 1, total_loss/steps_per_epoch))\n",
    "        total_score=total_loss/steps_per_epoch\n",
    "    return total_score\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
