---
title: "Speculative DecodingðŸš€ (ìž‘ì„± ì¤‘)"
category: LLM / Multimodal
tag: Multimodal
---







* ëª©ì°¨
{:toc}











ìµœê·¼ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” LLMë“¤ì€ Auto-regressive modelë¡œ, Transformers Decoder êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤. Auto-regressive modelì€ ìƒˆë¡œìš´ tokenì„ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•œë‹¤. ì¦‰, ì—¬ëŸ¬ tokenì´ ë³‘ë ¬ì ìœ¼ë¡œ ì˜ˆì¸¡ë˜ì§€ ì•ŠëŠ”ë‹¤. í˜„ìž¬ time stepì˜ tokenì´ ìžˆì–´ì•¼ ê·¸ ë‹¤ìŒ tokenì„ ì˜ˆì¸¡í•˜ë‹ˆê¹Œ ë‹¹ì—°í•œ ì´ì•¼ê¸°ì´ë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ output tokenì´ ê¸¸ ìˆ˜ë¡ ìƒì„± ì†ë„ê°€ ë§¤ìš° ëŠë ¤ì§„ë‹¤ëŠ” ë¬¸ì œê°€ ìžˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ìž Speculative Decodingì´ ì œì•ˆë˜ì—ˆë‹¤. 

# 1. Operating Process

Speculative Decodingì€ **main model**ê³¼ ê·¸ì— ë¹„í•´ ë¹„êµì  ìž‘ì€ sizeì˜ **assistant model(draft model)**ì´ í•„ìš”í•˜ë‹¤. ê¸°ë³¸ ê³¼ì •ì€ ì•„ëž˜ì™€ ê°™ë‹¤:

1. assistant modelì´ nê°œì˜ tokenìœ¼ë¡œ ì´ë£¨ì–´ì§„ sequenceë¥¼ ìƒì„±
2. main modelì´ single forward passë¡œ  assistant modelì´ ìƒì„±í•œ sequenceë¥¼ ê²€í† 
3. main modelì€ ê²€í†  ê³¼ì •ì—ì„œ sequenceë¥¼ ìˆœë°©í–¥ìœ¼ë¡œ ì²´í¬í•˜ë©° ìž˜ëª»ëœ tokenì„ ì°¾ì•„ë‚¸ë‹¤.
4. sequence ë‚´ ìž˜ëª»ëœ token ì´í›„ì˜ tokenë“¤ì„ ëª¨ë‘ ë²„ë¦¬ê³ , main modelì´ auto-regressiveí•˜ê²Œ ë‚˜ë¨¸ì§€ sequenceë¥¼ ë‹¤ì‹œ ì±„ì›Œ ë‚˜ê°„ë‹¤.

ì´ëŸ¬í•œ ê³¼ì •ì€ í¬ê²Œ ë‘ ê°œë…ì„ ë°”íƒ•ìœ¼ë¡œ í•œë‹¤:

- ìž‘ì€ sizeì˜ ëª¨ë¸ì€ inference ì†ë„ê°€ ë¹ ë¥´ë‹¤
- ë¹„êµì  í° sizeì˜ main modelì´ zero-baseì—ì„œ ìƒì„±í•˜ëŠ” ê²ƒë³´ë‹¤ í•˜ë‚˜ì”© ì½ìœ¼ë©° ê²€í† í•˜ëŠ” ì†ë„ê°€ ë” ë¹ ë¥´ë‹¤ (ì¸ê°„ë„ ê¸€ì„ ì“°ëŠ” ì‹œê°„ë³´ë‹¤ ì½ëŠ” ì‹œê°„ì´ ë¹ ë¥¸ ê²ƒì²˜ëŸ¼)
- ì–´ì°¨í”¼ main modelì´ ê²€í† í•˜ë©° ìžì‹ ì´ ì¼ì„ë§Œí•œ tokenê¹Œì§€ëŠ” ìˆ˜ìš©í•˜ê³  ê·¸ë ‡ì§€ ì•Šë‹¤ê³  íŒë‹¨ëœ tokenë¶€í„°ëŠ” ë³¸ì¸ì´ ì§ì ‘ ìƒì„±í•˜ë‹ˆ ê²°ê³¼ì ìœ¼ë¡œ main modelë§Œ ì‚¬ìš©í–ˆì„ ë•Œì™€ ì•„ì£¼ ìœ ì‚¬í•œ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤. (ìƒì„± ëª¨ë¸ì˜ ê²°ê³¼ëŠ” ì¼ì •í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— "ì•„ì£¼ ìœ ì‚¬í•œ ê²°ê³¼"ë¼ê³  í‘œí˜„)

# 2. Note

Speculative Decoding ì ìš© ì‹œ, ìœ ì˜í•´ì•¼ í•  ì ì´ ëª‡ ê°€ì§€ ìžˆë‹¤.

1. model ì„ íƒ
   
  ì ì ˆí•œ main modelê³¼ assistant model ì„ ì •ì´ ì¤‘ìš”í•˜ë‹¤. LLMë“¤ì´ spec-decì— ì ìš©í•˜ì˜€ì„ ë•Œ ì„œë¡œ ìž˜ í˜¸í™˜ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ë‘ ëª¨ë¸ ì„ íƒ ì‹œ ì¤‘ìš”í•œ ê²ƒì€ vocabì´ë‹¤. ë™ì¼í•œ vocabì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ë§Œ spec-decì— ì ìš© ê°€ëŠ¥í•˜ë‹¤. (ì„œë¡œ ë‹¤ë¥¸ neural architectureì™€ tokenizer ì¡°í•©ë„ ê°€ëŠ¥í•˜ì§€ë§Œ ë¹„íš¨ìœ¨ì ì´ë¼ê³  í•œë‹¤.)

  ì˜ˆë¥¼ ë“¤ì–´, LLaMa 2ëŠ” 32Kì˜ vocabì„ ì‚¬ìš©í•˜ê³  LLaMa 3ì€ 128Kì˜ vocabì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë‘ ëª¨ë¸ì€ spec-decì— ì ìš©í•  ìˆ˜ ì—†ë‹¤.
  
2. assistant model size

   spec-decì€ assistant modelì´ ëŒ€ë¶€ë¶„ì˜ ìž‘ì—…ì„ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— assistant modelì´ ë¹ ë¥¼ìˆ˜ë¡ ìµœì¢… ê²°ê³¼ ë°˜í™˜ ì†ë„ê°€ ë¹¨ë¼ì§„ë‹¤. ë”°ë¼ì„œ assistant modelì˜ parameter sizeê°€ ìž‘ì•„ token ìƒì„± ì†ë„ê°€ ë¹ ë¥´ë©´ì„œ ì •í™•ë„ë„ ë†’ì€ ëª¨ë¸ë¡œ ìž˜ ì„ ì •í•´ì•¼ í•˜ëŠ” ê²ƒì´ë‹¤.

3. main model acceptance rate

   main modelì´ assistant modelì´ ìƒì„±í•œ tokenì„ ìž˜ ìˆ˜ìš©í•´ ì¤˜ì•¼ í•œë‹¤.

   ë§Œì•½ assistant modelì´ ìƒì„±í•œ ê²°ê³¼ë¥¼ main modelì´ ëŒ€ë¶€ë¶„ ê±°ë¶€í•˜ì—¬ ëŒ€ë¶€ë¶„ì˜ tokenì„ main modelì´ ë‹¤ì‹œ ìƒì„±í•œë‹¤ë©´ spec-decì„ ì ìš©í•œ ê²ƒì´ ë¬´ì˜ë¯¸í•´ì§„ë‹¤. ì´ë ‡ê²Œ ë  ê²½ìš° TPS(Token Per Second) ì „ì²´ ìƒì„± ì‹œê°„ì— ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ ëœë‹¤.

   ë”°ë¼ì„œ ìˆ˜ìš©ë¥ ì´ ë†’ì€ main modelì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.

   LLaMa 3.1ì˜ ê²½ìš° 8Bì— ëŒ€í•´ 70Bì˜ acceptance rateê°€ 70%ë¼ê³  í•œë‹¤.
   

# 3. Speculative Decoding code
ìµœê·¼ ë‹¤ì–‘í•œ LLM Inference Engineë“¤ì´ ë‚˜ì˜¨ëŠ”ë° ëŒ€ë¶€ë¶„ Speculative Decodingì„ ì§€ì›í•œë‹¤.

## 3.1 huggingface generate

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, BitsAndBytesConfig
from pynvml import *
import time

def print_gpu_utilization():
    nvmlInit()
    handle = nvmlDeviceGetHandleByIndex(0)
    info = nvmlDeviceGetMemoryInfo(handle)
    print(f"GPU memory occupied: {info.used//1024**2} MB.")

set_seed(42)  # For reproducibility

checkpoint = "mistralai/Mixtral-8x7B-v0.1"
assistant_checkpoint = "mistralai/Mistral-7B-v0.1"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
compute_dtype = getattr(torch, "float16")
bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map="cuda", quantization_config=bnb_config)


prompt = []
prompt.append("Tell me about gravity.")
prompt.append("What is AI?")
prompt.append("Write an essay about intelligence.")
prompt.append("Cite 20 famous people.")
prompt.append("Give me the recipe for the best chicken curry.")

duration = 0.0
total_length = 0

assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint, device_map="cuda", quantization_config=bnb_config)

for i in range(len(prompt)):
  model_inputs = tokenizer(prompt[i], return_tensors="pt").to("cuda:0")
  start_time = time.time()
  output = model.generate(**model_inputs, assistant_model=assistant_model, max_length=500)[0] # assistant model ì§€ì •í•´ì¤€ë‹¤.
  duration += float(time.time() - start_time)
  total_length += len(output)
  tok_sec_prompt = round(len(output)/float(time.time() - start_time),3)
  print("Prompt --- %s tokens/second ---" % (tok_sec_prompt))
  print(print_gpu_utilization())

tok_sec = round(total_length/duration,3)
print("Average --- %s tokens/second ---" % (tok_sec))
```

## 3.2 vllm
  
```python
# vllm==0.5.4

from vllm import SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
import uuid
import asyncio

# === model prepare ================================================
model_args = AsyncEngineArgs(
            model="meta-llama/Meta-Llama-3.1-70B-Instruct", # main model
            speculative_model="meta-llama/Meta-Llama-3.1-8B-Instruct", # assistant model
            trust_remote_code=True,
            tensor_parallel_size=4, # ì§ìˆ˜ê°œë¡œ ë„£ì–´ì•¼ ìž‘ë™(GPUê°€ 3ê°œë©´ 2ë¡œ ì§€ì •)
            max_num_seqs=8,
            dtype="half", # FP16 bit precisionìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ load. full FP32ë³´ë‹¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ˆë°˜ìœ¼ë¡œ ì¤„ì–´ë“¦.
            use_v2_block_manager=True, # True ì•„ë‹ˆë©´ error ëœ¸
            enforce_eager=True # True ì•„ë‹ˆë©´ error ëœ¸
        )
# + 70B ëª¨ë¸ì€ ì•½ 140GBì˜ VRAMì„ ì°¨ì§€

llm_engine = AsyncLLMEngine.from_engine_args(model_args)

# === inference basic ================================================

prompt = "ë°”ë‹¤ê°€ íŒŒëž€ìƒ‰ì¸ ì´ìœ "
stream = True
max_tokens = 1024
model_input = {"max_tokens": max_tokens}
sampling_params = SamplingParams(**model_input)

idx = str(uuid.uuid4().hex)
vllm_generator = llm_engine.generate(prompt, sampling_params, idx)

# === inference stream ================================================

async def stream_tokens():
    full_text = ""
    async for request_output in llm_engine.generate(prompt, sampling_params, idx):
        if len(request_output.outputs) > 0:
            text = request_output.outputs[0].text
            delta = text[len(full_text):]
            full_text = text
            print(delta, end='', flush=True)
    print()

await stream_tokens()
```

  - vllm + Speculative Decoding Performance
 


# Reference 

> Fast Inference from Transformers via Speculative Decoding
