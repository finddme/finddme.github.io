---
title: "PEFT : Parameter-Efficient Finetuning methods "
category: LLM / Multimodal
tag: NLP
---







* 목차
{:toc}







Full Fine-Tuning에는 데이터, 컴퓨터 자원, 시간 등 대규모 자원이 요구되는 문제가 있다. Large Language Model이 많이 나오며 이렇게 자원이 많이 요구되는 Full Fine-Tuning을 대체하며 LLM을 효율적으로 학습시킬 수 있는 방법론들이 활발이 연구되고 있다. 해당 연구는 주로 LLM tuning 이후의 성능을 더 좋게 하거나 모델을 보다 빠르게 훈련시킬 수 있도록 하는 방향으로 진행되고 있다. 이와 같은 연구 중 하나인 Parameter-Efficient Fine-Tuning (PEFT)에는 다양한 변gud들이 존재하는데 대부분 상대적으로 적은 dataset으로 model의 일부 parameter를 업데이트하거나 소수의 새로운 파라미터를 추가하는 방법들이다. PEFT의 장점은 크게 세 가지로 볼 수 있다:

- Reduced memory footprint<br>
   PEFT은 model의 파라미터 중 제한된 일부 parameter만 업데이트하고 model의 원래 parameter는 유지시켜 학습 시 요구되는 메모리를 크게 줄일 수 있다.

- Faster training<br>
   훈련 할 parameter, 즉 업데이트되는 parameter의 수가 적기 때문에 자연스럽게 훈련 시간도 단축된다.

# LoRA

Low-Rank Adaptation (LoRA)는 저차원 행렬을 훈련시키는 방법으로, LLM을 빠르고 효과적으로 학습시킬 수 있도록 한다. Transformer 기반의 모델들의 경우 대부분의 parameter가 self attention layer에 있기 때문에 self attention layer weights matrix에 LoRA를 적용하는 것이 효과적이다.

<center><img width="400" src="https://github.com/finddme/finddme.github.io/assets/53667002/87e0634d-0680-48b7-a751-84cd8c098886"></center>
<center><em style="color:gray;">Low-Rank Adaption (LoRA)</em></center><br>

위 이미지처럼 LoRA는 pre-trained LLM layer의 parameter weight matrix $W$를 frozen 시키고 $W$ 외에 "adapter"라 불리는 두 개의 행렬 A와 B를 추가한다. 추가된 두 행렬은 $W$보다 작다. 예를 들어 $W$의 크기가 $d x d$일 때 A와 B의 크기는 $d x r$과 $r x d$이며, $r$($rank$)은 일반적으로 100 이하의 매우 작은 크기이다. $r$의 크기가 클 수록 더 많은 parameter를 학습시키게 된다. 더 많은 parameter를 학습시킨 다는 것은 더 좋은 성능으로 이어질 가능성이 있지만 학습 시간이 길어진다는 단점이 있다. 

학습이 진행 될 때 frozen된 $W$와 B\*A에 동일한 값을 입력한 후 B\*A의 출력을 original matrix $W$의 출력에 추가한다. 즉, 일부 parameter를 학습시키고 그 출력을 original prediction에 더하여 모델에 영향을 주는 방식으로 작동된다. 처음에 행렬 A는 평균이 0인 랜덤 값들로 구성된다(Random values of mean zero). 즉, 랜덤 변수의 값들이 평균적으로 0을 중심으로 분포된다. 그리고 B는 완전히 0으로 초기화된다. 이와 같은 처리를 통해 adpater 행렬이 original matrix $W$의 출력을 완전 초기부터 변경시키지 않도록 한다. A와 B의 parameter가 적절한 방향으로 tuning될 때 A와 B의 출력이 $W$의 출력에 영향을 미치도록 조정하기 위함이다. 

LoRA는 끝쪽에서만 이루어져야 하는 것은 아니고 neural network 내부의 깊은 layer에도 적용 가능하다.

# QLoRA

<center><img width="600" src="https://github.com/finddme/finddme.github.io/assets/53667002/f3978e20-6572-41c7-aa3d-471c9f07d4bb"></center>
<center><em style="color:gray;">Low-Rank Adaption (LoRA)</em></center><br>

QLoRA는 4-bit Quantized language model into Low Rank Adapters의 줄임말로, 양자화를 거친 모델에 대해 LoRA를 활용해 tuning하는 기법을 뜻한다. 계산량 및 메모리 감축을 위해 QLoRA의 저자들은 크게 세 가지 기법을 사용했다.

- 4-bit NormalFloat
  
Pretrained Model의 weight를 4-bit로 양자화시켰을 때 그 데이터 타입을 NormalFloat이라고 한다.

> 일반적으로 컴퓨터는 실수를 32-bit로 floating(fp32)하고 모델들은 보통 FP16으로 경량화되어 사용된다.

- Double Quantized

이 기법은 이중 양자화로, 양자화된 것을 또 양자화하는 것이다. 앞서 4-bit NormalFloat으로 양자화 된 weight를 또 양자화하는 것이 아니라 weight를 양자화하면서 발생한 quantization constant(양자화 상수)를 양자화하여 조금 더 가볍게 만드는 것이다.

- Paged Optimizer

이 기법은 모델 자체에 대한 경량화는 아니지만 제한된 컴퓨터 자원에서 모델을 사용할 때 GPU가 사용하는 VRAM page를 CPU의 RAM에도 일부 저장할 수 있게 해서 메모리를 최대한 사용핟로록 하도록 하는 방법이다. 이 기법의 적용으로 동일한 크기의 모델에 대해 OOM 발생을 피할 수 있는  같다.

> process를 memory에 올릴 때 process를 page 단위로 나누어 처리함.


# DoRA

<center><img width="400" src="https://github.com/finddme/finddme.github.io/assets/53667002/1269df50-6c25-4d26-a605-de98c19ef4b9"></center>
<center><em style="color:gray;">Low-Rank Adaption (LoRA)</em></center><br>

DoRA(Weight-Decomposed Low-Rank Adaption)는 LoRA의 변형 중 하나로, 행렬이 행렬의 길이/크기(magnitude)와 방향(direction)의 곱으로 분해(decompose)될 수 있다는 개념에서 출발한 방법론으로, weight matrix $W$는 matrix 크기 m과 방향 v로 분해되어 각각 독립적으로 tuning한다. 즉, pretrained matrix $W$를 magnitude vector m (size $1xd$)와 direction matrix V로 분리하여 magnitude와 direction을 독립적으로 학습시킨다. direction matrix V는 standard LoRA approach와 같이 B\*A matirx에 의해 강화되고, m은 또 따로 그대로 훈련된다. 이와 같이 direction 강화/업데이트에 집중하여 학습이 진행되기 때문에 Full Fine-Tuning에 준하는 tuning 방법이 될 수 있다고 DoRA 저자는 주장한다. 또한 LoRA가 학습에 사용하는 parameter보다 더 적은 수의 parameter를 사용하면서도 더 좋은 성능을 낸다고 한다. 

# 그 외 다른 PEFT방법론

## Adapter 

Adapter based model은 frozen pre-trained method 위에 layer를 추가하고(추가되는 위치는 조정 가능하) fine-tuning 과정에서 추가된 layer만 학습시키는 방법이다. 이때 추가된 layer를 adapter라고 부른다. 학습 시에는 adapter 외의 다른 모델 부분들은 변경되지 않는다. 추론 시 추가적인 부탐이 발생하여 비효율적이다.

## Soft Prompting (prompt tuning)

관련 포스트:

[PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)](https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/)

[P-tuniung: GPT Understands, Too](https://finddme.github.io/natural%20language%20processing/2022/12/21/Ptuning/)

[ P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/)

randomly initialised soft token을 input prompt에 추가하고, LLM의 가중치는 frozen시킨 상태에서 해당 embedding을 학습시킨다. prompt를 추가함으로써 모델에는 <prompt><instance> 형태가 입력된다. prompt에는 "다음 문장에 대한 감성을 긍정, 부정, 중립으로 표시하시오" 와 같이 모델이 task를 잘 인지하도록 하는 문구가 들어간다. 이 과정을 통해 frozen 된 모델이 해당 task에 대해 가장 답변을 잘하도록 만드는 prompt를 얻게 된다. 
