---
title: Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))
category: Natural Language Processing and Linguistics
tag: NLP & Linguistics
---

## 1. Introduction


이번에 살펴볼 논문은 Liu. et al. (2019)의 Linguistic Knowledge and Transferability of Contextual Representations이다. 본 논문에서 제시하는 문제는 아래와 같다:


1. What features of language do these vectors capture, and what do they miss?
2. How and why does **transferability** vary across representation layers in **contextualizers**?
3. How does the choice of pretraining task affect the vectors’ learned linguistic knowledge and transferability? 


- **Transferability**(Transfer Learning(전이 학습)이 얼마나 가능한지): Pre-Trained contexualizer가 특정 Task에 대해 어느정도의 성능을 보이느냐에 따라 전이가 잘 되었는지 아닌지를 판단한다.  
- **Contextualizer**: Sota를 찍은 NLP model에서 중요한 것은 Pre-trained representation이다. 전통적으로 word vector는 static한 word embedding이다. 즉 1 word, 1 vector이다. 이러한 방식은 Glove와 같은 모델에서 사용되었는데, 이러한 모델들은 단어들을 벡터화하는 것에 집중하여 단어 하나하나를 파악했기 때문에 문맥 파악에 취약하다. 최근 연구에서는 이러한 단점을 해결한 Elmo와 같은 모델을 통해 contextual word representations(CWR)을 사용한다. 


> Glove보다 BiLM의 성능이 높기도 하지만 둘의 큰 차이점은 다음과 같다:Glove는 특정 동사를 POS tagging할 때 co-occurrence 빈도가 높은 것을 기준으로 tagging이 진행되는 반면 BiLM은 morphological inflection과 morphological derivation을 구분하여 POS tag가 붙는다.(Peters et al. 2018)


## 2. Model Flow


<center><img width="450" alt="2021-03-06 (1)" src="https://user-images.githubusercontent.com/53667002/110228513-0ea38300-7f45-11eb-9a13-f5052cf94c31.png"></center>

모델의 흐름은 위와 같다. 위 그림은 POS tagging task에 대한 그림이다. 우선 Input Token이 모델에 입력되면 이는 이미 대량의 데이터로 학습된 Pretrained Contextualizer를 통해 feature를 갖는다. 이 feature들은 Probing model을 거쳐서 결과가 나온다. 이렇게 Pretrained model을 사용했을 때가 그렇지 않은 경우보다 더 좋은 성능을 낸다고 하는데, 이러한 방식이 linguistic knowledge를 가져서 높게 나오는 것인지, 어떻게 linguistic knowledge를 가지는지 그리고 어떻게 그 지식들을 전이하는지에 대해 실험해 보는 것이 본 논문의 주요 골자이다.


## 3. Probing Tasks


본 연구에서는 17개의 Probing task를 통해 Linguistic Knowledge를 잘 파악했는지 확인한다. 이 Task들은 크게 Token Labeling, Segmentation, Segmentation로 나뉠 수 있다.


### 3.1 Token Labeling

1\) **part-of-speech tagging (POS)**


형태소 분석 tagging이다. 이를 통해 CWR이 기본적인 통사를 파악했는지를 평가할 수 있다.


> dataset:  
the Penn Treebank (PTB; Marcus et al., 1993),  
the Universal Dependencies English Web Treebank (UDEWT; Silveira et al., 2014).


2\) **CCG supertagging (CCG)**

어것은 문장 내 단어들의 통사적 역할에 대한 세부적인 정보를 확인하는 task이다.


> dataset:  
CCGbank (Hockenmaier and Steedman, 2007), a conversion of the PTB into CCG derivations


3\) **Syntactic constituency ancestor tagging**

이 task의 경우 문장을 넣었을 때 문장 내에서 특정 단어가 어떤 단어에 대해 Parent인지, GParent인지, GGParent인지를 파악하는 것이다. 이를 통해 모델이 계층적 통사구조를 작 파악했는지 알 수 있다.


> dataset:  
phrasestructure tree (from the PTB)


어떤 문장이 입력되었을 때 문장 내의 단어들을 트리로 그렸을 때,


<center><img width="154" alt="2021-03-06 (2)" src="https://user-images.githubusercontent.com/53667002/110228593-b456f200-7f45-11eb-897a-b71be36a43f6.png"></center>

위 그림처럼 F자리에 오는 단어의 parent는 D이고, D의 입장에서 F와 G는 자식이다. 그리고 F와 G는 서로 sister이고 B는 F의 입장에서 grandparent이다. 


4\) **Semantic tagging**


문맥에서의 word’s semantic role(agent, patient, etc.)을 tagging하는 task이다. 이는 어휘적 의미를 파악하는지 그리고 불필요한 POS 변별하는지, 즉 POS tag에서의 유용한 것을 명확히 하는 지를 평가한다.


> dataset:  
dataset of Bjerva et al. (2016),  
the tagset has since been developed as part of the Parallel Meaning Bank (Abzianidze et al., 2017)


5\) **Preposition supersense disambiguation**

여러 의미를 가질 수 있는 Preposition이 각각 어떤 의미를 위해 사용되었는지 tag해주는 task이다. 이를 통해 의미의 모호성과 어휘 의미론적 지식을 검증할 수 있다. 앞서 살펴본 tagging task들(모든 token에 대해 decision을 만들었던)과 달리 이 모델은 single-token preposition에 대해 학습과 평가가 이루어진다.

> dataset:  
STREUSLE 4.0 corpus (Schneider et al., 2018)  
<img width="350" alt="2021-03-06 (3)" src="https://user-images.githubusercontent.com/53667002/110228670-334c2a80-7f46-11eb-90e5-81968eba72e2.png">


6\) **Event Factuality (EF)**


이것은 해당 Event가 실제로 발생했는지 안 했는지 확인하는 task이다. 예를 들어 Jo didn’t remember to leave와 Jo didn’t remember leaving에서 전자는 안 떠난 것이고 후자는 떠난 것이다. 이에 대해 factuality관점에서 바라보면 전자는 발생하지 않은 것이고 후자는 발생한 것이다. (해당 모델은 발생/비발생 예측 값을 [-3, 3]범위에서 값을 매기도록 훈련된다.


> dataset:  
Universal Decompositional Semantics It Happened v2 dataset (Rudinger et al., 2018)


### 3.2 Segmentation


7\) **Syntactic chunking (Chunk)**


단어들이 서로 어떻게 관계되는지를 나누어 단어가 가지는 span과 boundary를 판별하는 task이다.


> dataset:  
CoNLL 2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000)


8\) **Named entity recognition (NER)**


개체명 인식. 고유명사를 얼마나 잘 찾아내는지를 평가하는 task이다.


> dataset:  
CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003)


9\) **Grammatical error detection (GED)**


문법 교정기


> dataset:  
First Certificate in English (Yannakoudakis et al., 2011) dataset,  
converted into sequence-labeling format by Rei and Yannakoudakis (2016)


10\) **conjunct identification (Conj)**


coordination으로 연결된 것인지 판별하는 task.


> dataset:  
coordinationannotated PTB of Ficler and Goldberg (2016)


### 3.3 Pairwise Relations
11\) **Arc prediction**

binary classification task로, 두 token이 관계되어 있는지 확인하는 task이다.


> dataset:  
PTB (converted to UD),  
UD-EWT  


12\) **Arc classification** 

multiclass classification task로, 두 token이 관계되어 있는지, 그리고 어떤 관계이 있는지 분류하는 task이다.


> dataset:  
PTB (converted to UD),  
UD-EWT  


13\) **syntactic dependency arc prediction** 


14\) **syntactic dependency arc classification**


15\) **semantic dependency arc prediction**


16\) **semantic dependency arc classification**


17\) **coreference arc prediction**


공지시관계를 확인하는 task이다.


> dataset  
CoNLL 2012 shared task (Pradhan et al., 2012)  


## 4. Models


### 4.1 Probing Model


Probing Model은 선형모델을 사용했다.


### 4.2 Contextualizers


Contextualizer로는 6가지 모델이 사용되었다.


- **ELMo (Peters et al., 2018a)**: ELMo는 biLM을 통해 독립적으로 학습된 두 contextualizer의 결과를 concatenate한다. 여기서 사용된 ELMo model들은 sentence-shuffled newswire text (the 1 Billion Word Benchmark; Chelba et al., 2014) 800M token으로 훈련되었다.  
1) ELMo (original): contextualiz하기 위해 2-layer LSTM을 사용한다.  
2) ELMo (4- layer)  
3) ELMo (transformer): 6-layer transformer를 사용한다.


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4\) **OpenAI transformer**: 

left-to-right 12-layer transformer language model로, contiguous text from over 7,000 unique unpublished books (BookCorpus; Zhu et al., 2015) 800M token으로 훈련되었다.


- **BERT** (Devlin et al., 2018): bidirectional transformer로 masked language modeling task 와 next sentence prediction task 훈련을 진행하였고, BookCorpus 와 English Wikipedia를 합쳐 약 3300M token으로 훈련되었다.  
5) BERT (base, cased): 12-layer transformer 사용  
6) BERT (large, cased): 24-layer transformer 사용  



## 5. Pretrained Contextualizer Comparison:  What features of language do these vectors capture, and what do they miss?

## 6. Analyzing Layerwise Transferability: How and why does transferability vary across representation layers in contextualizers?

## 7. Transferring Between Tasks: How does the choice of pretraining task affect the vectors’ learned linguistic knowledge and transferability? 



## Reference
> https://arxiv.org/abs/1808.08079  
> https://dmitry.ai/t/topic/172  
> Deep Learning 2017  

