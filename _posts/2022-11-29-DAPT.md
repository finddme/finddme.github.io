---
title: DAPT-TAPT | Don’t Stop Pretraining; Adapt Language Models to Domains and Tasks
category: Natural Language Processing
tag: NLP
---








* 목차
{:toc}












# Introduction
본 논문은 특정 task 혹은 domain에 대해 맞춤 학습을 수행한 LM(Language Model)이 더 좋은 성능을 보일 것이라는 전제를 기반으로 하며, 이를 다양한 실험을 통해 입증하였다. 최근 자연어처리 분야에서는 LM을 기반으로 downstream task에 대해 fine-tuning을 진행하는 방식이 많이 사용된다. 즉, LM에서 학습된 representation을 기반으로 해당 모델 뒤에 특정 task를 수행할 layer를 추가하여 학습하는 방식이 많이 사용된다. 수행하고자 하는 task가 LM을 기반으로 하는 만큼 fine-tuning model의 성능은 LM의 영향을 받게 된다. 이는 곧 LM의 data distribution이 downstream task 결과에도 영향을 준다는 의미로 이어질 수 있다. 오늘날 LM은 일반적으로 백과사전, 뉴스 기사 등 웹 크롤링 데이터를 학습한다. 

## Data distribution
Figure 1은 data distribution을 그림으로 표현한 것이다.

<center><img width="400" src="https://user-images.githubusercontent.com/53667002/204420933-8ff11cce-3b5f-4e9b-9815-cadff46e5be3.png"></center>

Figure 1 중앙에 위치한 짙은 색의 타원 부분은 실제 풀고자 하는 task의 distribution을 나타내고, 연한 회색 부분의 경우에는 해당 task와 관련된 data의 분포를 나타낸다. 양 옆으로 넓게 퍼진 영역 중 오른쪽에 위치한 주황색 부분은 original LM의 분포이고 왼쪽의 파란색 부분은 task data가 속한 domain 전반에 대한 data distribution을 나타낸다. 

task distribution은 original LM과 target domain distribution 영역에 걸쳐 있는 것을 확인할 수 있다. 따라서 본 논문은 특정 task 혹은 domain-specific unlabeled data에 대해 추가 학습을 한 LM을 기반으로 fine-tuning을 했을 때 더 좋은 성능을 보일 것이라는 전제 하에 다양한 실험을 진행하였다. 여기에서 진행한 실험들은 모두 Roberta를 기반으로 진행된다.

> [Roberta]
Roberta는 BERT계열의 모델이다. Roberta의 경우 BERT가 under training되었다고 주장하며 이를 해결하기 위한 네 가지 방법을 제안했다.
1) 더 많은 데이터를 더 큰 batch로 더 오래 학습시킨다.
2) sequence length를 늘려 학습한다.
3) BERT가 수행하는 task(MLM, NSP) 중 MLM만을 사용하여 학습한다.
4) MLM 수행 시 masking pattern을 매 epoch마다 다르게 준다. 
