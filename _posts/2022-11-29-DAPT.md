---
title: DAPT-TAPT | Don’t Stop Pretraining; Adapt Language Models to Domains and Tasks
category: Natural Language Processing
tag: NLP
---








* 목차
{:toc}












# 1. Introduction
본 논문은 특정 task 혹은 domain에 대해 맞춤 학습을 수행한 LM(Language Model)이 더 좋은 성능을 보일 것이라는 전제를 기반으로 하며, 이를 다양한 실험을 통해 입증하였다. 최근 자연어처리 분야에서는 LM을 기반으로 downstream task에 대해 fine-tuning을 진행하는 방식이 많이 사용된다. 즉, LM에서 학습된 representation을 기반으로 해당 모델 뒤에 특정 task를 수행할 layer를 추가하여 학습하는 방식이 많이 사용된다. 수행하고자 하는 task가 LM을 기반으로 하는 만큼 fine-tuning model의 성능은 LM의 영향을 받게 된다. 이는 곧 LM의 data distribution이 downstream task 결과에도 영향을 준다는 의미로 이어질 수 있다. 오늘날 LM은 일반적으로 백과사전, 뉴스 기사 등 웹 크롤링 데이터를 학습한다. 

## 1.1 Data distribution
Figure 1은 data distribution을 그림으로 표현한 것이다.

<center><img width="400" src="https://user-images.githubusercontent.com/53667002/204420933-8ff11cce-3b5f-4e9b-9815-cadff46e5be3.png"></center>

Figure 1 중앙에 위치한 짙은 색의 타원 부분은 실제 풀고자 하는 task의 distribution을 나타내고, 연한 회색 부분의 경우에는 해당 task와 관련된 data의 분포를 나타낸다. 양 옆으로 넓게 퍼진 영역 중 오른쪽에 위치한 주황색 부분은 original LM의 분포이고 왼쪽의 파란색 부분은 task data가 속한 domain 전반에 대한 data distribution을 나타낸다. 

task distribution은 original LM과 target domain distribution 영역에 걸쳐 있는 것을 확인할 수 있다. 따라서 본 논문은 특정 task 혹은 domain-specific unlabeled data에 대해 추가 학습을 한 LM을 기반으로 fine-tuning을 했을 때 더 좋은 성능을 보일 것이라는 전제 하에 다양한 실험을 진행하였다. 여기에서 진행한 실험들은 모두 Roberta를 기반으로 진행된다.

> [Roberta]
> Roberta는 BERT계열의 모델이다. Roberta의 경우 BERT가 under training되었다고 주장하며 이를 해결하기 위한 네 가지 방법을 제안했다.
1) 더 많은 데이터를 더 큰 batch로 더 오래 학습시킨다.
2) sequence length를 늘려 학습한다.
3) BERT가 수행하는 task(MLM, NSP) 중 MLM만을 사용하여 학습한다.
4) MLM 수행 시 masking pattern을 매 epoch마다 다르게 준다. 

## 1.2 Related studies

앞서 본 논문은 target task 혹은 domain data를 통해 추가 학습시킨 LM이 downstream task 성능 향상에 영향을 준다는 전제를 가지고 있다고 언급한 바 있다. 이러한 전제는 특정 domain을 위한 pre-train 모델을 생성하여 좋은 성능을 보인 사례들을 기반으로 성립된 것으로 보인다. 해당되는 사례에는 LEGAL-BERT, Bio-BERT, SCI-BERT 등이 있다. 본 논문에서는 Bio-BERT를 언급하며 continued pretraining 기법을 통해 좋은 성능을 보이긴 했지만 해당 기법이 labeled task data의 양 그리고 target domain과 original pretraining corpus의 유사도에 얼마나 영향을 받는지에 대한 설명이 부족하다고 지적했다.

## 1.3 Experiments setting

본 논문에서 진행된 실험은 domain-specific한 large corpora 그리고 task와 직접적으로 관련된 unlabeled data를 사용하여 진행된다. 전자를 사용하여 진행한 continued pretraining 방법론을 DAPT(domain-adaptive pretraining)라 칭하며, 후자를 사용하여 진행한 continued pretraining 방법론은 TAPT(task-adaptive pretraining)라고 한다. 
LM의 추가학습이 downstream task에 미치는 영향과 그 이유를 분석하기 위한 다양한 실험들이 진행된다. 우선 4 가지 domain(biomedical, computer science publications, news , reviews)을 기반으로 각 domain마다 두 개의 classification task, 총 8개의 task에 대한 실험을 진행했으며, low resource와 high resource setting에서의 실험도 진행했다. 위 4가지 domain을 선정한 이유는 각 domain마다 text classification data가 있어서 DAPT와 TAPT에 대한 성능 비교가 용이하기 때문이다. 또 다른 실험으로는 DAPT 및 TAPT 수행 시 타 domain이나 task에 대해서도 knowledge transfer 되는지에 대한 실험이 있다. 마지막으로 인간이 만든 dataset과 자동으로 선택된 unlabeled dataset으로 진행한 pretraining에 대한 실험도 진행했다.

# 2. Domain-Adaptive Pretraining

Domain-Adaptive Pretraining실험에는 4개의 domain(biomedical, computer science publications, news , reviews)에 대해 각각 biomedical (BIOMED) papers, computer science (CS) papers, newstext from REALNEWS 그리고 AMAZON reviews data가 사용되었다. 

## 2.1 Analyzing Domain Similarity
DAPT를 수행하기 전, 앞으로 진행할 실험 결과에 대한 명확한 분석을 위해 각 domain data와 Roberta가 학습한 data(original LM data)의 유사도를 확인하였다. Figure 2는 각 domain vocab에서 stopword를 제외하고 가장 많이 등장한 10000개의 단어에 대한 유사도를 측정한 결과이다.

<center><img width="400" src="https://user-images.githubusercontent.com/53667002/204422173-6d436c2c-3237-4f08-9e23-5587e1b63642.png"></center>

위 결과를 보면 Roberta pretraining domain은 news와 review domain과의 유사도가 높은 반면 CS와 BioMed에 대해서는 유사도가 낮은 것을 확인할 수 있다. original LM data domain과의 유사도가 낮을수록 DAPT를 통한 성능 향상 가능성이 높다고 볼 수 있다.

## 2.2 Experiments

<center><img width="900" src="https://user-images.githubusercontent.com/53667002/204422252-9ce85e6b-f397-449a-8c33-26bde73dbd6a.png"></center>

Table 1은 각 domain에 대해 추가 학습을 진행 한 결과를 보여준다. 본 논문의 실험에 사용된 back born model은 160G data를 학습한 Roberta base이며, 각 domain에 대해 해당 모델을 12.5k step씩 추가 학습시켰다. 학습 결과를 보면 news domain를 제외한 다른 domain에서 DAPT 수행 후 MLM loss가 감소한 것을 확인할 수 있다. 이전에 분석한 domain 유사도 분석 결과를 바탕으로 실험 결과를 분석해 보면 original LM과의 유사도가 낮을수록 DAPT 적용 후 MLM loss 감소율이 더 큰 것을 확인할 수 있다.

앞서 언급한 바와 같이 추가학습한 모델을 기반으로 각 domain마다 두 가지 classification task에 대한 fine-tuning을 진행하였다. 아래 Table 2로 downstream task dataset 정보를 확인할 수 있다.

<center><img width="900" src="https://user-images.githubusercontent.com/53667002/204422319-beeb7009-f850-4767-9444-08f1126db811.png"></center>

## 2.3 Domain Relevance for DAPT
downstream task 결과를 통해 domain relevance가 DAPT에 어떤 영향을 주는지 확인하는 실험을 진행하였다. Tabel 3에 나타난 결과에서 CHEMPROT와 RCT의 평가 metrics은 micro-f1 score이고 나머지는 mecro-f1 score이다. 평가지표를 다르게 적용한 이유는 data의 class imbalance 문제 때문으로 보인다. mecro f1 score의 경우에는 class 균형을 고려하지 않고 결과를 산출하는 평가지표로, 이를 적용한 데이터는 class imbalance 문제가 심하지 않아 이를 적용한 것으로 생각된다. DAPT를 수행한 이후 모든 결과들에 대해 성능이 향상된 것을 확인할 수 있다. 하지만 이러한 성능 향상이 데이터 증가로 인한 것인지 확인하기 위해 해당 표에는 관련 없는 domain data로 수행한 DAPT의 결과도 제시되어 있다. 관련 없는 domain data로 수행한 DAPT의 결과는 CS domain을 제외하고는 모두 성능이 감소되어 domain과 관련된 data가 DAPT로 인한 성능 향상에 중요한 역할을 한다는 것이 증명되었다.

<center><img width="400" src="https://user-images.githubusercontent.com/53667002/204422393-c7dc399a-62a1-4851-a4bf-dfc40c11db53.png"></center>

## 2.4 Domain Overlap

각 domain이 공유하는 data boundary 파악을 위한 정성적 실험을 진행하였다. Figure 2를 보면 Review와 News domain의 data 유사도는 40%이다. 이 두 domain에 대한 domain overlap실험 결과가 Table 4에 정리되어 있다. 해당 table에서 주황색으로 강조된 것들이 overlap된 부분이다. 이 실험을 통해 domain이 다른 data 간에도 겹치는 부분이 있다는 것을 알 수 있다.

<center><img width="900" src="https://user-images.githubusercontent.com/53667002/204422451-c89c1359-7cdd-4971-b8ab-3cf4fd666486.png"></center>

# 3. Task-Adaptive Pretraining

<center><img width="900" src="https://user-images.githubusercontent.com/53667002/204422510-21267a15-4754-4213-9dea-8aa02cc5f3e1.png"></center>

앞서 설명한 DAPT와는 달리 TAPT(Task-Adaptive Pretraining)는 task와 관련된 unlabeled data에 대해 추가학습을 진행하는 것이다. task 관련 범위는 domain의 범위보다 작기 때문에 상대적으로 적은 data로 학습은 진행할 수 있다. Table 5는 original Roberta와 DAPT수행, TAPT수행, DAPT+TAPT 수행 이후 진행한 downstream task 결과이다. TAPT 수행 결과, 우선 baseline보다는 모두 향상했고, PAT보다 적은 data를 사용했음에도 RCT, HYPERPARTISAN, AGNEWS, HELPFULNESS 그리고 IMDB에 있어DAPT보다 높은 성능을 보인 것을 확인할 수 있다. 

## 3.1 Combined DAPT and TAPT

DAPT+TAPT는 DAPT 적용 이후 TAPT까지 추가적으로 수행한 것으로, 이 방법론이 모든 task에 있어 가장 좋은 성능을 보였지만 catastrophic forgetting에 대한 우려가 있다고 언급하며 이에 대한 연구를 future work로 남겨 두었다.

## 3.2 Cross-Task Transfer

<center><img width="900" src="https://user-images.githubusercontent.com/53667002/204422599-b42c536e-21db-4a82-b50a-34055b9a3bf5.png"></center>

또한 같은 domain에 속한 다른 task간의 knowledge transfer가 되는지 확인한 실험도 진행하였다. 예를 들어 RCT unlabeled data로 학습한 LM(RCT에 대한 TAPT)으로 CHEMPROT labeled data에 대한 fine-tuning을 진행하는 방식의 실험인데 이러한 방법론을 본 논문에서 Transfer-TAPT라고 칭한다. Table 6에 해당 실험에 대한 결과가 정리되어 있다. 전반적으로 성능이 좋지 않은데, 이는 동일한 domain에 속해도 각 task의 data distribution이 다를 수 있다는 것을 보여준다. 더 나아가 이 실험의 결과를 통해 왜 domain 지식에 대한 adapting만으로는 충분하지 않은지, 그리고 왜 DAPT 이후에 TAPT를 수행해야 하는지도 설명할 수 있다.

# 4. Augmenting Training Data for Task-Adaptive Pretraining

더 많은 task distribution에 속한 unlabeled data가 더 많은 상황에 대한 결과를 확인하기 위한 실험도 진행하였다. task data augmentation 방식으로는 task와 관련된 unlabeled data를 사람이 선별하는 방식과 자동으로 고르는 방식이 제안되었다.

## 4.1 Human Curated-TAPT

명확한 source로부터 사람이 직접 다량의 unlabeled data를 수집하여 task training data distribution과 유사한 분포의 dataset을 생성하여 TAPT를 수행하는 것을 Human Curated-TAPT라 부른다. 

1) Data (Table 2 참고)
- RC-500: low-resource setting된 RCT data. 18040개의 RCT data 중 500개만 labeled data로 사용하고 나머지는 unlabeled data로 TAPT학습에 사용한다. (동일한 dataset에서 빼 온 것이기 때문에 동일한 task distribution에 속한다)
- HYPERPARTISAN: 5000개의 unlabeled data를 TAPT에 사용
- IMDB: task annotator가 50000개의 unlabeled data를 직접 수집
- 
2) Results
<center><img width="400" src="https://user-images.githubusercontent.com/53667002/204422811-af777be1-71a7-4603-b0bb-56f168a3585b.png"></center>

Table 7를 보면 Curated-TAPT, TAPT, DAPT+TAPT에 대한 실험 결과가 비교할 수 있다. Curated-TAPT 결과가 이전에 실험 결과가 가장 좋았던 DAPT+TAPT보다 좋게 나온 것을 확인할 수 있다. 그리고 DAPT이후 Curated-TAPT를수행한 결과가 모든 task에서 가장 높은 성능을 보였다. 이러한 결과는 task distribution에 속한 다량의 curating data가 end-task 성능에 좋은 영향을 미친다는 것을 시사한다.

## 4.2 Automated Data Selection for TAPT

두 번째 증강 방법은 다량의 in-domain corpus에서 task distribution과 연관된 unlabeled text를 찾아 다량의 unlabeled data를 자동으로 생성하는 방식이다. 조금 더 구체적으로 말하자면, embedding space 내에 task 와 domain이 공유하는 space에서 domain data 중 task-relevant data를 찾아 후보를 선별하여 data를 증강시키는 것이다. 이 실험에서 중요한 것은 다량의 문장을 embedding하기 위해 embedding 모델은 가벼우면서 빨라야 한다는 것이다. 저자는 이러한 조건을 충족시키는 모델인 lightweight bag-of-words language model인 VAMPIRE를 사용하여 (i) task sentence 근처에 있는 k개의 domain data 50/100/150개(kNN-TAPT), (ii) 랜덤하게 선별한 data 50개(RAND-TAPT)를 뽑았다.

<center><img width="400" src="https://user-images.githubusercontent.com/53667002/204422989-3ff9e1c0-ace1-42b3-8675-f5bb7632aec5.png"></center>

Table 8에 나타난 실험 결과를 보면 RAND-TAPT의 성능이 가장 낮고, kNN-TAPT에서는 k가 클수록 높은 성능을 보이는 것을 확인할 수 있다.

## 4.3 Computational Requirements

Table 9는 BIOMED domain의 RTC-500에 대해 지금까지 수행한 모든 adaptation technique의 computational requirements이다.

<center><img width="400" src="https://user-images.githubusercontent.com/53667002/204422914-ba5eeacb-f7d6-40ec-99b8-88a05332a197.png"></center>
