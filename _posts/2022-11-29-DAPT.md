---
title: DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks
category: Natural Language Processing
tag: NLP
---








* 목차
{:toc}











## Introduction
본 논문은 특정 task 혹은 domain에 대해 맞춤 학습을 수행한 LM(Language Model)이 더 좋은 성능을 보일 것이라는 전제를 기반으로 하며, 이를 다양한 실험을 통해 입증하였다. 최근 자연어처리 분야에서는 LM을 기반으로 downstream task에 대해 fine-tuning을 진행하는 방식이 많이 사용된다. 즉, LM에서 학습된 representation을 기반으로 해당 모델 뒤에 특정 task를 수행할 layer를 추가하여 학습하는 방식이 많이 사용된다. 수행하고자 하는 task가 LM을 기반으로 하는 만큼 fine-tuning model의 성능은 LM의 영향을 받게 된다. 이는 곧 LM의 data distribution이 downstream task 결과에도 영향을 준다는 의미로 이어질 수 있다. 오늘날 LM은 일반적으로 백과사전, 뉴스 기사 등 웹 크롤링 데이터를 학습한다. 
