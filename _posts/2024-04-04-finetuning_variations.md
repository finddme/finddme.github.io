---
title: "Fine-tuning variations : RLHF/PPO, DPO, ORPO (작성 중)"
category: LLM / Multimodal
tag: NLP
---







* 목차
{:toc}






# Reference

> Training language models to follow instructions with human feedback
>
> Direct Preference Optimization: Your Language Model is Secretly a Reward Model
>
> ORPO: Monolithic Preference Optimization without Reference Model
