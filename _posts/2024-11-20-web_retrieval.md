---
title: "Web Auto crawling + Retrieval ğŸŒâ™¾ï¸"
category: Dev Log
tag: Development
---







* ëª©ì°¨
{:toc}












# Git Repository

[https://github.com/finddme/AutoCrawl_Retrieval](https://github.com/finddme/AutoCrawl_Retrieval)

[https://github.com/finddme/func_react_stream_light](https://github.com/finddme/func_react_stream_light)

# ê°œë°œ ê³„íš / ê°œìš”

- input: ë©”ì¸ page url
  - íƒ­ ë° í•˜ìœ„ í˜ì´ì§€ ìë™ crawling
  - ~~text ì •ë³´ ìˆ˜ì§‘~~ -> text ì •ë³´ ìˆ˜ì§‘ + ê°ì¢… ìë£Œ ë‹¤ìš´ë¡œë“œ(pdf, docx, hwp, image, video, excel ...)
  - DB ì €ì¥ -> ë‹¤ìš´ë¡œë“œëœ ìë£Œë“¤ì— ëŒ€í•œ ì¶”ê°€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ê°œë°œ í•„ìš”
    - video: [https://github.com/finddme/Video_Retrieval](https://github.com/finddme/Video_Retrieval)ì˜ ë°ì´í„° ì²˜ë¦¬ ë°©ì‹ ì ìš©
    - image: [https://github.com/finddme/Video_Retrieval](https://github.com/finddme/Video_Retrieval)ì˜ ì˜ìƒ ìº¡ì³ ì´ë¯¸ì§€ ì²˜ë¦¬ ë°©ì‹ ì ìš©
    - pdf, docx, hwp: pymupdf4llm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ì¼ê´„ ì²˜ë¦¬
    - excel: [https://github.com/finddme/Excel_to_Chat](https://github.com/finddme/Excel_to_Chat)ì˜ ë°ì´í„° ì²˜ë¦¬ ë°©ì‹ ì ìš©
- output: ë‹µë³€ ì¶œì²˜ url + ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€(LLM ìƒì„±)

+ ~~blog search boxë¥¼ serverless í˜•ì‹ìœ¼ë¡œ ë°”ê¾¸ëŠ” ì‘ì—…ì—ë„ ì ìš© ì˜ˆì • [func_react_stream_light](https://github.com/finddme/func_react_stream_light)~~ **ê°œë°œ ì™„ë£Œ**
+ ~~blog search boxë¥¼ serverless í˜•ì‹ìœ¼ë¡œ ë°”ê¾¸ëŠ” ì´ìœ : ì»´í“¨í„°ë¥¼ í•­ìƒ ì¼œ ë‘ëŠ” ê²ƒì´ ë¶€ë‹´ìŠ¤ëŸ½ë‹¤. gcp, awsëŠ” ëˆì´ ë§ì´ ë‚˜ê°„ë‹¤.............~~

# Auto Crawling

<center><img width="600" src="https://github.com/user-attachments/assets/bb9eb0ed-035f-407e-9dc4-3e9f0d5a4e27"></center>
<center><em style="color:gray;">https://mermaid.live/</em></center><br>

## ì…ë ¥, ì¶œë ¥, ë³‘ë ¬ì²˜ë¦¬
```python
async def crawl(main_url, max_pages=None,crawling_verbose=False):
    # ë©”ì¸ URL ì…ë ¥ ë°›ê¸°
    to_crawl = set()
    to_crawl.add(main_url)
    
    results = []
    total_downloaded = []
    visited_urls = set()
    
    while to_crawl and (max_pages is None or len(visited_urls) < max_pages):
        async with AsyncWebCrawler(verbose=crawling_verbose) as crawler: 
            async with aiohttp.ClientSession() as session:
                current_batch = list(to_crawl)[:min(5, len(to_crawl))]  # ì²˜ë¦¬í•  URLë“¤ batchë¡œ ë¬¶ìŒ
                domain = urlparse(current_batch[0]).netloc
                print(f"Crawling batch of {len(current_batch)} URLs from domain: {domain}")

                tasks = [process_url(url, crawler, domain,visited_urls,session) for url in current_batch] # crawling task ë¦¬ìŠ¤íŠ¸ ìƒì„±
                
                batch_results = await asyncio.gather(*tasks) # íƒœìŠ¤í¬ ë³‘ë ¬ ì‹¤í–‰ 

                all_new_links = set()
                for url, (page_info, downloaded_file, new_link) in zip(current_batch, batch_results):
                    if page_info:
                        results.append(page_info)
                        total_downloaded.extend(downloaded_file)
                        all_new_links.update(new_link)
                    visited_urls.add(url) # ì²˜ë¦¬ëœ URLì€ visited_urlsì— ì¶”ê°€í•˜ê³  to_crawlì—ì„œ ì œê±°
                    to_crawl.remove(url)
                
                to_crawl.update(all_new_links - visited_urls) # ìƒˆë¡œ ë°œê²¬í•œ ë§í¬ ì¤‘ì—ì„œ ì•„ì§ ë°©ë¬¸ ì•ˆ í•œ urlì„ í¬ë¡¤ë§ ëŒ€ìƒì— ì¶”ê°€
                
                await asyncio.sleep(1)  
    
    return results
```

## crawling task ìˆ˜í–‰

```python
async def process_url(url, crawler, domain,visited_urls,session):
    download_dir=f"./sample_result/j_{domain}_site_crawling"
    for dir_type in ['images', 'documents', 'videos']:
        os.makedirs(os.path.join(download_dir, dir_type), exist_ok=True)
    try:
        result = await crawler.arun(url=url) # URL Crawling

        # Crawling ë‚´ìš© ì €ì¥
        page_info = {
            'url': url,
            'content': result.markdown,
            'title': result.title if hasattr(result, 'title') else 'No Title',
            'crawled_at': datetime.now().isoformat()
        }

        ############# file extraction ë¶€ë¶„ #############
        file_links = set()
        patterns = [
            r'\[([^\]]*)\]\(([^)]+)\)',  
            r'href=["\'](.*?)["\']',      
            r'src=["\'](.*?)["\']'        
        ]

        for pattern in patterns:
            matches = re.findall(pattern, result.markdown)
            for match in matches:
                file_url = match[1] if isinstance(match, tuple) else match
                absolute_url_file = urljoin(url, file_url)
                if get_file_type(absolute_url_file):
                    file_links.add(absolute_url_file)

        download_tasks = [download_file(file_url, session, download_dir) for file_url in file_links] # PDF/Image/Video ... ìë£Œ ë‹¤ìš´ë¡œë“œ
        downloaded_files = await asyncio.gather(*download_tasks)
        downloaded_files = [f for f in downloaded_files if f]
        
        page_info['downloaded_files'] = downloaded_files
            
        ############# link extract ë¶€ë¶„ (Crawling ë‚´ìš©ì—ì„œ URL ì¶”ì¶œ)#############
        urls = re.findall(r'\[([^\]]*)\]\(([^)]+)\)', result.markdown)
        new_links = set()
        
        for _, found_url in urls:
            absolute_url = urljoin(url, found_url)
            # URL ê²€ì¦
            if should_crawl(absolute_url, domain,visited_urls):
                new_links.add(absolute_url)
                
        return page_info, downloaded_files, new_links
    except Exception as e:
        print(f"Error crawling {url}: {str(e)}")
        return None, None, set()

```
