---
title: Word2vec:Skip-gram
category: Natural Language Processing
tag: word2vec
---

## Skip-gram prediction

Skip-gram은 center word를 기준으로 양 옆에 있는 단어(output context words)를 예측할 때 최적의 확률 값을 갖도록 학습하는 모델이다. 따라서 우선 하나의 center word를 두고 양 옆에 있는 몇 개의 단어를 예측할 것인지 window size를 정한다. Window size가 $m$이라면 center word를 이용해 예측하는 주변 단어의 개수는 $2m$개가 되는 것이다.

<center><img width="886" alt="2019-11-06 (1)" src="https://user-images.githubusercontent.com/53667002/68258825-4a8db600-007b-11ea-9df7-6c5d8326cf9a.png"></center>

## Architecture of Skip-gram

<center><img width="800" alt="2019-11-06 (3)" src="https://user-images.githubusercontent.com/53667002/68263585-aeb77680-0089-11ea-8aae-0a3c5d241512.png"></center>
**<center>그림(1)</center>**


위 그림을 보면 Skip-gram모델은 입력 층 - 가중치행렬$W$ - hidden layer - 가중치 행렬 $W'$ – 출력 층으로 이루어져 있다. 

### Parameter

그림(1)을 보면 $W$의 차원과 $W’$의 차원은 서로 전치된 차원이라는 것을 확인할 수 있는데 $W$와 $W'$가 같은 행렬은 아니다. $W$는 center word vector에 대응하는 행렬이고, $W'$는 output context word vector와 대응하는 행렬이다. 그림을 보면 입력층의 차원은 1$xV$이다($V$= 임베딩하려는 단어의 수). 그리고 hidden layer 노드 수($N$)는 곧 embedding 차원 수로, 사용자가 임의로 설정이 가능한데 입력층의 차원과 hidden layer의 각 노드가 1:1대응이 되야 하기 때문에 그들 가운데에 위치한 $W$의 차원은 $V x N$이 된다. 

### skip-gram model works

이제 모델의 전체적인 구조가 진행되는 과정을 수식으로 살펴보겠다.

1\. Center word를 one-hot-vector로 만든다. (문서 $d$에 사용된 단어 $w_t$(embedding하려는 단어들)를 모두 추출하여 one-hot-encoding한다.)

$$x\in\mathbb{R}^{|V|}$$

2\. Embedding matrix $W$와 이전에 one-hot-encoding한 center word를 곱하여 embedded word vector를 구한다. 

$$v_c=\mathbf{W}x\in\mathbb{R}^n$$

이 작업은 embedding matrix W에서 해당 단어에 해당하는 행(column)을 look up해 오는 것이라 말할 수 있다.

<center><img width="481" alt="2019-10-23 (5)" src="https://user-images.githubusercontent.com/53667002/68264390-0bb42c00-008c-11ea-86c9-f2b3b2b81cd7.png"></center>

3\. embedded vector를 context matrix $W'$와 곱해서 score vector를 구한다. 

$$z=\mathbf{W}^{\prime}v_c$$

4\. Score vector를 확률 값으로 만든다.

$$\hat{y}=\text{softmax}(z)$$

5\. $\hat{y}$(예측 값(확률 값)과 각 위치의 $y$(정답 값)를 cross-entropy(loss-function) $H$를 사용하여 loss를 줄여가며 학습한다. 

$$J=\sum^{2m}_{i}H(\hat{y},y_{i})$$

## Model Training

이제 모델의 학습이 진행되는 과정에 대해 설명하겠다. 해당 모델 학습의 목표는 다음 식을 최대화하는 것이다:

$$L(\theta)=logP⁡(+|{ c }_{ P },{ o }_{ p })+\sum_{i=1}^klogP⁡(-|{ c }_{ n_i },{ o }_{ n_i })$$ 
**<center>수식(1)</center>**


위 식은 한 iteration마다 모델 파라미터($\theta$)가 업데이트되는데 이때 한 쌍의 positive sample ($c_P$, $o_p$ : 실제 같이 쓰이는 단어쌍들의 집합)과 $k$쌍의 negative sample($c_{n_i}$, $o_{n_i}$ : 코퍼스 전체에서 랜덤 추출한 함께 사용되지 않은 단어 쌍)이 학습된다는 것을 표현한 log-likelihood function이다. 

위 수식에서 $P(+|{ c }_{ P },{ o }_{ p } )$는 positive sample에서 center word($c$)가 output context word($o$)와 함께 쓰일 확률이다. 이 확률은 다음과 같이 정의될 수 있다:

$$P(+|c_P,o_p)=\frac{ 1 }{ 1+exp⁡({ -u }_{ c }{ v }_{ o }) }$$ 
**<center>수식(2)</center>**


해당 모델의 목표는 예측된 positive sample인 center word와 context word쌍이 positive일 확률이 높이는 것이기 때문에 위 식의 결과를 최대화하는 방향으로 학습이 진행되야 한다. 수식(2)에서 $u_c$는 $W$의 column vector, 그리고 $v_o$는 $W'$의 row vector이다. 이 수식을 최대화하기 위해서는 분모인 $1+exp⁡({ -u }_{ c }{ v }_{ o })$값을 줄여야 한다. 따라서 두 벡터($u_c$ 와  $v_o$)의 내적 값을 키워야 하는데 해당 벡터의 내적은 곧 cosine similarity라 할 수 있으므로 해당 값이 커지면 positive sample에서 두 벡터간의 유사도가 높아지는 것과 같다.

**<center>미완성 게시물. 작성 중.</center>**
