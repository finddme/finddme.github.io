---
title: "LLaMa 3.1"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}










# LLaMa 3.1
Llama 3.1 모델을 INT4로 성공적으로 양자화
context length를 128K로 확장, 8개 언어를 지원
Llama 3.1 405B는 최초 frontier-level open source. 지금까지 open source LLM은 closed source LLM에 비해 성능이 좋지 않았지만 LLaMa 3.1 405B는  closed source model에 견줄만한 성능을 보인다.
reference system을 포함하고 있다. 

이번 release에서는 8B와 70B model의 upgrad가 있다. 두 모델은 multilingual을 지원하며, context length가 128K로 크게 증가했다. 또한 state-of-the-art tool을 사용하여 전반적인 reasoning capability를 향상시켰다. 이를 통해 이번에 공개된 LLaMa 모델은 advanced use case(long-form text summarization, multilingual conversation, coding)성능이 크게 향상되었다.

# Evaluation

LLaMa 3.1는 다양한 언어에 대한 150개 이상의 benchmark에 대해 성능 평가를 진행하였고, real-world scenario를 통한 human evaluation도 진행하였다. 아래 표 이번 LLaMa 3.1의 flagship model인 405B 모델의 실험 결과이다. GPT-4, GPT-4o, Claude 3.5 Sonnet과 비교하였을 때 경쟁력있는 성능을 보이는 것으로 확인되었다. 8B와 70B또한 비슷한 크기의 모델들과 비요하였을 때 경쟁력있는 성능을 보였다.

<center><img width="600" src="https://github.com/user-attachments/assets/fbe09953-9432-41a4-8d3a-a13fc2eaf751"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>

<center><img width="600" src="https://github.com/user-attachments/assets/be723b15-099a-4459-8a43-b6438de5e9ce"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>

<center><img width="600" src="https://github.com/user-attachments/assets/a5bd11e8-cce6-472c-84c0-f8de73f426a6"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>

# Model Architecture

<center><img width="600" src="https://github.com/user-attachments/assets/85ef60d7-cf56-49d7-bc7d-4edec4bb7502"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>


training stability를 maximize하기 위해 MOE model이 아닌 standard decoder-only transformer model architecture를 선택하였다. 

학습 시, supervised fine-tuning과 direct preference optimization 과정을 거침으로써 각 round에서 highest quality synthetic data를 생성하고 각 capability’s performance를 향상시킬 수 있었다.

405B의 경우 15 trillion(15조)이상의 token으로 학습을 수행했다. 이와 같은 규모의 학습을 수행하며 합리적인 시간 내에 원하는 결과를 얻기 위해 Meta 팀은 full training stack을 최적화하고, 16,000개 이상의 H100 GPU를 사용하였다. 

이전 버전들과 비교하여 data 품질도 개션시켰다. careful pre-processing과 curation pipeline 개발을 통해 pre-training data의 품질을 개선시켰고, 더 엄격한 품질 보증 및 filtering approach를 통해 post-training data를 개선시켰다.

LLM의 scaling laws(확장 법칙)에 따라 이번 flagship model(405B)모델은 8B와 70B에 비해 높은 성능을 보이다. 따라서 405B를 활용하여 8B와 70B의 post-training quality를 개선하였다.


## INT4 Performance

405B


Meta팀은 Llama-3.1 모델들(405B, 70B, 8B)의 가중치를 INT4 데이터 유형으로 양자화하는 데 성공하였다. parameter당 bits를 16에 4로 줄임으로써 compute requirement를 효과적으로 줄이고, disk size와 GPU memory 요구량을 75% 감소시키며 성능 수준은 그대로 유지하였다.
이로 인해 405B 모델의 경우, 일반적으로 두 개의 8x80GB GPU 노드가 필요하지만 이제 A100이나 H100 GPU를 사용하는 경우 단 4개의 GPU만으로도 단일 서버에서 실행할 수 있게 되었다.
이는 배포 비용을 약 4배나 줄이는 성과이다.

Llama-3.1–405B의 경우, OpenLLM benchmark에서 INT4 버전은 평균 86.47점을, unquantized 버전은 86.63을 기록하여 두 버전 간의 성능 차이가 매우 작은 것으로 확인하였다.

Llama-3.1–70B 또한 INT4 버전의 경우 78.54점을, unquantized model은 78.67점을 기록하여 양자화 전 후 모델간의 성능 차이가 크지 않았다. 하지만 8B는 unquantized 67.57,  INT4 69.32로 70B와 405B에 비해서는 차이를 보이지만 이정도 차이는 disk size와 GPU memory 75% 감소와 같은 효율성 향상을 고려할 때 충분히 수용할 만한 수준이다.
