---
title: "LLaMa 3.1"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}










# LLaMa 3.1 

- 405B, 70B, 8B 버전이 공개되었다.
- INT8, FP8로 양자화 버전 공개 ([INT4양자화 버전 meta 공식 x](https://huggingface.co/collections/neuralmagic/llama-31-quantization-66a3f907f48d07feabb8f300))
- context length를 128K로 확장, 8개 언어를 지원(Guard model)
- Llama 3.1 405B는 최초 frontier-level open source. 지금까지 open source LLM은 closed source LLM에 비해 성능이 좋지 않았지만 LLaMa 3.1 405B는  closed source model에 견줄만한 성능을 보인다.
reference system을 포함하고 있다. 
- 이번 release에서는 8B와 70B model의 upgrad가 있다. 두 모델은 multilingual을 지원하며, context length가 128K로 크게 증가했다. 또한 state-of-the-art tool을 사용하여 전반적인 reasoning capability를 향상시켰다. 이를 통해 이번에 공개된 LLaMa 모델은 advanced use case(long-form text summarization, multilingual conversation, coding)성능이 크게 향상되었다.

# LLaMa 3 vs LLaMa 3.1

## context length and multilingual token

|                    | LLaMa 3.1 | LLaMa 3 |
| ------------------ | --------- | ------- |
| context length     | 128K      | 8K      |
| multilingual token | 8%        | 5%      |


## math and reasoning capability

LLaMa 3.1이 LLaMa 3보다 수학 분야와 추론 능력이 더 뛰어나다. Meta의 tech blog([llama3]( https://ai.meta.com/blog/meta-llama-3/), [llama3.1]( https://ai.meta.com/blog/meta-llama-3-1/))에 아래와 같은 내용이 기재되어 있다:

| (8B) | LLaMa 3.1               | LLaMa 3            |
| ---- | ----------------------- | ------------------ |
| MATH | MATH (0-shot, CoT) 73.0 | MATH (5-shot) 68.4 |
| MMLU | 51.9                    | 30.0               |

## architecture

LLaMa 3과 LLaMa 3.1은 동일한 dense network architecture를 가지고 있다. 

(8B 기준)
- decoder-only transformer
- 32 layers
- 4K embedding dimension
- 32 heads
- 8KV heads
- rotary positional encoding (RoPE) 사용
- 4 grouped query attention (GQA)
- 15T(15조) token으로 사전 훈련
- 8,192개의 token sequence로 사전 훈련
- tokenizer vocab size 128K

**LLaMa 3.1**

```
n_vocab          = 128256
n_merges         = 280147
vocab_only       = 0
n_ctx_train      = 131072
n_embd           = 4096
n_layer          = 32
n_head           = 32
n_head_kv        = 8
n_rot            = 128
n_swa            = 0
n_embd_head_k    = 128
n_embd_head_v    = 128
n_gqa            = 4
n_embd_k_gqa     = 1024
n_embd_v_gqa     = 1024
f_norm_eps       = 0.0e+00
f_norm_rms_eps   = 1.0e-05
f_clamp_kqv      = 0.0e+00
f_max_alibi_bias = 0.0e+00
f_logit_scale    = 0.0e+00
n_ff             = 14336
n_expert         = 0
n_expert_used    = 0
causal attn      = 1
pooling type     = 0
rope type        = 0
rope scaling     = linear
freq_base_train  = 500000.0
freq_scale_train = 1
n_ctx_orig_yarn  = 131072
model params     = 8.03 B
```

**LLaMa 3**
```
n_vocab          = 128256
n_merges         = 280147
n_ctx_train      = 8192
n_embd           = 4096
n_layer          = 32
n_head           = 32
n_head_kv        = 8
n_rot            = 128
n_swa            = 0
n_embd_head_k    = 128
n_embd_head_v    = 128
n_gqa            = 4
n_embd_k_gqa     = 1024
n_embd_v_gqa     = 1024
f_norm_eps       = 0.0e+00
f_norm_rms_eps   = 1.0e-05
f_clamp_kqv      = 0.0e+00
f_max_alibi_bias = 0.0e+00
f_logit_scale    = 0.0e+00
n_ff             = 14336
n_expert         = 0
n_expert_used    = 0
causal attn      = 1
pooling type     = 0
rope type        = 0
rope scaling     = linear
freq_base_train  = 500000.0
freq_scale_train = 1
n_ctx_orig_yarn  = 8192
model params     = 8.03 B
```

# Model Architecture

<center><img width="600" src="https://github.com/user-attachments/assets/85ef60d7-cf56-49d7-bc7d-4edec4bb7502"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>


training stability를 maximize하기 위해 MOE(mixture-of-experts) model이 아닌 standard decoder-only transformer model architecture를 선택하였다. 

학습 시, supervised fine-tuning과 direct preference optimization 과정을 거침으로써 각 round에서 highest quality synthetic data를 생성하고 각 capability’s performance를 향상시킬 수 있었다.

405B의 경우 15 trillion(15조)이상의 token으로 학습을 수행했다. 이와 같은 규모의 학습을 수행하며 합리적인 시간 내에 원하는 결과를 얻기 위해 Meta 팀은 full training stack을 최적화하고, 16,000개 이상의 H100 GPU를 사용하였다. 

이전 버전들과 비교하여 data 품질도 개션시켰다. careful pre-processing과 curation pipeline 개발을 통해 pre-training data의 품질을 개선시켰고, 더 엄격한 품질 보증 및 filtering approach를 통해 post-training data를 개선시켰다.

LLM의 scaling laws(확장 법칙)에 따라 이번 flagship model(405B)모델은 8B와 70B에 비해 높은 성능을 보이다. 따라서 405B를 활용하여 8B와 70B의 post-training quality를 개선하였다.

# Training Recipes

LLaMa 3.1 논문의 꽃은 이 부분이다. 이전 LLaMa 1이나 2의 논문보다 학습 방법을 더 구체적으로 적어 놓았다.

Meta 팀은 학습 시 수렴 속도는 느리지만 가장 basic한 모델을 채택함으로써 학습의 안정성을 높였다. 최근 많이 사용되는 복잡한 MOE(mixture-of-experts) 모델이 아닌 기본적인 standard dense Transformer model architecture에 대해 약간의 조작만 추가한 모델을 사용하였다. 

기본적인 방식으로 pre-trained model을 구출한 후  post-training을 수행하였다. post-training 단계에서도 supervised finetuning (SFT), rejection sampling (RS), direct preference optimization (DPO)을 사용하여 복잡한 알고리즘은 사용하지 않았다. 

## Pre-training Annealing

  학습 과정에는 Annealing 기법이 적용되었다. Annealing은 학습 중 learning rate를 점진적으로 낮추는 방법이다. 이는 model parameter를 정확하게 조정하여 학습 중 파라미터가 크게 업데이트되어 모델이 불안정해지는 경우를 방지한다. Annealing 단계에서는 데이터 품질이 중요하다. 따라서 가능한 높은 품질의 데이터를 학습시켜 성능과 일반화를 향상시킨다.

## Instruction and chat fine-tuning

LLaMa 3.1 Instruction model 개발을 위한 post-training 과정에서 집중한 것은 아래 세 가지이다.

- 더 많은 기능 지원
- 128K context window 구현
- model 크기 증가

instruction-following capability 개선과정에 LLaMa 3.1 405B를 활용하였다. 

Instruction model은 pre-trained model위에 여러 round의 alignment 작업을 수행하여 개발되었. 각 round에는 Supervised Fine-Tuning (SFT), Rejection Sampling (RS), Direct Preference Optimization (DPO)가 포함된다. 

## Scaling Laws for Downstream Tasks

Scaling Law는 모델 성능이 model size, data size, 그리고 compute resource에 따라 어떻게 변화하는지를 설명하는 법칙이다. Scaling Law는 주어진 compute resource에 대해 최적의 성능을 낼 수 있는 model과 data size를 선택하는 데에 도움을 준다. 예를 들어 어느정도의 compute resource를 투입하면 어느정도 크기의 모델이 최적일지, 이 모델이 얼마나 잘 작동할지 예측하는 것에 도움을 준다.

> **Parameter Scaling**<br>
>
> 일반적으로 파라미터 수가 증가하면 모델의 표현 능력이 향상되어 성능이 좋아지지만, 이는 데이터 크기와 컴퓨팅 자원의 충분한 지원이 있어야 한다.<br>
>
> **Data Scaling**<br>
>
> 충분히 큰 데이터셋이 주어지면 모델은 더 일반화된 패턴을 학습할 수 있어 성능이 향상된다. 하지만 데이터 크기의 증가에 따라 성능 향상 속도는 감소할 수 있다.<br>
>
> **Compute Scaling**<br>
>
> compute resouce가 충분할수록 모델 학습 시간이 단축되거나, 더 큰 모델을 훈련시킬 수 있게 되어 성능이 향상될 수 있다.<br>

하지만 기존의 scaling law에는 몇 가지 한계가 있었다. 

1. **제한적인 prediction** : 기존의 scaling law는 단순히 다음단어를 얼마나 잘 예측하는지에 중점을 두고 있어 특정 task에서 얼마나 좋은 성과를 낼지는 알기 어렵다. 
2. **불확실성** :  small compute budget으로 만든 scaling law이기 때문에 더 큰 computing resource를 투입했을 때에 해당 법칙이 잘 맞을지 불확실하다.

*(1) Existing scaling laws typically predict only next-token prediction loss rather than specific
benchmark performance. (2) Scaling laws can be noisy and unreliable because they are developed based on
pre-training runs conducted with small compute budgets (Wei et al., 2022b).*

Meta 팀은 위 문제를 해결하기 위해 아래 두 방법론을 적용하였다:

1. downstream task에 대해 compute-optimal model의 negative log-likelihood와 training FLOP 간의 상관관계를 찾는다.
2. 모델의 성능과 compute resource 간의 관계식을 만든다.

위 두 방법론을 기반으로 Meta 팀은 다양한 계산 자원(FLOPs)과 모델 크기(파라미터 수)를 사용해 모델을 학습시켜 이를 통해 특정 FLOPs에서 가장 높은 성능을 내는 모델들을 찾아 이를 예측하는 compute-optimal model을 만들었다. 즉, specific compute budget이 주어졌을 때 몇 개의 training token을 사용하는 것이 최적인지 예측하는 관계식을 만들었다.

이 관계식을 통해 Meta 팀은 모델의 downstream task 성능을 예측하였다. 

주어진 floating point operations (FLOPs)에 따라 원하는 benchmark 성능을 얻기 위해 GPU를 얼마나 오래 실행해야 하는지를 예측한 것이다. 컴퓨팅 최적화 모델에 대해 특정 training flop 수를 기준으로 downstream task 성능을 예측할 수 있다고 한다.

> FLOPs

> FLOPs는 연산 복잡도를 나타낸다.<br> FLOPs는 딥러닝과 머신러닝에서 모델의 연산량과 복잡도를 나타내는 중요한 지표로, 모델 선택과 최적화 과정에서 고려해야 할 요소이다.<br>

> FLOPs는 딥러닝 모델의 효율성과 성능을 비교하는 데 많이 사용된다.<br>
> 예를 들어, 모델 A가 10 GFLOPs를 요구하고, 모델 B가 20 GFLOPs를 요구한다면, 일반적으로 모델 A가 더 적은 연산 자원을 소모하는 효율적인 모델이라고 할 수 있다. <br>
> 하지만 반드시 FLOPs가 낮다고 해서 더 좋은 성능을 보장하는 것은 아니다. 모델의 성능과 효율성을 함께 고려해야 한다.<br>

> 하지만 모델이 실제로 얼마나 빠르게 실행되는지는 하드웨어와 최적화 방법에 따라 달라진다.<br>
> 따라서 같은 FLOPs 값을 가진 모델도 하드웨어에 따라 실행 속도는 달라질 수 있다. <br>

## Reward Model

llama 3.1에서 사용된 Reward model은 RLHF가 아닌 rejection sampling 방식으로 작동된다 (Direct Preference Optimization (DPO)). 

Reward Model은 pre-/post-training 단계에서 각각 상이하게 사용되었다. 

- Pre-training: 이 단계에서 사용된 Reward Model은 distilBERT를 기반으로 한다. 이 모델은 데이터 품질을 효율적으로 분류하는 데에 강점을 지니고 있어 initial training 단계에서 사용하였다.
- Post-training: 이 단계에서는 llama 3.1 initial model을 Reward Model로 사용하였다. 


# Training Data

Meta 팀의 모델 개선 및 데이터 품질 개선 방법론은 flywheel 전략을 떠오르게 한다.

하지만 모델이 모델의 실수를 학습할 수 있다는 문제점을 발견하여 verifier model을 학습 과정에 삽입하였다.

> flywheel 전략

> 비즈니스에 필요한 다양한 항목들이 서로 유기적으로 연결이 되면서 한쪽의 힘이 한쪽으로 전달되고 이 힘이 다시 다른 쪽으로 전달되는 과정을 통해서 시너지를 만들어내는 것을 말한다.

## filtering

Meta 팀은 LLaMa 3 개발 시 LLaMa3에 사용될 데이터를 filtering 하기 위해 LLaMa 2를 사용한 것과 같이 LLaMa 3.1 학습에도 LLaMa 3을 사용하였다.

## multilingual 

Meta 팀은 pre-training 학습 중간에 분기를 만들어 90%의 multilingual token으로 구성된 data mix를 사용해 사전학습을 이어감으로써 multilingual model을 구축하였다. 해당 모델 구축 이유는 non-English 데이터 수집이다. 매우 크고 성능 좋은 모델을 사용해서 작은 모델의 학습 데이터를 생성하기 위해 multilingual expert model을 학습시킨 것이다. 

## data source 

데이터의 출처에 대해서는 논문에서 단지 "from a variety of data sources"라고만 기재되어 있다. 최근 Reddit과 Twitter와 같이 인공지능 학습 데이터로 빈번히 사용되던 출처들이 데이터에 대한 요금을 부과하고 있다고 한다. Meta 팀은 허가 받지 않은 데이터는 사용하지 않았다고는 하지만 데이터 출처를 명확히 밝히지도 않았다.

## +

html로 긁어 온 데이터가 대부분이기 때문에, 수학과 코드 데이터의 품질을 향상시키기 위해 indent, 기호 등을 잘 처리하도록 parser 정교화 작업을 하여 수학 및 코드 분야 데이터 품질을 높였다고 한다.

# Reasoning and Mathematics



# Evaluation

LLaMa 3.1는 다양한 언어에 대한 150개 이상의 benchmark에 대해 성능 평가를 진행하였고, real-world scenario를 통한 human evaluation도 진행하였다. 아래 표 이번 LLaMa 3.1의 flagship model인 405B 모델의 실험 결과이다. GPT-4, GPT-4o, Claude 3.5 Sonnet과 비교하였을 때 경쟁력있는 성능을 보이는 것으로 확인되었다. 8B와 70B또한 비슷한 크기의 모델들과 비요하였을 때 경쟁력있는 성능을 보였다.

<center><img width="600" src="https://github.com/user-attachments/assets/fbe09953-9432-41a4-8d3a-a13fc2eaf751"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>

<center><img width="600" src="https://github.com/user-attachments/assets/be723b15-099a-4459-8a43-b6438de5e9ce"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>

<center><img width="600" src="https://github.com/user-attachments/assets/a5bd11e8-cce6-472c-84c0-f8de73f426a6"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>




# Reference

> https://ai.meta.com/blog/meta-llama-3-1/

> https://llama.meta.com/

> https://ai.meta.com/research/publications/the-llama-3-herd-of-models/
