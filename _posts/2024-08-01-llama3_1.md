---
title: "LLaMa 3.1 (정리 초안안)"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}










# LLaMa 3.1 

- 405B, 70B, 8B 버전이 공개되었다.
- INT8, FP8로 양자화 버전 공개 
- context length를 128K로 확장, 8개 언어를 지원(Guard model)
- Llama 3.1 405B는 최초 frontier-level open source. 지금까지 open source LLM은 closed source LLM에 비해 성능이 좋지 않았지만 LLaMa 3.1 405B는  closed source model에 견줄만한 성능을 보인다.
reference system을 포함하고 있다. 
- 이번 release에서는 8B와 70B model의 upgrad가 있다. 두 모델은 multilingual을 지원하며, context length가 128K로 크게 증가했다. 또한 state-of-the-art tool을 사용하여 전반적인 reasoning capability를 향상시켰다. 이를 통해 이번에 공개된 LLaMa 모델은 advanced use case(long-form text summarization, multilingual conversation, coding)성능이 크게 향상되었다.

# LLaMa 3 vs LLaMa 3.1

## context length and multilingual token

|                    | LLaMa 3.1 | LLaMa 3 |
| ------------------ | --------- | ------- |
| context length     | 128K      | 8K      |
| multilingual token | 8%        | 5%      |


## math and reasoning capability

LLaMa 3.1이 LLaMa 3보다 수학 분야와 추론 능력이 더 뛰어나다. Meta의 tech blog([llama3]( https://ai.meta.com/blog/meta-llama-3/), [llama3.1]( https://ai.meta.com/blog/meta-llama-3-1/))에 아래와 같은 내용이 기재되어 있다:

| (8B) | LLaMa 3.1               | LLaMa 3            |
| ---- | ----------------------- | ------------------ |
| MATH | MATH (0-shot, CoT) 73.0 | MATH (5-shot) 68.4 |
| MMLU | 51.9                    | 30.0               |

## architecture

LLaMa 3과 LLaMa 3.1은 동일한 dense network architecture를 가지고 있다. 

(8B 기준)
- decoder-only transformer
- 32 layers
- 4K embedding dimension
- 32 heads
- 8KV heads
- rotary positional encoding (RoPE) 사용
- 4 grouped query attention (GQA)
- 15T(15조) token으로 사전 훈련
- 8,192개의 token sequence로 사전 훈련
- tokenizer vocab size 128K

**LLaMa 3.1**

```
n_vocab          = 128256
n_merges         = 280147
vocab_only       = 0
n_ctx_train      = 131072
n_embd           = 4096
n_layer          = 32
n_head           = 32
n_head_kv        = 8
n_rot            = 128
n_swa            = 0
n_embd_head_k    = 128
n_embd_head_v    = 128
n_gqa            = 4
n_embd_k_gqa     = 1024
n_embd_v_gqa     = 1024
f_norm_eps       = 0.0e+00
f_norm_rms_eps   = 1.0e-05
f_clamp_kqv      = 0.0e+00
f_max_alibi_bias = 0.0e+00
f_logit_scale    = 0.0e+00
n_ff             = 14336
n_expert         = 0
n_expert_used    = 0
causal attn      = 1
pooling type     = 0
rope type        = 0
rope scaling     = linear
freq_base_train  = 500000.0
freq_scale_train = 1
n_ctx_orig_yarn  = 131072
model params     = 8.03 B
```

**LLaMa 3**
```
n_vocab          = 128256
n_merges         = 280147
n_ctx_train      = 8192
n_embd           = 4096
n_layer          = 32
n_head           = 32
n_head_kv        = 8
n_rot            = 128
n_swa            = 0
n_embd_head_k    = 128
n_embd_head_v    = 128
n_gqa            = 4
n_embd_k_gqa     = 1024
n_embd_v_gqa     = 1024
f_norm_eps       = 0.0e+00
f_norm_rms_eps   = 1.0e-05
f_clamp_kqv      = 0.0e+00
f_max_alibi_bias = 0.0e+00
f_logit_scale    = 0.0e+00
n_ff             = 14336
n_expert         = 0
n_expert_used    = 0
causal attn      = 1
pooling type     = 0
rope type        = 0
rope scaling     = linear
freq_base_train  = 500000.0
freq_scale_train = 1
n_ctx_orig_yarn  = 8192
model params     = 8.03 B
```

# Model Architecture

<center><img width="600" src="https://github.com/user-attachments/assets/85ef60d7-cf56-49d7-bc7d-4edec4bb7502"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>


training stability를 maximize하기 위해 MOE(mixture-of-experts) model이 아닌 standard decoder-only transformer model architecture를 선택하였다. 

학습 시, supervised fine-tuning과 direct preference optimization 과정을 거침으로써 각 round에서 highest quality synthetic data를 생성하고 각 capability’s performance를 향상시킬 수 있었다.

405B의 경우 15 trillion(15조)이상의 token으로 학습을 수행했다. 이와 같은 규모의 학습을 수행하며 합리적인 시간 내에 원하는 결과를 얻기 위해 Meta 팀은 full training stack을 최적화하고, 16,000개 이상의 H100 GPU를 사용하였다. 

이전 버전들과 비교하여 data 품질도 개션시켰다. careful pre-processing과 curation pipeline 개발을 통해 pre-training data의 품질을 개선시켰고, 더 엄격한 품질 보증 및 filtering approach를 통해 post-training data를 개선시켰다.

LLM의 scaling laws(확장 법칙)에 따라 이번 flagship model(405B)모델은 8B와 70B에 비해 높은 성능을 보이다. 따라서 405B를 활용하여 8B와 70B의 post-training quality를 개선하였다.

# Pre-Training

- **keyword**
  - a large-scale training corpus 선별 및 filtering
  - scaling law 개발
  - 대규모 모델에 대한 효율적 pre-training 방법 개발

LLaMa 3.1 논문에서는 이전 LLaMa 1이나 2의 논문보다 학습 방법을 더 구체적으로 적어 놓았다.

## Pre-Training Data

LLaMa 3.1 사전 학습에는 web 수집 데이터가 사용되었다. Web에서 수집된 데이터는 다양한 cleaning 작업을 거쳐서 모델 학습에 적합하도록 만들었다. 

데이터를 수집한 웹 출처는 정확히 적혀 있지 않지만 성인 콘텐츠나 유해한 웹사이트에서는 데이터를 수집하지 않았다고 한다.

- **PII and safety filtering**<br>
  PII(개인 식별 정보)나 안전하지 않은 내용이 포함된 데이터를 제거하는 filtering 작업을 수행한다.
- **Text extraction and cleaning**<br>
  - 웹 페이지의 HTML 코드를 분석하여 불필요한 부분(광고 등)은 제외하고 주요 내용만 추출한다.
  - 수학이나 코드와 같은 내용은 따로 특수 처리하여 해당 구조를 유지시킨다. 예를 들어 수학 공식이 image로 삽입되어 있는 경우, 해당 이미지의 alt attribute를 함께 유지하여 수집한다.
  - 실험 결과, Markdown 형식의 문서보다 일반 text 형식의 문서를 학습하는 것이 모델 성능 향상에 좋은 것으로 확인되어 Markdown 문법으로 작성된 데이터는 제거하였다.
- **De-duplication**
  - URL-level de-duplication: 동일한 URL에서 가져온 데이터 중 최신 버전만 남긴다.
  - Document-level de-duplication: 비슷한 문서가 있으면 하나만 남기고 제거한다.
  - Line-level de-duplication: 텍스트에서 반복되는 line은 제거한다. -> 이 과정이 모델 성능 향상에 도움이 된 것 같다고 한다.
- **Heuristic filtering**
  - 추가적으로 heuristic하게 낮은 품질의 문서, 비정상적인 문서, 반복이 많은 문서 등을 제거한다.
  - 또한 “dirty word” counting을 통해 성인 콘텐츠를 걸러냈다.
  - 특정 toekn이 과도하게 포함된 문서도 걸러냈다.
- **Model-based quality filtering**
  데이터 품질 분류를 위한 모델을 사용하였다.
  - Wikipedia의 정보가 참조될만한 품질인지 확인하는 fasttext 기반 classifier
  - LLaMa2의 예측 결과를 기반으로 문서 품질 평가 task를 학습한 Roberta-based classifier<br>
  등을 사용했다.
- **Code and reasoning data**
  웹에서 수집한 코드나 수학 관련 데이터는 일반 text과는 다른 처리 pipeline을 타게 했다. <br>
  LLaMA2에 의해 annotated된 data를 학습한 DistilRoberta모델로 품질을 분류했다.
- **Multilingual data**
  - fasttext-based language identification model을 사용하여 문서를 176개의 언어로 분류하고 각 언어별로 문서와 중 중복을 제거한다.
  - 언어별로 추가적인 filtering과 품질 평가를 수행한다.

### Determining the Data Mix

high-quality language model을 만들기 위해서는 어떤 종류의 데이터를 얼마나 섞을지 잘 결정해야 한다. 그러기 위해 해당 데이터가 **"어떤 데이터"**인지 먼저 알아야 하고 어떤 데이터들을 **"각각 얼만큼씩"** 섞어야 할지 알아야 한다. 이를 위해 Meta 팀은 아래 두 방법을 사용하였다.

- **Knowledge classification**<br>
  LLaMa 3.1의 학습데이터에는 방대한 양의 Web data가 포함되어 있다. 이 데이터가 어떤 종류의 지식을 담고 있는지 분류하는 모델을 만들어 web에 과도하게 분포되어 있는 엔터 관련 데이터를 덜어냈다.
- **Scaling laws for data mix**<br>
  어떤 data mix가 모델 성능에 좋은 영향을 미칠지 확인하기 위해 작은 모델들 여러개를 여러 비율의 데이터들로 학습시켜 보고 그 결과를 기반으로 큰 모델이 어떤 성능을 낼지 예측하는 실험을 수행했다.
  - Data mix summary:<br>
    - general knowledge token: roughly 50%
    - mathematical and reasoning token: 25%
    - code token:  17%
    - multilingual token: 8%

### Annealing Data

모델 성능을 더 높이기 위해, '애닐링'이라는 기법을 사용한다. 이 기법은 소량의 고품질 데이터를 추가로 사용해 모델을 미세 조정하는 것이다.

Annealing을 통해 소규모 dataset이 모델 성능 향상에 얼마나 기여했는지 확인할 수 있다. 따라서 이 방법을 사용하면 모덴 데이터에 대해 모두 다 sacling law 실험을 하지 않고도 data 각각의 가치를 효율적으로 판단할 수 있다. 


## Scaling, Efficiency

효율적인 모델 학습을 위해 Meta 팀은 다양한 방법을 적용하였다.

### 4D parallelism

효율적인 학습을 위해 네 가지 서로 다른 병렬 처리 방법을 결합하여 학습을 진행하였다. 

1. **Tensor Parallelism(TP)**<br>
  모델의 개별적인 weight tensor를 여러 multiple chunk로 나눠 다른 GPU device에 배치한다. 이는 각 GPU가 모델의 일부만을 처리하여 전체 메모리 사용량을 줄일 수 있게 한다.
2. Pipeline Parallelism, PP
   model을 layer 단위로 나눠 각 layer를 서로 다른 GPU에서 동시에 처리하도록 한다. 이는 모델의 각 layer들이 동시에 처리될 수 있어 전체 학습 속도가 빨라진다.
3. Context Parallelism, CP
  input context를 segments로 나누어 long sequence length input을 처리할 때 발생하는 memory bottleneck문제를 완화시켰다.
4. Data Parallelism, DP
   데이터를 여러 GPU에서 동시에 처리한 후 각 GPU에서 처리한 결과를 동기화한다. 이 과정에서  model, optimizer, gradient는 GPU들에 나누어 저장한다.

위와 같은 방법으로 GPU가 얼마나 효율적으로 연산을 수행했는지 나타내는 지표인 BF16 Model FLOPs Utilization (MFU; Chowdhery et al. (2023))에 대해 38-43%를 달성했다고 한다.

하지만 pipeline 병렬 처리에서 몇 가지 문제점이 발견되어 개선 중이라고 한다.

- Batch size constraint: pipeline 병렬 처리에서는 GPU당 지원되는 배치 크기에 제약이 있다.
- Memory imbalance: 모델의 embedding layer와 warm-up micro-batch 때문에 학습 시작 부분에서 메모리 소비가 불균형하다는 문제가 있다.
- Computation imbalance: 모델의 마지막 layer에서 결과와 loss를 계산할 때 전체 처리 속도의 병목이 생긴다.

## Scaling Laws for Downstream Tasks

Scaling Law는 모델 성능이 model size, data size, 그리고 compute resource에 따라 어떻게 변화하는지를 설명하는 법칙이다. Scaling Law는 주어진 compute resource에 대해 최적의 성능을 낼 수 있는 model과 data size를 선택하는 데에 도움을 준다. 예를 들어 어느정도의 compute resource를 투입하면 어느정도 크기의 모델이 최적일지, 이 모델이 얼마나 잘 작동할지 예측하는 것에 도움을 준다.

> **Parameter Scaling**<br>
>
> 일반적으로 파라미터 수가 증가하면 모델의 표현 능력이 향상되어 성능이 좋아지지만, 이는 데이터 크기와 컴퓨팅 자원의 충분한 지원이 있어야 한다.<br>
>
> **Data Scaling**<br>
>
> 충분히 큰 데이터셋이 주어지면 모델은 더 일반화된 패턴을 학습할 수 있어 성능이 향상된다. 하지만 데이터 크기의 증가에 따라 성능 향상 속도는 감소할 수 있다.<br>
>
> **Compute Scaling**<br>
>
> compute resouce가 충분할수록 모델 학습 시간이 단축되거나, 더 큰 모델을 훈련시킬 수 있게 되어 성능이 향상될 수 있다.<br>

하지만 기존의 scaling law에는 몇 가지 한계가 있었다. 

1. **제한적인 prediction** : 기존의 scaling law는 단순히 다음단어를 얼마나 잘 예측하는지에 중점을 두고 있어 특정 task에서 얼마나 좋은 성과를 낼지는 알기 어렵다. 
2. **불확실성** :  small compute budget으로 만든 scaling law이기 때문에 더 큰 computing resource를 투입했을 때에 해당 법칙이 잘 맞을지 불확실하다.

*(1) Existing scaling laws typically predict only next-token prediction loss rather than specific
benchmark performance. (2) Scaling laws can be noisy and unreliable because they are developed based on
pre-training runs conducted with small compute budgets (Wei et al., 2022b).*

Meta 팀은 위 문제를 해결하기 위해 아래 두 방법론을 적용하였다:

1. downstream task에 대해 compute-optimal model의 negative log-likelihood와 training FLOP 간의 상관관계를 찾는다.
2. 모델의 성능과 compute resource 간의 관계식을 만든다.

위 두 방법론을 기반으로 Meta 팀은 다양한 계산 자원(FLOPs)과 모델 크기(파라미터 수)를 사용해 모델을 학습시켜 이를 통해 특정 FLOPs에서 가장 높은 성능을 내는 모델들을 찾아 이를 예측하는 compute-optimal model을 만들었다. 즉, specific compute budget이 주어졌을 때 몇 개의 training token을 사용하는 것이 최적인지 예측하는 관계식을 만들었다.

이 관계식을 통해 Meta 팀은 모델의 downstream task 성능을 예측하였다. 

주어진 floating point operations (FLOPs)에 따라 원하는 benchmark 성능을 얻기 위해 GPU를 얼마나 오래 실행해야 하는지를 예측한 것이다. 컴퓨팅 최적화 모델에 대해 특정 training flop 수를 기준으로 downstream task 성능을 예측할 수 있다고 한다.

> FLOPs

> FLOPs는 연산 복잡도를 나타낸다.<br> FLOPs는 딥러닝과 머신러닝에서 모델의 연산량과 복잡도를 나타내는 중요한 지표로, 모델 선택과 최적화 과정에서 고려해야 할 요소이다.<br>

> FLOPs는 딥러닝 모델의 효율성과 성능을 비교하는 데 많이 사용된다.<br>
> 예를 들어, 모델 A가 10 GFLOPs를 요구하고, 모델 B가 20 GFLOPs를 요구한다면, 일반적으로 모델 A가 더 적은 연산 자원을 소모하는 효율적인 모델이라고 할 수 있다. <br>
> 하지만 반드시 FLOPs가 낮다고 해서 더 좋은 성능을 보장하는 것은 아니다. 모델의 성능과 효율성을 함께 고려해야 한다.<br>

> 하지만 모델이 실제로 얼마나 빠르게 실행되는지는 하드웨어와 최적화 방법에 따라 달라진다.<br>
> 따라서 같은 FLOPs 값을 가진 모델도 하드웨어에 따라 실행 속도는 달라질 수 있다. <br>

## Training Recipes

Meta 팀은 학습 시 수렴 속도는 느리지만 가장 basic한 모델을 채택함으로써 학습의 안정성을 높였다. 최근 많이 사용되는 복잡한 MOE(mixture-of-experts) 모델이 아닌 기본적인 standard dense Transformer model architecture에 대해 약간의 조작만 추가한 모델을 사용하였다. 

기본적인 방식으로 pre-trained model을 구출한 후  post-training을 수행하였다. post-training 단계에서도 supervised finetuning (SFT), rejection sampling (RS), direct preference optimization (DPO)을 사용하여 복잡한 알고리즘은 사용하지 않았다. 

### Annealing
모델 성능 향상을 위해 pre-triaining 과정에 Annealing 기법이 적용되었다. Annealing은 학습 중 learning rate를 점진적으로 낮추는 방법이다. 이는 model parameter를 정확하게 조정하여 학습 중 파라미터가 크게 업데이트되어 모델이 불안정해지는 경우를 방지한다. Annealing 단계에서는 데이터 품질이 중요하다. 따라서 가능한 높은 품질의 데이터를 학습시켜 성능과 일반화를 향상시킨다.


================================================================================================================================












# Post-Training

## Instruction and chat fine-tuning

LLaMa 3.1 Instruction model 개발을 위한 post-training 과정에서 집중한 것은 아래 세 가지이다.

- 더 많은 기능 지원
- 128K context window 구현
- model 크기 증가

instruction-following capability 개선과정에 LLaMa 3.1 405B를 활용하였다. 

Instruction model은 pre-trained model위에 여러 round의 alignment 작업을 수행하여 개발되었. 각 round에는 Supervised Fine-Tuning (SFT), Rejection Sampling (RS), Direct Preference Optimization (DPO)가 포함된다. 


## Reward Model

llama 3.1에서 사용된 Reward model은 RLHF가 아닌 rejection sampling 방식으로 작동된다 (Direct Preference Optimization (DPO)). 

Reward Model은 pre-/post-training 단계에서 각각 상이하게 사용되었다. 

- Pre-training: 이 단계에서 사용된 Reward Model은 distilBERT를 기반으로 한다. 이 모델은 데이터 품질을 효율적으로 분류하는 데에 강점을 지니고 있어 initial training 단계에서 사용하였다.
- Post-training: 이 단계에서는 llama 3.1 initial model을 Reward Model로 사용하였다. 


# Training Data

Meta 팀의 모델 개선 및 데이터 품질 개선 방법론은 flywheel 전략을 떠오르게 한다.

하지만 모델이 모델의 실수를 학습할 수 있다는 문제점을 발견하여 verifier model을 학습 과정에 삽입하였다.

> flywheel 전략

> 비즈니스에 필요한 다양한 항목들이 서로 유기적으로 연결이 되면서 한쪽의 힘이 한쪽으로 전달되고 이 힘이 다시 다른 쪽으로 전달되는 과정을 통해서 시너지를 만들어내는 것을 말한다.

## filtering

Meta 팀은 LLaMa 3 개발 시 LLaMa3에 사용될 데이터를 filtering 하기 위해 LLaMa 2를 사용한 것과 같이 LLaMa 3.1 학습에도 LLaMa 3을 사용하였다.

## multilingual 

Meta 팀은 pre-training 학습 중간에 분기를 만들어 90%의 multilingual token으로 구성된 data mix를 사용해 사전학습을 이어감으로써 multilingual model을 구축하였다. 해당 모델 구축 이유는 non-English 데이터 수집이다. 매우 크고 성능 좋은 모델을 사용해서 작은 모델의 학습 데이터를 생성하기 위해 multilingual expert model을 학습시킨 것이다. 

## data source 

데이터의 출처에 대해서는 논문에서 단지 "from a variety of data sources"라고만 기재되어 있다. 최근 Reddit과 Twitter와 같이 인공지능 학습 데이터로 빈번히 사용되던 출처들이 데이터에 대한 요금을 부과하고 있다고 한다. Meta 팀은 허가 받지 않은 데이터는 사용하지 않았다고는 하지만 데이터 출처를 명확히 밝히지도 않았다.

## +

html로 긁어 온 데이터가 대부분이기 때문에, 수학과 코드 데이터의 품질을 향상시키기 위해 indent, 기호 등을 잘 처리하도록 parser 정교화 작업을 하여 수학 및 코드 분야 데이터 품질을 높였다고 한다.

# Reasoning and Mathematics



# Evaluation

LLaMa 3.1는 다양한 언어에 대한 150개 이상의 benchmark에 대해 성능 평가를 진행하였고, real-world scenario를 통한 human evaluation도 진행하였다. 아래 표 이번 LLaMa 3.1의 flagship model인 405B 모델의 실험 결과이다. GPT-4, GPT-4o, Claude 3.5 Sonnet과 비교하였을 때 경쟁력있는 성능을 보이는 것으로 확인되었다. 8B와 70B또한 비슷한 크기의 모델들과 비요하였을 때 경쟁력있는 성능을 보였다.

<center><img width="600" src="https://github.com/user-attachments/assets/fbe09953-9432-41a4-8d3a-a13fc2eaf751"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>

<center><img width="600" src="https://github.com/user-attachments/assets/be723b15-099a-4459-8a43-b6438de5e9ce"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>

<center><img width="600" src="https://github.com/user-attachments/assets/a5bd11e8-cce6-472c-84c0-f8de73f426a6"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>




# Reference

> https://ai.meta.com/blog/meta-llama-3-1/

> https://llama.meta.com/

> https://ai.meta.com/research/publications/the-llama-3-herd-of-models/
