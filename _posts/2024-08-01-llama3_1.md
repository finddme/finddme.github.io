---
title: "LLaMa 3.1"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}










# LLaMa 3.1 

- 405B, 70B, 8B 버전이 공개되었다.
- INT8, FP8로 양자화 버전 공개 ([INT4양자화 버전 meta 공식 x](https://huggingface.co/collections/neuralmagic/llama-31-quantization-66a3f907f48d07feabb8f300))
- context length를 128K로 확장, 8개 언어를 지원(Guard model)
- Llama 3.1 405B는 최초 frontier-level open source. 지금까지 open source LLM은 closed source LLM에 비해 성능이 좋지 않았지만 LLaMa 3.1 405B는  closed source model에 견줄만한 성능을 보인다.
reference system을 포함하고 있다. 
- 이번 release에서는 8B와 70B model의 upgrad가 있다. 두 모델은 multilingual을 지원하며, context length가 128K로 크게 증가했다. 또한 state-of-the-art tool을 사용하여 전반적인 reasoning capability를 향상시켰다. 이를 통해 이번에 공개된 LLaMa 모델은 advanced use case(long-form text summarization, multilingual conversation, coding)성능이 크게 향상되었다.

# LLaMa 3 vs LLaMa 3.1

## context length and multilingual token

|                    | LLaMa 3.1 | LLaMa 3 |
| ------------------ | --------- | ------- |
| context length     | 128K      | 8K      |
| multilingual token | 8%        | 5%      |


## math and reasoning capability

LLaMa 3.1이 LLaMa 3보다 수학 분야와 추론 능력이 더 뛰어나다. Meta의 tech blog([llama3]( https://ai.meta.com/blog/meta-llama-3/), [llama3.1]( https://ai.meta.com/blog/meta-llama-3-1/))에 아래와 같은 내용이 기재되어 있다:

| (8B) | LLaMa 3.1               | LLaMa 3            |
| ---- | ----------------------- | ------------------ |
| MATH | MATH (0-shot, CoT) 73.0 | MATH (5-shot) 68.4 |
| MMLU | 51.9                    | 30.0               |

## architecture

LLaMa 3과 LLaMa 3.1은 동일한 dense network architecture를 가지고 있다. 

(8B 기준)
- decoder-only transformer
- 32 layers
- 4K embedding dimension
- 32 heads
- 8KV heads
- rotary positional encoding (RoPE) 사용
- 4 grouped query attention (GQA)
- 15T(15조) token으로 사전 훈련
- 8,192개의 token sequence로 사전 훈련
- tokenizer vocab size 128K

**LLaMa 3.1**

```
n_vocab          = 128256
n_merges         = 280147
vocab_only       = 0
n_ctx_train      = 131072
n_embd           = 4096
n_layer          = 32
n_head           = 32
n_head_kv        = 8
n_rot            = 128
n_swa            = 0
n_embd_head_k    = 128
n_embd_head_v    = 128
n_gqa            = 4
n_embd_k_gqa     = 1024
n_embd_v_gqa     = 1024
f_norm_eps       = 0.0e+00
f_norm_rms_eps   = 1.0e-05
f_clamp_kqv      = 0.0e+00
f_max_alibi_bias = 0.0e+00
f_logit_scale    = 0.0e+00
n_ff             = 14336
n_expert         = 0
n_expert_used    = 0
causal attn      = 1
pooling type     = 0
rope type        = 0
rope scaling     = linear
freq_base_train  = 500000.0
freq_scale_train = 1
n_ctx_orig_yarn  = 131072
model params     = 8.03 B
```

**LLaMa 3**
```
n_vocab          = 128256
n_merges         = 280147
n_ctx_train      = 8192
n_embd           = 4096
n_layer          = 32
n_head           = 32
n_head_kv        = 8
n_rot            = 128
n_swa            = 0
n_embd_head_k    = 128
n_embd_head_v    = 128
n_gqa            = 4
n_embd_k_gqa     = 1024
n_embd_v_gqa     = 1024
f_norm_eps       = 0.0e+00
f_norm_rms_eps   = 1.0e-05
f_clamp_kqv      = 0.0e+00
f_max_alibi_bias = 0.0e+00
f_logit_scale    = 0.0e+00
n_ff             = 14336
n_expert         = 0
n_expert_used    = 0
causal attn      = 1
pooling type     = 0
rope type        = 0
rope scaling     = linear
freq_base_train  = 500000.0
freq_scale_train = 1
n_ctx_orig_yarn  = 8192
model params     = 8.03 B
```

# Model Architecture

<center><img width="600" src="https://github.com/user-attachments/assets/85ef60d7-cf56-49d7-bc7d-4edec4bb7502"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>


training stability를 maximize하기 위해 MOE(mixture-of-experts) model이 아닌 standard decoder-only transformer model architecture를 선택하였다. 

학습 시, supervised fine-tuning과 direct preference optimization 과정을 거침으로써 각 round에서 highest quality synthetic data를 생성하고 각 capability’s performance를 향상시킬 수 있었다.

405B의 경우 15 trillion(15조)이상의 token으로 학습을 수행했다. 이와 같은 규모의 학습을 수행하며 합리적인 시간 내에 원하는 결과를 얻기 위해 Meta 팀은 full training stack을 최적화하고, 16,000개 이상의 H100 GPU를 사용하였다. 

이전 버전들과 비교하여 data 품질도 개션시켰다. careful pre-processing과 curation pipeline 개발을 통해 pre-training data의 품질을 개선시켰고, 더 엄격한 품질 보증 및 filtering approach를 통해 post-training data를 개선시켰다.

LLM의 scaling laws(확장 법칙)에 따라 이번 flagship model(405B)모델은 8B와 70B에 비해 높은 성능을 보이다. 따라서 405B를 활용하여 8B와 70B의 post-training quality를 개선하였다.

# Training Recipes

Meta 팀은 학습 시 수렴 속도는 느리지만 가장 basic한 모델을 채택함으로써 학습의 안정성을 높였다. 최근 많이 사용되는 복잡한 MOE(mixture-of-experts) 모델이 아닌 기본적인 standard dense Transformer model architecture에 대해 약간의 조작만 추가한 모델을 사용하였다. 

기본적인 방식으로 pre-trained model을 구출한 후  post-training을 수행하였다. post-training 단계에서도 supervised finetuning (SFT), rejection sampling (RS), direct preference optimization (DPO)을 사용하여 복잡한 알고리즘은 사용하지 않았다. 

## Pre-training Annealing

  학습 과정에는 Annealing 기법이 적용되었다. Annealing은 학습 중 learning rate를 점진적으로 낮추는 방법이다. 이는 model parameter를 정확하게 조정하여 학습 중 파라미터가 크게 업데이트되어 모델이 불안정해지는 경우를 방지한다. Annealing 단계에서는 데이터 품질이 중요하다. 따라서 가능한 높은 품질의 데이터를 학습시켜 성능과 일반화를 향상시킨다.

## Scaling Laws for Downstream Tasks

Meta 팀은 compute-optimal model을 사용하여 benchmark dataset에 대한 LLaMa 3의 downstream task 성능을 예측하였다.



## Instruction and chat fine-tuning

LLaMa 3.1 Instruction model 개발을 위한 post-training 과정에서 집중한 것은 아래 세 가지이다.

- 더 많은 기능 지원
- 128K context window 구현
- model 크기 증가

instruction-following capability 개선과정에 LLaMa 3.1 405B를 활용하였다. 

Instruction model은 pre-trained model위에 여러 round의 alignment 작업을 수행하여 개발되었. 각 round에는 Supervised Fine-Tuning (SFT), Rejection Sampling (RS), Direct Preference Optimization (DPO)가 포함된다. 


## Reward Model

llama 3.1에서 사용된 Reward model은 RLHF가 아닌 rejection sampling 방식으로 작동된다 (Direct Preference Optimization (DPO)). 

Reward Model은 pre-/post-training 단계에서 각각 상이하게 사용되었다. 

- Pre-training: 이 단계에서 사용된 Reward Model은 distilBERT를 기반으로 한다. 이 모델은 데이터 품질을 효율적으로 분류하는 데에 강점을 지니고 있어 initial training 단계에서 사용하였다.
- Post-training: 이 단계에서는 llama 3.1 initial model을 Reward Model로 사용하였다. 



## +

html로 긁어 온 데이터가 대부분이기 때문에, 수학과 코드 데이터의 품질을 향상시키기 위해 indent, 기호 등을 잘 처리하도록 parser 정교화 작업을 하여 수학 및 코드 분야 데이터 품질을 높였다.


# Evaluation

LLaMa 3.1는 다양한 언어에 대한 150개 이상의 benchmark에 대해 성능 평가를 진행하였고, real-world scenario를 통한 human evaluation도 진행하였다. 아래 표 이번 LLaMa 3.1의 flagship model인 405B 모델의 실험 결과이다. GPT-4, GPT-4o, Claude 3.5 Sonnet과 비교하였을 때 경쟁력있는 성능을 보이는 것으로 확인되었다. 8B와 70B또한 비슷한 크기의 모델들과 비요하였을 때 경쟁력있는 성능을 보였다.

<center><img width="600" src="https://github.com/user-attachments/assets/fbe09953-9432-41a4-8d3a-a13fc2eaf751"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>

<center><img width="600" src="https://github.com/user-attachments/assets/be723b15-099a-4459-8a43-b6438de5e9ce"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>

<center><img width="600" src="https://github.com/user-attachments/assets/a5bd11e8-cce6-472c-84c0-f8de73f426a6"></center>
<center><em style="color:gray;">Mixture-of-Agents Enhances Large Language Model Capabilities</em></center><br>




# Reference

> https://ai.meta.com/blog/meta-llama-3-1/

> https://llama.meta.com/
