---
title: "Self-RAG + CRAG : Query Router | Reranker | Relevant Grader | Hallucination Grader | Answer Grader"
category: Dev Log
tag: Development
---







* ëª©ì°¨
{:toc}












# Git Repository

[https://github.com/finddme/SELF_RAG_CRAG](https://github.com/finddme/SELF_RAG_CRAG)

- ë³¸ ê°œë°œë¬¼ì€ì€ Self-Reflective RAGê³¼ Corrective RAGë¥¼ ìœµí•©í•œ RAG systemì´ë‹¤.
- [SELF-RAG: Learning to Retrieve, Generate and Critique through self-reflection](https://arxiv.org/abs/2310.11511)ì—ì„œ ì‚¬ìš©ëœ reflection token ì˜ˆì¸¡ modelì€ ë³¸ ê°œë°œë¬¼ì—ì„œ LLMìœ¼ë¡œ ëŒ€ì²´í•˜ì˜€ê³ , Corrective RAG chunk ì í•©ì„± íŒë‹¨ ëª¨ë¸ë˜í•œ ë³¸ ê°œë°œë¬¼ì—ì„œ LLMìœ¼ë¡œ ëŒ€ì²´í•˜ì˜€ë‹¤. (ê°ê° ë³¸ ê°œì‹œë¬¼ì—ì„œëŠ” Query Routerì™€ Answer Relevant Graderë¡œ ì¹­í•˜ê³  ìˆë‹¤.)
- í•´ë‹¹ ê°œë°œë¬¼ì€ ìµœëŒ€í•œ ë‹¤ì–‘í•œ Graderê°€ í¬í•¨ëœ pipelineì„ ë‹¤ë£¨ì§€ë§Œ ì‹¤ìš©ì ì¸ ë°©ë²•ì´ë¼ê³ ëŠ” í•  ìˆ˜ ì—†ë‹¤. pipelineìƒ ë‹¨ê³„ ë³„ë¡œ LLMì„ 1~Në²ˆ í†µê³¼í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì†ë„ê°€ ë§ì´ ëŠë ¤ì§„ë‹¤.
- ë˜í•œ ê²½ìš°ì— ë”°ë¼ Retrieval ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ search ë‹¨ê³„ì—ì„œ ê³„ì† webserchë¥¼ ì§„í–‰í•  ê°€ëŠ¥ì„±ë„ ìˆê³ , ë‹µë³€ì´ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” ë‹µë³€ ìƒì„±ì„ ì—¬ëŸ¬ë²ˆ í•  ê°€ëŠ¥ì„±ì´ ìˆê¸° ë•Œë¬¸ì— ê° ë‹¨ê³„ë³„ë¡œ ìµœëŒ€ ì§„í–‰ íšŸìˆ˜ë¥¼ ì œí•œí•˜ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤.
- ë³¸ ê°œë°œë¬¼ì˜ ëª©ì ì€ SELF-RAGê³¼ CRAGì„ í•œë²ˆì— êµ¬í˜„í•´ ë³´ê³ , ê° ë‹¨ê³„ë³„ ì ìš© ê°€ëŠ¥ ê¸°ìˆ ì„ ìµíˆê¸° ìœ„í•¨ì— ìˆë‹¤.

# RAG ê°œë°œ í™˜ê²½ë³„ ì‚¬ìš© ê°€ëŠ¥ ê¸°ìˆ 

- Internet-accessible env/Closed network envì™€ ìœ ë£Œ/ë¬´ë£Œì— í¬í•¨ë˜ëŠ” ê¸°ìˆ ë“¤ì€ ê°ê° ìƒí˜¸ ì¤‘ì²©ë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.
- ì•„ë˜ëŠ” ì„œë²„ íì‡„ ì—¬ë¶€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê° ê¸°ìˆ ë“¤ì„ ë¶„ë¥˜í•œ í‘œì´ë‹¤.
  - ìœ ë£ŒğŸŸ¡/ë¬´ë£ŒğŸ”µ

<html>
  <head>
    <style type="text/css">
      .line{border-bottom: 1px solid #BDB8C1;}
      .line2{border-bottom: 2px solid #BDB8C1;}
      .line3{border-bottom: 1px solid #BDB8C1; background-color: #F7F7F7;}
      .line4{border-bottom: 2px solid #BDB8C1; background-color: #F7F7F7;}
      table, th, td {
         border:1px solid #BDB8C1;
         background-color: #FFFFFF;
       }
    </style>
   </head>
   <body>
     <table style="border-collapse:collapse" align=center>
       <tr>
         <th class="line2" bgcolor="#F8F7F9"> </th>
         <th class="line4" bgcolor="#F8F7F9">Internet-accessible env</th><th class="line4" bgcolor="#F8F7F9">Closed network env</th>
       </tr>
       <tr>
         <td class="line3"><strong>RAG Framework</strong></td>
         <td class="line" colspan="2" valign=middle>
           <li>ğŸ”µlangchain</li>
           <li>ğŸ”µllamaindex</li>
         </td>
       </tr>
       <tr>
         <td class="line3"><strong>Multi-Agent Framework</strong></td>
         <td class="line" colspan="2" valign=middle>
           <li>ğŸ”µlangGraph</li>
           <li>ğŸ”µAutoGen</li>
           <li>ğŸ”µCrew AI</li>
         </td>
       </tr>
       <tr>
         <td class="line3"><strong>LLM</strong></td>
         <td class="line">
           <li>ğŸŸ¡openai</li>
           <li>ğŸŸ¡claude</li>
           <li>...</li>
         </td>
         <td class="line">
           <li>ğŸ”µhuggingface model</li>
           <li>ğŸ”µlocal model</li>
         </td>
       </tr>
       <tr>
         <td class="line3"><strong>Inference accelerate</strong></td>
         <td class="line">
           <li>ğŸ”µGROQ</li>
         </td>
         <td class="line">
           <li>ğŸ”µvllm</li>
           <li>ğŸ”µStreamer+Threading</li>
           <li>ğŸ”µLMDeploy</li>
           <li>ğŸ”µ<a href="https://github.com/unslothai/unsloth">unsloth</a></li>
           <li>ğŸ”µ<a href="https://huggingface.co/blog/assisted-generation">Greedy(Speculative) Decoding with assisted generation</a></li>
           <li>ğŸ”µAirLLM</li>
           <li>ğŸ”µ<a href="https://github.com/pytorch-labs/gpt-fast">gpt-fast</a></li>
           <li>ğŸ”µ<a href="https://huggingface.co/docs/optimum/en/intel/openvino/inference">OpenVINO</a></li>
         </td>
       </tr>
       <tr>
         <td class="line3"><strong>text embedding</strong></td>
         <td class="line">
           <li>ğŸŸ¡openai</li>
         </td>
         <td class="line">
           <li>ğŸ”µSentencetransformers</li>
           <li>ğŸŸ¡Google Vertex embedding</li>
           <li>...</li>
         </td>
       </tr>
       <tr>
         <td class="line3"><strong>vector DB</strong></td>
         <td class="line" colspan="2" valign=middle>
           <li>ğŸ”µWevieate</li>
           <li>ğŸ”µFaiss</li>
           <li>...</li>
         </td>
       </tr>
       <tr>
         <td class="line3"><strong>web search</strong></td>
         <td class="line">
           <li>ğŸ”µTavily</li>
         </td>
         <td class="line">
           <li>ğŸ”µlangchain_community.utilities .oooWrapper</li>
         </td>
       </tr>
       <tr>
         <td class="line3"><strong>Reranker</strong></td>
         <td class="line">
           <li>ğŸ”µcohere</li>
         </td>
         <td class="line">
           <li>ğŸ”µFlagEmbeddingReranker</li>
           <li>ğŸ”µhuggingface rerank model</li>
           <li>ğŸ”µCrossEncoder</li>
           <li>ğŸ”µJinaRerank</li>
           <li>ğŸ”µCohereRerank</li>
         </td>
       </tr>
       <tr>
         <td class="line3"><strong>Application Interface</strong></td>
         <td class="line" colspan="2" valign=middle>
           <li>FastAPI (graphic interface o)</li>
           <li>Flask</li>
           <li>Gradio (graphic interface o)</li>
           <li>Streamlit (graphic interface o)</li>
           <li>cherrypy</li>
           <li>Django</li>
           <li>Chainlit</li>
           <li>...</li>
         </td>
       </tr>
   </table>
 </body>
</html>

# ê¸°ëŠ¥ë³„ êµ¬ì„± ìš”ì•½ 

<center><img width="1000" src="https://github.com/finddme/finddme.github.io/assets/53667002/8488df8b-14b3-4ecc-8ece-be2e140a8221"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

<center><img width="250" src="https://github.com/user-attachments/assets/39467eb1-0c1e-46d0-b16c-6e305c8ade48"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

- RAG Framework : Langchain
- Workflow control : LangGraph
- LLM : Mixtral-8x7b
- Inference accelerate : GROQ
- text embedding : sentence-transformers/all-MiniLM-L6-v2
- vector DB : Wevieate
- reranker : BAAI/bge-reranker-v2-m3
- chunk method : RecursiveCharacterTextSplitter
- web search : DuckDuckGoSearch
- Application Interface : Chainlit

# Embedding model : sentence-transformers/all-MiniLM-L6-v2

```
from sentence_transformers import SentenceTransformer
import numpy as np
embedd_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

def get_embedding_st(text) : 
    res = embedd_model.encode(text)
    res = list(map(float, res))
    return res
```

# Vector DB ì¤€ë¹„ : Weaviate

<center><img width="300" src="https://github.com/finddme/finddme.github.io/assets/53667002/888744e8-2bac-4e06-a875-d594142e9cef"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>


## Weaviate setting

1\. weaviate docker setting

weaviate docker-compose file: (https://weaviate.io/developers/academy/py/starter_multimodal_data/setup_weaviate/create_docker)[https://weaviate.io/developers/academy/py/starter_multimodal_data/setup_weaviate/create_docker]

```command
docker-compose -f wevieate.yml up -d
```

-> weaviate client url ìƒì„±

2\. create weaviate class

```python

client = weaviate.Client("weaviate client url")

class_obj = {
    "class": "Test",
    "vectorizer": "none",
}

client.schema.create_class(class_obj)
```

## prepare Documents for Retrieval

1\. crawling

```python
import requests
from bs4 import BeautifulSoup
soup1 = BeautifulSoup(html1, 'html.parser')
request1 = requests.get("https://finddme.github.io/")
html1 = request1.text
links1 = soup1.select('h4 > a')
urls=[]
tags=["llm","dev","natural"]
for link in links1:
    if link.has_attr('href'):
        href=link.get('href')
        for t in tags:
            if t in href.split("/")[1]:
                urls.append("https://finddme.github.io"+href)
```
2\. split / chunking

```python
docs = [WebBaseLoader(url).load() for url in urls]
docs_list = [item for sublist in docs for item in sublist]

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000, chunk_overlap=0
)
doc_splits = text_splitter.split_documents(docs_list)
```

```python
space_check=re.compile("\s{1,}")
chunks=[]
pass_first=["finddme ","âŠ¹  Portfolio","Â© 2024 viein_serenity_singularity_simplicity_savage. This work is liscensed under CC BY-NC 4.0."]
for i in doc_splits:
    c={'text':i.page_content, 'title':i.metadata["source"].split("/")[-2]}
    if c["text"].split("\n")[0] in pass_first:pass
    else:
        save_c={'text':re.sub(space_check," ",c['text']),'title':c['title']}
        chunks.append(save_c)
```

3. chunks save

```python
client = weaviate.Client("weaviate client url")

client.batch.configure(batch_size=100)

with client.batch as batch:
    for i, chunk in enumerate(chunks):
        try:
            vector = get_embedding_st(chunk["text"])
            # print(type(vector[0]))
            batch.add_data_object(data_object=chunk, class_name="Test", vector=vector)
        except Exception as e:
            # print(i, type(vector[0]), chunk)
            print("except",i)
            vector = get_embedding_st(chunk["text"])
            vector=list(map(float, vector))
            batch.add_data_object(data_object=chunk, class_name="Test", vector=vector)
            # print("sucsess")
        print("save",i)

```

# Query Router : Groq

```json
{'input': query, 'output':"websearch/vectorstore"}
```

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
"""
pydantic_v1
- https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/pydantic/
- íŠ¹ì • schemaì— ë§ê²Œ outputì„ êµ¬ì„±í•˜ë„ë¡ LLMì— quaryí•  ìˆ˜ ìˆê²Œí•˜ëŠ” parser.
"""

class RouteQuery(BaseModel):
    """Route a user query to the most relevant datasource."""

    datasource: Literal["vectorstore", "websearch"] = Field(
        ...,
        description="Given a user question choose to route it to web search or a vectorstore.",
    )

"""
ref.
https://python.langchain.com/v0.2/docs/integrations/chat/groq/
Supported Models: https://console.groq.com/docs/models
"""
llm = ChatGroq(temperature=0, groq_api_key=GROQ_API_KEY)
structured_llm_router = llm.with_structured_output(RouteQuery)

system = """You are an expert at routing a user question to a vectorstore or web search.
The vectorstore contains documents related to language processing, and artificial intelligence.
Use the vectorstore for questions on these topics. For all else, use web-search."""
route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)

question_router = route_prompt | structured_llm_router

def route_question(state):
    print("---ROUTE QUESTION---")
    question = state["question"]
    source = question_router.invoke({"question": question}) # output: datasource='websearch/vectorstore'
    if source.datasource == 'websearch':
        print("---ROUTE QUESTION TO WEB SEARCH---")
        return "websearch"
    elif source.datasource == 'vectorstore':
        print("---ROUTE QUESTION TO RAG---")
        return "vectorstore"

```

# Retrieve : Weaviate

<center><img width="200" src="https://github.com/finddme/finddme.github.io/assets/53667002/7f2966e5-3cd3-4d6b-b6e9-2c91e38fabae"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

```json
{'input': query, 'output':{"documents": documents, "question": query}}
```

```python
def retrieve(state):
    print("---RETRIEVE from Vector Store DB---")
    question = state["question"]

    # Retrieval
    query_vector = get_embedding(question)
    documents = client.query.get("Test", ["text","title"]).with_hybrid(question, vector=query_vector).with_limit(6).do()
    return {"documents": documents, "question": question}
```

# Reranker

```python
from FlagEmbedding import FlagReranker
reranker_model = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True, device="cpu")

def reranker_fr(state):
    print("---Reranking---")
    question = state["question"]
    documents = state["documents"]
    
    # Rerank
    sentence_pairs = [[question, rr["text"]] for rr in documents["data"]["Get"]["Test"]]
    similarity_scores = reranker_model.compute_score(sentence_pairs)
    paired_list = list(zip(similarity_scores, documents["data"]["Get"]["Test"]))
    paired_list.sort(key=lambda x: x[0],reverse=True)
    sorted_b = [item[1] for item in paired_list]
    documents["data"]["Get"]["Test"]=sorted_b
    return {"documents": documents, "question": question}
```

# Relevant Grader : langchain

<center><img width="200" src="https://github.com/finddme/finddme.github.io/assets/53667002/c09ef5b1-d706-4157-b0d8-c85af9c282e3"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

```json
{'input': {"documents": documents, "question": query}, 'output':"yes/no"}
```

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
"""
pydantic_v1
- https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/pydantic/
- íŠ¹ì • schemaì— ë§ê²Œ outputì„ êµ¬ì„±í•˜ë„ë¡ LLMì— quaryí•  ìˆ˜ ìˆê²Œí•˜ëŠ” parser.
"""
```

```python
class GradeDocuments(BaseModel):
    """Binary score for relevance check on retrieved documents."""

    binary_score: str = Field(description="Documents are relevant to the question, 'yes' or 'no'")

# LLM with function call 
structured_llm_grader_docs = llm.with_structured_output(GradeDocuments)

# Prompt 
system = """You are a grader assessing relevance of a retrieved document to a user question. \n 
    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""

grade_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
    ]
)

retrieval_grader_relevance = grade_prompt | structured_llm_grader_docs

def grade_documents(state):
    print("---CHECK DOCUMENT RELEVANCE TO QUESTION---")
    question = state["question"]
    documents = state["documents"]
    
    # Score each doc
    filtered_docs = []
    web_search = "No"
    for d in documents["data"]["Get"]["Test"]:
        score = retrieval_grader_relevance.invoke({"question": question, "document": d["text"]}) # output: GradeDocuments(binary_score='yes/no')
        grade = score.binary_score
        # Document relevant
        if grade.lower() == "yes":
            print("---GRADE: DOCUMENT RELEVANT---")
            filtered_docs.append(d)
        # Document not relevant
        else:
            print("---GRADE: DOCUMENT NOT RELEVANT---")
            # We do not include the document in filtered_docs
            # We set a flag to indicate that we want to run web search
            web_search = "Yes"
            continue
    return {"documents": filtered_docs, "question": question, "web_search": web_search}
```


# Web search : DuckDuckGoSearch

<center><img width="200" src="https://github.com/finddme/finddme.github.io/assets/53667002/55a7f63f-87e0-4bd2-9027-73953ec9ea2d"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

```python
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from langchain.schema import Document
def web_search_ddg(state):
    print("---WEB SEARCH. Append to vector store db---")
    question = state["question"]
    documents = state["documents"]

    # Web search
    search_res=search.run(question)
    search_res1=[]
    for s in search_res.split(", title:"):
        search_res1.append(s.replace("[snippet: ","").replace("]",""))
    web_results="\n".join(search_res1)
    web_results = Document(page_content=web_results)
    if documents is not None:
        documents.append(web_results)
    else:
        documents = [web_results]
    return {"documents": documents, "question": question}

```

```python
def decide_to_generate(state):
    print("---ASSESS GRADED DOCUMENTS---")
    question = state["question"]
    web_search = state["web_search"]
    filtered_documents = state["documents"]

    if web_search == "Yes":
        # All documents have been filtered check_relevance
        # We will re-generate a new query
        print("---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---")
        return "websearch"
    else:
        # We have relevant documents, so generate answer
        print("---DECISION: GENERATE---")
        return "generate"
```

# Generate : Groq

<center><img width="200" src="https://github.com/finddme/finddme.github.io/assets/53667002/21d6fa1a-3ad6-428b-8d72-9be1cd9e9f15"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

```json
{'input': {"documents": documents, "question": query} + prompt, 'output':"llm result"}
```

```python
from langchain_core.output_parsers import StrOutputParser # ì¶œë ¥ë¬¼ì„ ê¸°ë³¸ str í˜•íƒœë¡œ ë°›ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬

prompt = ChatPromptTemplate.from_template(
    """You are a Korean-speaking assistant specializing in question-answering tasks. 
    Use the provided context informations and relevant documents to answer the following question as accurately as possible. 
    If the answer is not clear from the context or if you do not know the answer, explicitly state "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤." (I don't know). 
    Use three sentences maximum and keep the answer concise.
    All responses must be given in Korean.
    Based on the given information, return a very detailed response.
Question: {question}
Context: {context}
Answer:"""
)

# Chain
rag_chain = prompt | llm | StrOutputParser()

def generate(state):
    """
    Generate answer using RAG on retrieved documents

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation, that contains LLM generation
    """
    print("---GENERATE Answer---")
    question = state["question"]
    documents = state["documents"]
    
    # RAG generation
    generation = rag_chain.invoke({"context": documents, "question": question})
    return {"documents": documents, "question": question, "generation": generation}
```


# Hallucination Grader : Groq

<center><img width="200" src="https://github.com/finddme/finddme.github.io/assets/53667002/384de248-b1f4-4480-9dc4-a457d98cac04"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

```json
{'input': {"documents": documents, "question": query}, 'output':"yes/no"}
```

```python
class GradeHallucinations(BaseModel):
    """Binary score for hallucination present in generation answer."""

    binary_score: str = Field(description="Don't consider calling external APIs for additional information. Answer is supported by the facts, 'yes' or 'no'.")
 
# LLM with function call 
structured_llm_grader_hallucination = llm.with_structured_output(GradeHallucinations)
 
# Prompt 
system = """You are a grader assessing whether an LLM generation is supported by a set of retrieved facts. \n 
     Restrict yourself to give a binary score, either 'yes' or 'no'. If the answer is supported or partially supported by the set of facts, consider it a yes. \n
    Don't consider calling external APIs for additional information as consistent with the facts."""

hallucination_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "Set of facts: \n\n {documents} \n\n LLM generation: {generation}"),
]
)
  
hallucination_grader = hallucination_prompt | structured_llm_grader_hallucination
```

# Answer Grader : Groq

<center><img width="200" src="https://github.com/finddme/finddme.github.io/assets/53667002/9edee9ca-0ed8-4c54-ac94-5dd8bec64e36"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

```json
{'input': {"documents": documents, "question": query}, 'output':"yes/no"}
```

```python
class GradeAnswer(BaseModel):
    """Binary score to assess answer addresses question."""

    binary_score: str = Field(description="Answer addresses the question, 'yes' or 'no'")

# LLM with function call 
structured_llm_grader_answer = llm.with_structured_output(GradeAnswer)

# Prompt 
system = """You are a grader assessing whether an answer addresses / resolves a question \n 
     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question."""
answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "User question: \n\n {question} \n\n LLM generation: {generation}"),
    ]
)

answer_grader = answer_prompt | structured_llm_grader_answer
```

# Hallucination + Answer grader pipeline

```python
def grade_generation_v_documents_and_question(state):
    """
    Determines whether the generation is grounded in the document and answers question

    Args:
        state (dict): The current graph state

    Returns:
        str: Decision for next node to call
    """

    print("---CHECK HALLUCINATIONS---")
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]

    score = hallucination_grader.invoke({"documents": documents, "generation": generation}) # output : GradeHallucinations(binary_score='yes')
    grade = score.binary_score

    # Check hallucination
    if grade == "yes":
        print("---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---")
        # Check question-answering
        print("---GRADE GENERATION vs QUESTION---")
        score = answer_grader.invoke({"question": question,"generation": generation}) # output : GradeAnswer(binary_score='yes')
        grade = score.binary_score
        if grade == "yes":
            print("---DECISION: GENERATION ADDRESSES QUESTION---")
            return "useful"
        else:
            print("---DECISION: GENERATION DOES NOT ADDRESS QUESTION---")
            return "not useful"
    else:
        pprint("---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---")
        return "not supported"

```

# Set RAG Graph : langGraph

```python
from langgraph.graph import END, StateGraph

workflow = StateGraph(GraphState)

# Define the nodes
workflow.add_node("websearch", web_search) # web search # key: action to do
workflow.add_node("retrieve", retrieve) # retrieve
workflow.add_node("grade_documents", grade_documents) # grade documents
workflow.add_node("generate", generate) # generatae

workflow.add_edge("websearch", "generate") #start -> end of node
workflow.add_edge("retrieve", "grade_documents")

# Build graph
workflow.set_conditional_entry_point(
    route_question,
    {
        "websearch": "websearch",
        "vectorstore": "retrieve",
    },
)
 
workflow.add_conditional_edges(
    "grade_documents", # start: node
    decide_to_generate, # defined function
    {
        "websearch": "websearch", #returns of the function
        "generate": "generate",   #returns of the function
    },
)
workflow.add_conditional_edges(
    "generate", # start: node
    grade_generation_v_documents_and_question, # defined function
    {
        "not supported": "generate", #returns of the function
        "useful": END,               #returns of the function
        "not useful": "websearch",   #returns of the function
    },
)

# Compile
app = workflow.compile()
```

# Run Graph

```python
from pprint import pprint
inputs = {"question": "LLaMa3 êµ¬ì¡°ì— ëŒ€í•´ ì•Œë ¤ì¤˜"}
for output in app.stream(inputs):
    for key, value in output.items():
        pprint(f"Finished running: {key}:")
pprint(value["generation"])
```

```
output:
LLaMa 3ì€ Metaì—ì„œ ê°œë°œí•œ Open Source LLM(Large Language Model) ëª¨ë¸ë¡œ, LLaMa 2ë¥¼ ì´ì–´ ë°œí‘œëœ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ Instruct Modelê³¼ Pre-trained Modelë¡œ 8B, 70B ë‘ ì‚¬ì´ì¦ˆê°€ ê³µê°œë˜ì—ˆìœ¼ë©°, ì´ëŠ” 2024ë…„ 4ì›” 18ì¼ ê¸°ì¤€ í•´ë‹¹ íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ ëª¨ë¸ ì¤‘ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. LLaMa 3ëŠ” ì½”ë“œ ìƒì„±, instruction ìˆ˜í–‰ ëŠ¥ë ¥ì´ í¬ê²Œ í–¥ìƒë˜ì–´ ëª¨ë¸ì„ ë³´ë‹¤ ë‹¤ì–‘í•˜ê²Œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ standard decoder-only transformer architectureë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³ , 128K token ìˆ˜ë¥¼ ê°€ì§„ vocabì„ ì‚¬ìš©í•˜ì—¬ ì–¸ì–´ë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ encodingí•©ë‹ˆë‹¤. LLaMa 3ëŠ” 15T ê°œì˜ tokenìœ¼ë¡œ í•™ìŠµë˜ì—ˆìœ¼ë©°, ì´ëŠ” LLaMa 2ë³´ë‹¤ ì•½ 7ë°° ë” í° í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.
```

# Graph visualization

```
ref.
https://langchain-ai.github.io/langgraph/how-tos/visualization/#using-mermaid-pyppeteer

```

```python
from IPython.display import Image, display
from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeColors

print(app.get_graph().draw_mermaid())

display(
    Image(
        app.get_graph().draw_mermaid_png(
            draw_method=MermaidDrawMethod.API,
        )
    )
)
```

# Chainlit

<center><img width="1000" src="https://github.com/user-attachments/assets/85ef7e08-fba3-45b2-b213-7e20796a23f1"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

```
# python scriptë¡œ ì‹¤í–‰
# command:
# chainlit run rag_chainlit.py -w --port 8000 --host 0.0.0.0

from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
import chainlit as cl

@cl.on_message
async def run_convo(message: cl.Message):
    #"what is the weather in sf"
    inputs = {"question": message.content}
    cb = cl.AsyncLangchainCallbackHandler(stream_final_answer=True)
    config = RunnableConfig(callbacks=[cb])
    
    # res = app.invoke(inputs, config=RunnableConfig(callbacks=[
    #     cl.LangchainCallbackHandler(
    #         # to_ignore=["ChannelRead", "RunnableLambda", "ChannelWrite", "__start__", "_execute"]
    #         # can add more into the to_ignore: "agent:edges", "call_model"
    #         # to_keep=

    #     )]))
    res = await app.ainvoke(inputs,config=config)
    # await cl.Message(content=res["question"][-1].content).send()
    await cl.Message(content=res["generation"]).send()
```
