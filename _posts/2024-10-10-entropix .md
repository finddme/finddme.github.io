---
title: "Entropix⚡🔥 (작성 중)"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}










최근 많이 사용되는 LLM들은 문맥에 맞는 일관된 텍스트를 잘 생성하지만 복잡한 추론이 필요한 작업에 대한 hallucination과 shallow reasoning 문제가 있다. 즉 잘못된 정보를 말하거나 복잡한 추론이 요구되는 문제에 대해 얕은 추론만 하는 문제가 있다. 이와 같은 문제를 해결하기 위해 최근에는 entropy 기반 sampling이 많이 연구 되고 있다. entropy 기반 sampling은 decoding 과정에서 entropy를 측정하여 모델이 불확실하게 생성하는 부분을 잘 파악하고 모델이 token을 보다 효과적으로 선택하도록 하는 방법론이다. 

Entropix는 모델의 entropy와 varentropy(variance of entropy)를 통합하여 모델이 불확실성을 더 잘 인식하도록하여 더 정확하고 일관된 출력을 생성하도록 하는 방법론을 제안한다. 이 방법론은 모델의 신뢰도 수준(confidence level)에 따라 샘플링 전략을 조정함으로써 추론 중 연쇄 사고를 시뮬레이션하는 것을 목표로 한다. 간단히 말하자면, 모델이 token을 생성할 때 entropy가 높으면 모델에 불확실하다는 신호를 주어 sample parameter를 조정하여 보다 정확한 추론을 유도한다는 것이다. 이는 Chain-of-Thought (CoT)방식의 추론처럼 진행된다.

> **Chain-of-Thought (CoT)**: 복잡한 질문에 대해 단계별로 논리적인 사고를 하도록 하는 기술

```markdown
**Entropy**

- LLM을 통해 다음 token을 예측할 때 불확실성(uncertainty) 혹은 무작위성(randomness)을 측정하여 다음 token에 대한 확률분포가 얼마나 넒게 퍼져있는지 정량화한 것으로, 모델이 다음 token을 예측할 때 얼마나 불확실한지 측정하는 지표가 된다.
- entropy가 높을수록 모델이 불확실한 상태이다.

**Varentropy**

- entropy의 변동성을 측정한 것. 즉, entropy의 분산이다.
- 이는 모델이 주어진 context에서 모델의 불확실성과 예측의 다양성을 갖는지 나타내며, 모델이 token을 예측하는 과정이 얼마나 안정적인지 보여주는 지표이다.
- varentropy가 낮을수록 불확실성이 고르게 분포하는 것이고, 높을수록 불확실성의 변동 폭이 크다는 것이다.
- 예를 들어 어떤 구간에서는 매우 확신을 갖는데 어떤 구간에서는 매우 불확실하게 token을 예측하면 varentropy가 높게 측정된다.

**Varentropy 계산**
1. 확률과 log probability 계산
  softmax를 사용하여 현재 위치에서 예측 결과로 반환 가능한 token에 대한 각 확률을 계산하고, 그들의 log probability를 산출한다.
2. entropy 계산
  현재 위치에서의 확률 분포에 대한 entropy를 계산한다.
3. varentropy 계산
  각 token에 대해 정보량(negative log probability)와 평균 정보량(entropy)을 제곱하고, 이를 해당 token의 확률에 더한다.
```

# Entropix method

Entropix는 entropy와 varentropy를 기반으로 모델이 다음 token을 예측하는 방법을 조정한다.

- 모델의 entropy와 varentropy가 낮을 때는 예측에 대한 확신이 있다는 의미이기 때문에 일반적인 방식대로 진행한다.
- 모델의 entropy와 varentropy가 높을 때는 예측에 대한 확신이 낮다는 의미이기 때문에 다른 가능성을 더 많이 탐색하거나 다른 추론 방식을 고려한다.

이와 같은 방법은 모델이 확신이 없을 때 더 깊이 생각하는 것처럼 행동하게 만들어 결과적으로 더 정확하고 일관된 출력을 만들어낸다.

아래는 Entropix의 진행 과정을 도식화한 것이다.

<center><img width="300" src="https://github.com/user-attachments/assets/56757316-38d4-4142-ab85-bcda49a30e09"></center>
<center><em style="color:gray;">https://southbridge-research.notion.site/</em></center><br>

# Entropix main keyword

- **Entropy-based decision making**

  logits의 entropy와 varentropy로 모델의 불확실성을 평가하고 이에 따라 sampling 전략을 조정한. 

  > logits은 모델이 vocab 내 각 token에 대해 출력하는 정규화되지 않은 값이다. 이는 sequence에서 다음 token이 될 각 token에 대한 모델의 "신뢰도"를 나타낸다.

- **Attention-aware sampling**

  attention pattern에서 도출된 entropy나 일치율(agreement) 등의 지표들을 활용해 sampling parameter 전략을 세운다.

  > Entropix에서는 attention score를 활용하여 attention entropy와 agreement와 같은 지표를 계산하며, 이는 sampling 전략에 중요한 정보를 제공한다.

- **Dynamic parameter adjustment**

  temperature, top-k, top-p와 같은 sampling parameter가 현재의 contet와 모델 상태에 따라 동적으로 조정된다.

- **Adaptive multi-sample approach**

  중간 정도의 불확실성을 가질 때에는 여러 샘플을 생성하고 이를 평가하여 가장 적합한 토큰을 선택한다.
  
# Entropix : Impact on LLM Inference

- **Improved coherence**

  모델 추론 과정에 불확실성 측정 결과를 반영함으로써 일관성을 유지하는 데 도움을 준다.

- **Enhanced context sensitivity**

  attention-aware sampling을 통해 생성 과정 전반에 걸쳐 context를 더 잘 보존하는 데에 도움을 준다.

- **Reduced hallucinations**

  동적으로 sampling parameter를 조정하여 불확실성이 높은 상황에서 hallucination을 줄이는 데 도움을 준다.

- **Flexible generation**

  현재 context에 따라 다양한 sampling 전략을 유연하게 전환하여 더 세밀하고 적절한 text 생성을 가능하게 한다.

# Entropix's sampling strategies
<html>
  <table border="0" cellpadding="0" cellspacing="0" width="714" style="">
    <thead>
      <tr height="23" style="height:17.4pt">
        <th>　</th>
        <th>　</th>
        <th>varentropy</th>
        <th>　</th>
      </tr>
    </thead>
   <colgroup><col width="70" span="2" style="width:53pt">
   <col width="302" style="mso-width-source:userset;mso-width-alt:9651;width:226pt">
   <col width="272" style="mso-width-source:userset;mso-width-alt:8704;width:204pt">
   </colgroup>
    <tbody>
      <tr height="23" style="height:17.4pt">
        <td height="23" class="xl70" style="height:17.4pt">　</td>
        <td class="xl71">　</td>
        <td class="xl67" style="border-top:none;border-left:none">low</td>
        <td class="xl67" style="border-top:none;border-left:none">high</td>
      </tr>
      <tr height="74" style="mso-height-source:userset;height:55.2pt">
        <td rowspan="2" height="144" class="xl65" style="height:107.4pt;border-top:none">entropy</td>
        <td class="xl67" style="border-top:none;border-left:none">low</td>
        <td class="xl66" width="302" style="border-top:none;border-left:none;width:226pt">확신이
    높은 상태. <br>
      일관성 있는 확신.<br>
      greedy sampling 사용</td>
        <td class="xl66" width="272" style="border-top:none;border-left:none;width:204pt">예측
    token의 다양성이 높은 상태.<br>
      탐색적 sampling을 통해 다양한 가능성을 조사한다.</td>
      </tr>
      <tr height="70" style="height:52.2pt">
        <td height="70" class="xl67" style="height:52.2pt;border-top:none;border-left:
    none">high</td>
        <td class="xl66" width="302" style="border-top:none;border-left:none;width:226pt">일관적으로
    불확실한 상태<br>
      설명 삽입이나 탐색 범위를 늘려야 한다.</td>
        <td class="xl66" width="272" style="border-top:none;border-left:none;width:204pt">높은
    불확실성, 일관성 매우 부족<br>
      parameter를 조정하여 높은 불확실성 sampling을 유도한다.</td>
      </tr>
    </tbody>
  </table>
</html>

<center><img width="300" src="https://github.com/user-attachments/assets/1dc7fd35-57c6-4335-96f6-85c50ece7fa5"></center>
<center><em style="color:gray;">https://southbridge-research.notion.site/</em></center><br>

