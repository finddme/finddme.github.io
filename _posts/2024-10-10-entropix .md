---
title: "Entropix⚡"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}










# Entropix
최근 많이 사용되는 LLM들은 문맥에 맞는 일관된 텍스트를 잘 생성하지만 복잡한 추론이 필요한 작업에 대한 hallucination 및 shallow reasoning 문제가 있다. 즉, 복잡한 추론이 요구되는 문제에 대해 잘못된 정보를 말하거나 얕은 추론만 하는 문제가 있다. 이와 같은 문제를 해결하기 위해 최근에는 entropy 기반 sampling이 많이 연구 되고 있다. entropy 기반 sampling은 decoding 과정에서 entropy를 측정하여 모델이 불확실하게 생성하는 부분을 잘 파악하고 모델이 token을 보다 효과적으로 선택하도록 하는 방법론이다. 

Entropix는 모델의 entropy와 varentropy(variance of entropy)를 통합하여 모델이 불확실성을 더 잘 인식하도록하여 더 정확하고 일관된 출력을 생성하도록 하는 방법론을 제안한다. 이 방법론은 모델의 신뢰도 수준(confidence level)에 따라 샘플링 전략을 조정함으로써 추론 중 연쇄 사고를 시뮬레이션하는 것을 목표로 한다. 간단히 말하자면, 모델이 token을 생성할 때 entropy가 높으면 모델에 불확실하다는 신호를 주어 sample parameter를 조정하여 보다 정확한 추론을 유도한다는 것이다. 이는 Chain-of-Thought (CoT)방식의 추론처럼 진행된다.

> **Chain-of-Thought (CoT)**: 복잡한 질문에 대해 단계별로 논리적인 사고를 하도록 하는 기술

```markdown
**Entropy**

- LLM을 통해 다음 token을 예측할 때 불확실성(uncertainty) 혹은 무작위성(randomness)을 측정하여 다음 token에 대한 확률분포가 얼마나 넒게 퍼져있는지 정량화한 것으로, 모델이 다음 token을 예측할 때 얼마나 불확실한지 측정하는 지표가 된다.
- entropy가 높을수록 모델이 불확실한 상태이다.

**Varentropy**

- entropy의 변동성을 측정한 것. 즉, entropy의 분산이다.
- 이는 모델이 주어진 context에서 모델의 불확실성과 예측의 다양성을 갖는지 나타내며, 모델이 token을 예측하는 과정이 얼마나 안정적인지 보여주는 지표이다.
- varentropy가 낮을수록 불확실성이 고르게 분포하는 것이고, 높을수록 불확실성의 변동 폭이 크다는 것이다.
- 예를 들어 어떤 구간에서는 매우 확신을 갖는데 어떤 구간에서는 매우 불확실하게 token을 예측하면 varentropy가 높게 측정된다.

**Varentropy 계산**
1. 확률과 log probability 계산
  softmax를 사용하여 현재 위치에서 예측 결과로 반환 가능한 token에 대한 각 확률을 계산하고, 그들의 log probability를 산출한다.
2. entropy 계산
  현재 위치에서의 확률 분포에 대한 entropy를 계산한다.
3. varentropy 계산
  각 token에 대해 정보량(negative log probability)와 평균 정보량(entropy)을 제곱하고, 이를 해당 token의 확률에 더한다.
```


## Main Components

1. Language Model: LLM(Entropox에서는 LLaMa 3.1 사용)
2. KV-Cache: 추론 최적화를 위해 key와 value tensor를 저장하는 cache
3. Metric Calculator: entropy, varentropy, attention-based metric을 계산하는 도구들
4. Sampling Strategies: sampling 조정 전략
5. Adaptive Sampler: sampling 전략을 선택하고 적용하는 모듈 

<center><img width="300" src="https://github.com/user-attachments/assets/c4164c1b-d86d-4ee3-8a07-832b234798bd"></center>
<center><em style="color:gray;">https://southbridge-research.notion.site/</em></center><br>

## Method

Entropix는 entropy와 varentropy를 기반으로 모델이 다음 token을 예측하는 방법을 조정한다.

- 모델의 entropy와 varentropy가 낮을 때는 예측에 대한 확신이 있다는 의미이기 때문에 일반적인 방식대로 진행한다.
- 모델의 entropy와 varentropy가 높을 때는 예측에 대한 확신이 낮다는 의미이기 때문에 다른 가능성을 더 많이 탐색하거나 다른 추론 방식을 고려한다.

이와 같은 방법은 모델이 확신이 없을 때 더 깊이 생각하는 것처럼 행동하게 만들어 결과적으로 더 정확하고 일관된 출력을 만들어낸다.


## Data Flow and Decision-Making Process

아래는 Entropix의 진행 과정을 도식화한 것이다.

<center><img width="600" src="https://github.com/user-attachments/assets/69de036a-f54a-4164-b54f-902d38deb0ff"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

### Step 1: Token Generation

모델이 입력 토큰을 처리하여 logit과 attention score를 생성

### Step 2: Metric Calculation

Metric Calculator가 모델의 출력(logit과 attention score)을 기반으로 entropy, varentropy, attention entropy, attention agreement, interaction strength를 계산

### Step 3: Strategy Selection

Adaptive Sampler가 계산된 metric을 분석하여 가장 적합한 sampling strategy를 선택

### Step 4: Parameter Adjustment

선택된 strategy와 metric에 따라 sampling parameter를 동적으로 조정

### Step 5: Token Sampling

선택된 sampling strategy가 적용되어 다음 token 예측

### Step 6: Iteration

현재 time step에서 예측된 token을 포함하여 1단계부터 위 과정을 반복하여 전체 sequence 생성.

# Reference

> [https://github.com/xjdr-alt/entropix](https://github.com/xjdr-alt/entropix)<br>
> [https://github.com/hallucinomeny/hyperobject/blob/main/hyperobject.py](https://github.com/hallucinomeny/hyperobject/blob/main/hyperobject.py)<br>
> [https://southbridge-research.notion.site/Entropixplained-11e5fec70db180b6bfafe878433c2104](https://southbridge-research.notion.site/Entropixplained-11e5fec70db180b6bfafe878433c2104)<br>
