---
title: Multimodal Learning with Transformers:A Survey
category: Multimodal
tag: Multimodal
---







* 목차
{:toc}










# 1. Introduction

인공지능은 인간의 지각 능력을 모방한 것이다. 일반적으로 modality는 특정한 센서를 통해 생성된 vison과 language와 같은 unique communication channel을 지칭한다. 인간은 세상과 상호작용할 때 다양한 modality 정보를 적절히 활용한다. 각 modality는 각각 다른 informationo source로 표현된다. 예를 들어 이미지는 수천개의 pixel을 통해 시각적으로 표현되고, 텍스트는 이산적인 단어들을 통해 표현된다. 인공지능 multimodal 구현을 위해서는 인간이 처리하는 각 modality 표현들과 유사한 정보를 데이터로 사용하고 각 정보들을 연결해야 한다.

본 논문은 다양한 modality와 task를 transformer를 통해 구현하고 학습시키는 것과 관련된 다양한 정보에 대해 다룬다. 

> <strong>[Transformers](https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/)</strong><br>
> 번역과제를 위해 NLP 분야에서 나온 모델로, 최근 다양한 인공지능 모델에 변형되어 활용되는 모델이다.<br>
> Transformers의 구조도 중요하지만 Transformers에서 사용된 Attention도 중요하다. Transformers에 크게 두 가진 Attention이 사용된다<br>
>   <strong>1. Self Attention:</strong> 하나의 sequence 내에서 수행되는 attention <br>
>   <strong>2. Encoder-Decoder Cross Attention:</strong> encoder에서 decoder로 넘어갈 때 사용되는 attention <br>
> Modality에 따라 Transformer의 Tokenization/Embedding 방식이 다르다.<br>
>   <strong>1. Vanilla Transformer(NLP)</strong><br>
>     
>   2. 



# 2. Multimodel Transformers


# Reference

> Multimodal Learning with Transformers:A Survey
