---
title: FastText
category: Natural Language Processing
tag: Word Representation
---

FastText는 2015년 Facebook에서 발표한 라이브러리인데 Word2Vec과 같이 distribution representation을 이용하여 비슷한 의미를 지닌 단어들은 유사한 word representation vector를 갖도록 하는 embedding방법이다. 해당 라이브러리는 Word2Vec을 제안한 T. Mikolov가 Word2Vec의 단점을 보완하여 새롭게 제시한 word representation방법이다. FestText는 Word2Vec과 달리 빈번히 등장하지 않는 단어(infrequent word)에 대해서도 높은 정확도를 자랑하며 모르는 단어(OOV)에 대해서도 계산이 가능하다. 

## sampling train dataset

발표된 라이브러리는 Word2Vec처럼 단어의 유사성을 cosine similarity를 사용하여 구하는 등 대부분의 내용은 Word2Vec과 유사하지만 단어를 word2vec보다 더 여러 부분으로 분리하여 다룬다는 차이점이 있다. 더 자세히 말하자면 Word2Vec은 단어를 분리 가능한 가장 작은 단위로 간주하여 한 단어의 vector값을 통으로 학습하지만 FastText는 분리 가능한 가장 작은 단위를 문자(character)로 간주하여 subword vector들의 합으로 단어를 표현한다. 따라서 FastText는 단어 vector가 아닌 subword vector를 학습하는 것이다. 즉 한 단어를 설정한 window size에 따라 문자 단위로 분리한 후, 그것들의 합으로 단어를 표현하여 분리된 각 vector들을 학습한다. 예를 들어 window size를 3으로 정한다면 하나의 단어를 다음과 같이 tri-gram으로 나타내게 된다. ‘<’, ’>’는 FastText모델에서 단어의 시작과 끝을 나타내는 특수 기호인데 이 기호를 포함하여 설정한 window size에 따라 각 단어 subword들을 분리시킨다.

<center><img width="1000" alt="2019-11-06 (13)" src="https://user-images.githubusercontent.com/53667002/68296579-480d7980-00d8-11ea-870a-8c811a8654df.png"></center>

위와 같이 FastText모델에서 단어 벡터는 예를 들어 “백설공주”라는 단어를 위 와 같이 5개로 분리한 것과 “백설공주” 전체를 embedding한 것의 합으로 표현된다. 이는 다음과 같이 정의될 수 있다:

$${ u }_{ 시나브로 }={ z }_{ <시나 }+{ z }_{ 시나브 }+{ z }_{ 나브로 }+{ z }_{ 브로> }+{ z }_{ <시나브로> }$$

$$u_t=\sum_{ {g} \in { G_t } }{z_g}$$ 

($G_t$는 target word($t$)에 속한 문자 단위 n-gram집합을 가리킨다.)

## Model Training

$$L(\theta)=logP⁡(+|{ c }_{ P },{ o }_{ p })+\sum_{i=1}^klogP⁡(-|{ c }_{ n_i },{ o }_{ n_i })$$ 

위 식은 한 iteration마다 모델 파라미터($\theta$)가 업데이트되는데 이때 한 쌍의 positive sample ($c_P$, $o_p$ : 실제 같이 쓰이는 단어쌍들의 집합)과 $k$쌍의 negative sample($c_{n_i}$, $o_{n_i}$ : 코퍼스 전체에서 랜덤 추출한 함께 사용되지 않은 단어 쌍)이 학습된다는 것을 표현한 log-likelihood function이다. Skip-gram의 log-likelihood function과 같지만 center word($c$)와 context word($o$)를 학습할 때 center word로부터 나온 문자(character)단위의 n-gram vector인 $z$의 등장으로 Skip-gram과 확률이 다르게 정의된다. 

위 식에서 $P(+\|c_P,o_p)$는 FastText Model에서 positive sample인 center word($c$)가 output context word($o$)와 함께 쓰일 확률이다. 이 확률은 다음과 같이 정의될 수 있다:

$$P(+|c_P,o_p)=\frac{ 1 }{ 1+exp⁡({ -u }_{ c }{ v }_{ o }) }=\frac{ 1 }{ 1+exp⁡(-\sum_{{ g } \in { G_t } }{ z }_{ { g }^{ T } }{ v }_{ o } )}$$

해당 모델은 예측된 positive sample인 center word와 output context word쌍이 positive일 확률이 높아야 하기 때문에 위 식의 결과를 최대화하는 방향으로 학습이 진행돼야 한다. 따라서 결국 분모지수를 최소화해야 하는 것이 목표이다. 이는 벡터 $z$과 $v_o$의 내적 값을 높임으로써 가능한데 벡터의 내적 값은 cosine similarity와 비례하기 때문에 내적 값이 커지면 벡터 간의 유사도 또한 높아지는 결과를 얻을 수 있다. 예를 들어 ‘백설공주’가 $c$이고 ‘먹었다’가 $o$일때, $z$는 “<백설”, “백설공”, “설공주”, “공주>”, “<백설공주>”이니 $z$와 $o$에 해당하는 단어벡터 $v_o$간의 유사도를 높이는 것이 해당 모델의 학습 목표라 생각하면 된다. 

$L(\theta)$를 구하는 식에서 $P⁡(-\| c_{n_i},o_{n_i})$는 negative sample인 $c_n$ 와 $o_n$가 negative sample일 확률이다. 따라서 negative sample을 negative sample이라고 얼마나 잘 맞췄는지에 대한 확률이기에 이 값 또한 최대화하는 방향으로 학습이 이루어져야 한다. 이 확률은 다음과 같이 정의될 수 있다:

$$P(-|c_n,o_n)=1- P(+|c_n,o_n)=\frac{ exp⁡(-u_c v_o ) }{ 1+exp⁡(-u_c v_o ) }=\frac{ exp⁡(-\sum_{g\inG_t}{ { z }_{ { g }^{ T }{ v }_{ o })}{ 1+exp⁡(-\sum_{g\inG_t}{ z }_{ { g }^{ T } }{ v }_{ o } )}$$

해당 식을 최대화 하기 위해서는 우변의 분자지수는 키우고 분모지수는 줄여야 한다. 이를 위해서는 $z$와 $v_o$의 내적 값을 줄여야 한다. 위에서 언급했 듯 벡터의 내적은 cosine similarity와 비례하므로 두 벡터의 내적 값이 줄어들면 두 벡터의 유사도 또한 낮아진다. 

이렇게 $L(\theta)$를 최대화하는 학습 과정은 positive sample 단어쌍의 유사도는 높이고 negative sample 단어쌍의 유사도는 낮추는 방향으로 학습이 진행된다. 즉 결국 center word와 context word의 쌍이 positive sample인지 nagative샘플인지 예측하는 것이기 때문에 이 과정을 통해 도출된 word embedding vector는 유사성에 관한 정보를 지니게 되는 것이다.

## Reference

> Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher."CS224n: Natural Language Processing with Deep Learning: Part1,"(winter 2017)

> Lee,Gichang.Sentence embeddings using korean corpora.seoul:acornpub,2019
