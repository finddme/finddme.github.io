---
title: "LLaVA(Large Language and Vision Assistant) : Visual Instruction Tuning"
category: Multimodal
tag: Multimodal
---







* 목차
{:toc}












# 1. Introduction

machine-generated instruction-following data를 사용하여 LLM을 instruction tuning하는 것이 새로운 task에 대한 zero-shot 성능을 향상시킨 다는 점은 이미 여러 연구를 통해 입증되었지만 이를 multimodal task에 적용하는 연구는 2023년 12월 11일 기준 아직 활발히 연구되고 있지 않다. 본 논문은  language-only GPT-4를 통해 multimodal language-image instruction-following data를 생성하고 이를 통해 instruction tuning을 진행하는 방법론을 제안한다. 이와 같이 생성된 모델을 LLaVA(Large Language and Vision Assistant)칭하며, 해당 모델은 vision encoder와 LLM을 end-to-end로 연결한 모델이다. Visual instruction following 연구를 위해 본 논문에서는 두 가지 evaluation benchmark를 만들었다. 해당 benchmark를 통해 GPT-4와 비교 평가한 결과, GPT-4에 준하는 성능을 보이는 것을 확인하였다.

본 논문의 contribution은 아래와 같이 요약할 수 있다:

<strong>- Visual instruction-tuning 방법론 제안</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual instruction-tuning 방법론을 제안함으로써 language-image multimodal 분야에 instruction-tuning을 적용을 통해 general-purpose visual assistant 구축<br>
<strong>- Multimodal instruction-following data 구축</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vision-language instruction-following data의 수량이 대규모 모델을 학습하기에 부족한 것이 본 연구의 큰 문제였다. 본 논문에서는 이 문제를 ChatGPT/GPT-4를 사용하여 image-text pair data를 instruction-following format으로 변환하는 pipeline을 구축함으로써 해결하였다. 해당 데이터셋을 통해 모델이 이미지에 대해 더 정교하게 이해함과 동시에 user instruction을 더 잘 수행할 수 있게 되었다.<br>
<strong>- Large multimodal model 개발</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 본 논문에서는  visual encoder로 CLIP을, language decoder로 Vicuna(LLaMa를 instruction tuning한 모델) 선정한 후 이를 연결하여 Large multimodal model을 개발하였고, 이 모델에 instructional vision-language data을 학습시켰다.<br>
<strong>- Multimodal instruction-following benchmark 생성(LLaVA-Bench)</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 본 논문에서는 두 가지 Multimodal instruction-following benchmark를 생성하여 공개하였다.<br>
<strong>- Open-source</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; multimodal instruction data, codebase, model checkpoints, visual chat demo를 모두 공개한다.<br>

> <strong>Instruction Tuning</strong><br>
> <br>
> Google이 발표한 FLAN을 통해 처음 제안된 방법론이다.
> 다양한 task가 instruction 형태로 들어가 있는 data(질문(prompt)-답변(completion) 쌍으로 구성)를 통해 Pre-trained model을 fine-tuning하는 방법론.<br>
> Instruction Tuning을 통해 사람이 원하는 대답을 생성할 수 있도록 모델의 weight를 업데이트 하는 방법이다.<br>
> Instruction Tuning은 자연어처리 분야에서 이루어진 많은 연구들을 통해 새로운 task에 대한 unseen task에 대한 성능을 향상시키는 것이 입증되었다. 즉, Instruction Tuning이 수행된 모델 prompt로 context나 예시를 입력하지 않아도 적절한 답변을 만들어내는 것이 확인되었다.<br>
> 하지만 많은 task에 대한 data를 수집하는 데에 많은 비용이 들고, 모델이 학습한 instrction data와 인간의 preference 간의 align이 잘 되지 않아 생기는 문제들도 있다. 그리고 hallucination(환각현상 : 거짓 정보 반환) 문제도 있다. hallucination 문제 완화를 위해 RLHF나 RAG 등이 사용되고 있다.<br>
> <br>
>> Lora 등의 기법을 사용하지 않으면 fine-tuning과 마찬가지로 모델 전체 weight가 update됨.<br>
>> 모델 자체가 엄청 크기 때문에 매우 많은 계산 자원과 시간이 필요하기 때문에 Lora와 같은 기법을 사용하거나 양자화를 수행하는 추세.<br>
>>  LoRA는 모델의 특정 파라미터를 저랭크(low-rank)로 분해하여 학습하는 방법으로, 효율적으로 파라미터 수를 줄여 메모리 사용량과 계산 비용을 감소시킴.<br>


# 2. GPT-assisted Visual Instruction Data Generation

우선 instruction tuning을 위해 Text-Only GPT-4나 ChatGPT를 활용하여 visual instrction data를 생성한다. 생성 과정을 간략히 요약하자면 다음과 같다: Visual Content를 인지할 수 없는 GPT-4모델을 위해 Image Caption과 Bounding Box를 이용하여 image에 대한 설명을 제공하여 onversation, Detailed Description, Complex Reasoning 문답 형태로 총 158,000개의 데이터셋을 생성한다.

CC, Laion과 같은 기존의 image-text pair dataset은 image와 caption text로 구성된 data이다. LLaVA 학습에는 multimodal instruction following data가 필요하다. 이 데이터를 human crowd-scouring으로 구축 하려니 시간도 많이 소요되고 less well-defined data(데이터 정의가 잘 되지 않은 데이터)가 구축될 가능성이 있어 연구진들은 ChatGPT/GPT-4를 활용한 데이터 구축 방법을 고안하였다.

연구진들은 기존에 공개된 Image-Text Pair data를 활용하여 image $X_v$와 그에 해당하는 caption $X_c$가 있을 때, ChatGPT/GPT-4에게 이미지 내용을 설명하도록 하는 질물 세트 $X_q$를 생성하라고 지시하여 instruction following data를 구축하였다. 이와 같은 방법은 저비용으로 dataset 구축을 가능하게 하지만 instruction과 response에 대해 다양성과 in-depth reasoning이 부족하다는 단점이 있다. 생성된 데이터는 아래와 같은 형태가 된다.

<center><img width="400" src="https://github.com/finddme/finddme.github.io/assets/53667002/bd6df9b4-4f8c-4bff-8009-122b41353b52"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

위 방법으로 Instruction-Following Dataset을 구축할 때 두 가지 문제가 있었다. 아래는 문제와 문제 완화를 위해 적용한 방법이다:

<strong>1. language-only GPT-4/ChatGPT를 사용했기 때문에 모델이 Visual Content를 인지할 수 없는 문제</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 이 문제 해결을 위한 대전제는 image를 LLM이 인식할 수 있는 시퀀스로 encoding 가능하게 하는 것이다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(i) Caption</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Caption을 GPT-4/ChatGPT에 함께 입력함으로써 language-only 모델이 visual scene을 알 수 있도록 한다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(ii) Bounding box</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; visual scene에서 특정 객체의 위치 정보를 좌표로 prompt에 넣어주어 language-only 모델이 객체를 인식할 수 있도록 한다.

<strong>2. Instruction-following data 유형 정의 및 유형별 포함 내용 정의 문제</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conversation, Detailed description, Complex reasoning 유형을 구축하여 세부 사항을 정의하여 총 158,000개의 data를 구축하였다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(i) Conversation (58,000개 생성)</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 주어진 이미지에 대해 Assistant와 Human이 대화하는 형태로 구성.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Assistant가 image를 보고 Human의 질문에 답하는 것과 같은 형식.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - image 내의 직관적인 시각적 정보에 대한 QAset으로 생성. (객체 유형, 객체 수, 객체 동작, 객체 위치, 객체 간의 상대적 위치 등)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(ii) Detailed description (23,000개 생성)</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - image의 caption과 box 정보를 제공한 후 이에 대한 상세 설명을 생성하라는 prompt를 입력하여 GPT-4가 image에 대한 상세 설명을 생성하도록 함.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - prompt는 다양하게 준비해 놓고 랜덤하게 입력<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(iii) Complex reasoning (77,000개 생성)</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 위 두 유형을 통해 시각적 내용 자체에 중점을 둔 data를 생성하한 후 이를 바탕으로 심층적 추론 질의응답셋을 GPT-4를 통해 생성한다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 답변에 단계별로 엄격한 논리를 포함하도록 prompt를 생성하고 이 prompt와 caption 그리고 box 정보를 모델에 함께 임력한다.<br>

위 내용을 요약 정리하면 아래와 같다.

1. 이런 데이터에서 caption과 box 정보를 바로 아래 이미지에 나온 pormpt의 \["context"\] 에 입력하여 language only GPT-4에 넣는. 이미지는 입력 x
<center><img width="400" src="https://github.com/finddme/finddme.github.io/assets/53667002/e092e267-f1e7-4732-ba77-ded4f8dcf746"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

<center><img width="400" src="https://github.com/finddme/finddme.github.io/assets/53667002/e0bfd98b-52c8-403e-8467-7614d3b27d26"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

2. 각 유형별로 아래와 같이 데이터가 생성된다.
<center><img width="400" src="https://github.com/finddme/finddme.github.io/assets/53667002/9abd95cc-5dcb-440a-a6e9-52f3c1e98c0c"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>


# 3. Visual Instruction Tuning

## 3.1 Architecture

LLaVA의 visual encoder와 LLM($f_ϕ$)을 연결한 구조를 가진다. 현재는visual encoder와 LLM에 대해 매우 여러 조합의 버전으로 LLaVA가 나왔지만 해당 논문에 작성된 LLaVA는 visual encoder로 CLIP Vision Encoder인 VIT-Large 14와 LLM으로는 LLama를 instruction tuning한 Vicuna를 용하였다. visual encoder와 LLM은 단순한 linear layer로 연결한다. 즉, LLM의 Word Embedding Sapce와 Visual Encoder의 Fueature Space를 Linear 결합한다.

<center><img width="700" src="https://github.com/finddme/finddme.github.io/assets/53667002/9b5d042e-b546-47c4-8d83-993318935037"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

 
## 3.2 Training

각 image $X_v$에 대해 turn 수가 $T$개인 multi-turn conversation data $(X_1^q, X_1^a, \cdots, X_T^q, X_T^a)$를 생성한다. 이때 답변은 dataset 상에서 Assistant에 해당하는 것으로 하고, $t$번째 turn의 instrction $X_t^{instruct}$은 아래와 같이 정의한다. 첫 번째 turn에만 image $X_v$를 입력하고 나머지 turn들은 text이기 때문에 첫 turn에 입력되는 $X_v$와 $X_q$의 순서는 크게 상관 없다.

<center><img width="600" src="https://github.com/finddme/finddme.github.io/assets/53667002/2c1692a6-8804-4654-84ff-b0802d90bf15"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

multimodal instruction-following sequence는 아래 이미지와 같은 형식으로 표현된다.

<center><img width="700" src="https://github.com/finddme/finddme.github.io/assets/53667002/d74d0f0e-9bbc-4f9f-a542-d32dd5de475d"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

LLM에 위 데이터를 넣어 original auto-regressive training task를 통해 token을 예측하며 학습을 수행한다. 위 예시는 두 turn만 나타내지만 실제 turn 수는 더 다양하다. 실제 구현은 사용된 모델에 따라 달라지겠지만 LLM으로 Vicuna를 사용할 때는 <STOP>을 ### 구분자로 설정했다. 모델은 assistant의 답변과 답변 중단 지점을 예측하도록 학습되기 때문에 위 이미지에서 초록색으로 표시된 sequence/tokens만 loss 계산에 사용된다.

sequence의 길이가 $L$일 때 정답 $X_a$에 대한 확률은 아래와 같이 계산된다. 이때 $X_{instruct<i}$는 현재 예측 token인 초록색 글씨 $x_i$ 이전의 모든 turn에 대한 instruction token을 의미하고, $X_{a<i}$는 현재 예측 토큰인 $x_i$ 이전의 모든 turn에 대한 answer token을 의미한다. 아래 수식은 image와 query에 대해 answer가 나올 확률을 계산할 때 image, instruction, answer에 대한 이전의 모든 token을 집어넣고 이를 모두 곱하여 확률값을 계산한다. 

<center><img width="500" src="https://github.com/finddme/finddme.github.io/assets/53667002/088baa57-451f-4781-885c-171f032f0e03"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>


LLaVA는 두 단계의 instruction-tuning 절를 거친다.

- **Stage 1. Pre-training for Feature Alignment:** CC3M의 subset을 기반으로 projection layer matrix만 업데이트한다.
- **Stage 2. Fine-tuning End-to-End:** 투영 행렬과 LLM 모두 두 가지 다른 사용 시나리오에 맞게 업데이트됩니다:
  - **Multimodal Chatbot(Visual Chat)**: LLaVA는 일상적인 사용자 지향 application(downstream) 수행을 위해 multimodal instruction-following data로 fine-tuning을 진행한다.
  - **Science QA**: LLaVA는 science domain을 위해 multimodal reasonsing dataset으로 로 fine-tuning을 진행한다.

### 3.2.1 Stage 1: Pre-training for Feature Alignment

Pre-Training 과정에서는 image Caption을 활용하여 Visual Tokenizer를 LLM에 호환 가능하도록 학습시키는 것이 주요 목적이다.

1) CC3M dataset을 595,000개의 image-text 쌍으로 filtering. 이때 instrction과 image, question을 입력으로 받고 원 데이터셋에서 caption에 해당하는 것을 예측하며 학습한다.
  
2) 학습 시에는 visual encoder와 LLM의 weight는 frozen시키고 trainable parameter인 $\theta = W$(projection matrix)에 대해서만 likelihood를 maximize한다. 이와 같은 과정을 통해 학습으 하면 image feature $H_v$가 pre-trained LLM word embedding에 align될 수 있다. 해당 단계는  frozen LLM에 대해 visual tokenizer를 학습시키는 것으로 볼 수 있다.


### 3.2.2 Stage 2: Fine-tuning End-to-End

Fine-tuing 과정에서는 LLM과 projection layer를 미세 조정하는 것이 주요 목적이다.

Fine-tuning 시, visual encoder weight는 frozen 시키고 projection layer와 LLM의 weight만 업데이트한다. 이 때 두 가지 scenario를 위한 fine-tuning을 수행한다.

**1) Multimodal Chatbot(Visual Chat)**<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 앞서 구축한 158K(15만 8천개) language-image instruction-following data로 fine-tuning

**2) Science QA**<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 과학 관련 질의응답 데이터로, 이미지 혹은 자연어로 질문이 들어가면 자연어로 답변을 반환하는 데이터이다. 해당 데이터의 question & context을 instruction으로 LLaVA에 입력하고 reasoning & answer를 output으로 설정하여 학습한다.


# 4. Experiments

Multimodal Chatbot과 ScienceQA dataset를 통해 추론 능력을 평가한다. 

```
hardware specification: A100 GPU * 8
hyperparameter 설정: Vicuna와 동일

[Pretrained Model]
pretraining dataset: filtered CC-595K subset
epoch: 1
learning rate: 2e-3


[Fine-tuning]
dataset: LLaVA-Instruct-158K dataset
epoch: 3
learning rate: 2e-5
batch size: 32
```

## 4.1 Multimodal Chatbot

### 4.1.2 Qualitative Evaluation (정성평가)

Multimodal Chatbot 추론 능력 평가를 위해 chatbot demo를 만들고 아래 표와 같이  in-depth image understanding을 요구하는 예제들을(GPT-4 논문에 나온 예제들) LLaVA, GPT-4, BLIP-2, OpenFlamingo에 입력하여 비교했다. BLIP-2와 OpenFlamingo는 ser’s instructions에 따르기보다는 장면에 대해 간단하게 설명하는 반면 LLaVA는 user’s instructions에 따라 정확하게 답변하는 것으로 확인되었다. 그리고 GPT-4와 비교했을 때, 유사한 추론 결과를 보이면서 보다 포괄적인 답변을 반환하는 것으로 확인된다. LLaVA는 비교적 소규모인 80K unique image를 학습한 모델임에도 학습 이미지와 다른 도메인에 속하는 이미에 대해서도 잘 이해하고 사용자의 요구에 맞게 잘 답변하는 것으로 확인되었다.

<center><img width="600" src="https://github.com/finddme/finddme.github.io/assets/53667002/a5719363-c5b2-4b0c-9d91-58b1f32cce17"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>


### 4.1.2 Quantitative Evaluation (정량평가)

1) GPT-4를 통해 생성된 결과를 참고 답안으로 삼고
2) LLaVA에도 동일한 질문에 대한 결과를 만든 후
3) 두 모델을 통해 생성된 결과를 GPT-4에 다시 넣어서 결과에 대한 helpfulness, relevance, accuracy 그리고 level of detail을 1에서 10 사이의 점수로 평가하게 한다. 그리고 평가 결과에 대한 설명도 제공하도록 한다.

본 논문에서는 위와 같은 과정에 사용될 두 개의 benchmark를 만들었다. 

**LLaVA-Bench (COCO)**

평가 데이터 생성 방법:

1. COCO-Val-2014에서 30개의 이미지를 랜덤하게 선택
2. 각 이미지에 대해 "2. GPT-assisted Visual Instruction Data Generation"에서 제안된 방식으로 세 가지 유형의 질문 (conversation, detailed description, complex reasoning)을 생성해서 총 90개의 질문을 만든다.

위 데이터로 Ablation study를 진행한 결과는 아래와 같다:

<center><img width="600" src="https://github.com/finddme/finddme.github.io/assets/53667002/80e85125-1d66-4dfd-8755-c95d52fbbc85"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

- instruction tuning을 하지 않은 모델보다 tuning을 한 모델이 50% 이상 성능이 좋다.
- Conv + 5% Detail + 10% Complex를 학습한 모델이 conversation만 학습한 모델보다 7% 정도 성능이 좋다.
- Full data(세 유형의 데이터 전체)를 학습한 모델이 가장 높은 성능을 보인다.

**LLaVA-Bench (In-the-Wild)**

- 실내, 실외, 밈, 그림, 스케치 등을 포함한 24개의 이미지와 60개의 질문을 수집한다
- 각 이미지에는 highly-detailed, manually-curated description이 있다.

<center><img width="600" src="https://github.com/finddme/finddme.github.io/assets/53667002/86d8ea07-3aed-4a16-880a-a27d496070aa"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

LLaVA, BLIP 그리고 OpenFlamingo에 대해 비교 실험을 진행한 결과, BLIP-2보다는 29%이상, OpenFlamingo보다는 48%이상 좋은 성능을 보였다.

<center><img width="600" src="https://github.com/finddme/finddme.github.io/assets/53667002/eb22f8a4-c333-4b23-80c6-00177b6777eb"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>


## 4.2 ScienceQA

ScienceQA는 3 subjects, 26 topics, 127 categories, and 379 skills에 걸쳐 다양한 도메인을 포함한 21k(21,000개)의 multimodal multiple choice question을 포함한다. 해당 데이터셋은 trainset 12726개, validation set 4241개, 그리고 teset set 4241개로 나뉜다. 

GPT-3.5 model (text-davinci-002) with and without chainof-thought (CoT),  LLaMA-Adapter, multimodal chain-of-thought (MM-CoT)로 실험한 결과는 아래 표와 같다.

<center><img width="600" src="https://github.com/finddme/finddme.github.io/assets/53667002/bdcc218f-06f0-4b2c-9cc0-ae39b883ded3"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

LLaVa Model만 이용한 결과, SOTA(당시 91.68%)에 근접한 성능(90.92%)을 내는 것으로 확인되었다. 

LLaVa와 GPT-4를 함께 사용한 실험도 했다. 

(i) GPT-4 complement: GPT-4의 답이 틀렸을 때 LLaVA 담면 사용. 이때 90.97%의 성능 기록

(ii) GPT-4 as the judge: LLaVA와 GPT-4의 답변이 다른 경우, GPT-4에게 두 답변을 넣고 최종 결론을 요구하였고, 그 결과 SOTA(당시 92.53%)를 달성하였다.


### 4.2.1 Ablations

ScienceQA에서 일부 선택사항을 제거하는 Ablation study도 진행했다. 

- Vision Encoder에서 Last Layer Feature를 사용하는 경우, Before the laset Layser Feature를 사용하는 경우보다 0.96%낮은 성능을 보였다고 한다.
    이에 대한 이유로 추측되는 것은 마지막 Layer는 그 직전 Layer보다 일반적이고 추상적인 이미지 속성을 담고 있기 때문으로 추정된다. 
- Answer를 먼저 예측하는 경우. 12Epoch에서 89.77%에 도달하였지만, Reasoning을 먼저 예측하는 경우에는 단 6 Epoch만으로도 89.77%에 도달하였다. 최종으로는 89.96%.
    그래서 저자들은 Reasoning을 먼저 하는 것이 더 좋지 않을까 했지만 CoT 전략이 학습 속도 개선에는 좋지만 최종 성능에는 큰 영향을 미치지 않았다고 한다.
- Pre-train을 거치지 않은 경우, Pre training을 했을 때보다 5.11% 성능이 낮았다. 이를 통해 Pre training의 중요성이 강조되었다. 
- 기존 13B 모델을 7B fh skwcns rufrhk 1.08% 성능이 감소하였다. 이에 따라 모델 크기에 따른 성능 차이가 존재함이 증명되었다.

            
# Reference

> Visual Instruction Tuning

> https://llava-vl.github.io/
