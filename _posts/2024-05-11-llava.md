---
title: "LLaVA(Large Language and Vision Assistant) : Visual Instruction Tuning"
category: Multimodal
tag: Multimodal
---







* 목차
{:toc}












# 1. Introduction

machine-generated instruction-following data를 사용하여 LLM을 instruction tuning하는 것이 새로운 task에 대한 zero-shot 성능을 향상시킨 다는 점은 이미 여러 연구를 통해 입증되었지만 이를 multimodal task에 적용하는 연구는 2023년 12월 11일 기준 아직 활발히 연구되고 있지 않다. 본 논문은  language-only GPT-4를 통해 multimodal language-image instruction-following data를 생성하고 이를 통해 instruction tuning을 진행하는 방법론을 제안한다. 이와 같이 생성된 모델을 LLaVA(Large Language and Vision Assistant)칭하며, 해당 모델은 vision encoder와 LLM을 end-to-end로 연결한 모델이다. Visual instruction following 연구를 위해 본 논문에서는 두 가지 evaluation benchmark를 만들었다. 해당 benchmark를 통해 GPT-4와 비교 평가한 결과, GPT-4에 준하는 성능을 보이는 것을 확인하였다.










# Reference

> Visual Instruction Tuning

> https://llava-vl.github.io/
