---
title: "LLaVA(Large Language and Vision Assistant) : Visual Instruction Tuning"
category: Multimodal
tag: Multimodal
---







* 목차
{:toc}












# 1. Introduction

machine-generated instruction-following data를 사용하여 LLM을 instruction tuning하는 것이 새로운 task에 대한 zero-shot 성능을 향상시킨 다는 점은 이미 여러 연구를 통해 입증되었지만 이를 multimodal task에 적용하는 연구는 2023년 12월 11일 기준 아직 활발히 연구되고 있지 않다. 본 논문은  language-only GPT-4를 통해 multimodal language-image instruction-following data를 생성하고 이를 통해 instruction tuning을 진행하는 방법론을 제안한다. 이와 같이 생성된 모델을 LLaVA(Large Language and Vision Assistant)칭하며, 해당 모델은 vision encoder와 LLM을 end-to-end로 연결한 모델이다. Visual instruction following 연구를 위해 본 논문에서는 두 가지 evaluation benchmark를 만들었다. 해당 benchmark를 통해 GPT-4와 비교 평가한 결과, GPT-4에 준하는 성능을 보이는 것을 확인하였다.

본 논문의 contribution은 아래와 같이 요약할 수 있다:

<strong>- Visual instruction-tuning 방법론 제안</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual instruction-tuning 방법론을 제안함으로써 language-image multimodal 분야에 instruction-tuning을 적용을 통해 general-purpose visual assistant 구축<br>
<strong>- Multimodal instruction-following data 구축</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vision-language instruction-following data의 수량이 대규모 모델을 학습하기에 부족한 것이 본 연구의 큰 문제였다. 본 논문에서는 이 문제를 ChatGPT/GPT-4를 사용하여 image-text pair data를 instruction-following format으로 변환하는 pipeline을 구축함으로써 해결하였다.<br>
<strong>- Large multimodal model 개발</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 본 논문에서는  visual encoder로 CLIP을, language decoder로 Vicuna(LLaMa를 instruction tuning한 모델) 선정한 후 이를 연결하여 Large multimodal model을 개발하였고, 이 모델에 instructional vision-language data을 학습시켰다.<br>
<strong>- Multimodal instruction-following benchmark 생성(LLaVA-Bench)</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 본 논문에서는 두 가지 Multimodal instruction-following benchmark를 생성하여 공개하였다.<br>
<strong>- Open-source</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; multimodal instruction data, codebase, model checkpoints, visual chat demo를 모두 공개한다.<br>

> <strong>Instruction Tuning</strong><br>
> <br>
> Google이 발표한 FLAN을 통해 처음 제안된 방법론이다.
> 다양한 task가 instruction 형태로 들어가 있는 data(질문(prompt)-답변(completion) 쌍으로 구성)를 통해 Pre-trained model을 fine-tuning하는 방법론.<br>
> Instruction Tuning을 통해 사람이 원하는 대답을 생성할 수 있도록 모델의 weight를 업데이트 하는 방법이다.<br>
> Instruction Tuning은 자연어처리 분야에서 이루어진 많은 연구들을 통해 새로운 task에 대한 unseen task에 대한 성능을 향상시키는 것이 입증되었다. 즉, Instruction Tuning이 수행된 모델 prompt로 context나 예시를 입력하지 않아도 적절한 답변을 만들어내는 것이 확인되었다.<br>
> 하지만 많은 task에 대한 data를 수집하는 데에 많은 비용이 들고, 모델이 학습한 instrction data와 인간의 preference 간의 align이 잘 되지 않아 생기는 문제들도 있다. 그리고 hallucination(환각현상 : 거짓 정보 반환) 문제도 있다. hallucination 문제 완화를 위해 RLHF나 RAG 등이 사용되고 있다.<br>
> <br>
>> Lora 등의 기법을 사용하지 않으면 fine-tuning과 마찬가지로 모델 전체 weight가 update됨.<br>
>> 모델 자체가 엄청 크기 때문에 매우 많은 계산 자원과 시간이 필요하기 때문에 Lora와 같은 기법을 사용하거나 양자화를 수행하는 추세.<br>
>>  LoRA는 모델의 특정 파라미터를 저랭크(low-rank)로 분해하여 학습하는 방법으로, 효율적으로 파라미터 수를 줄여 메모리 사용량과 계산 비용을 감소시킴.<br>



# Reference

> Visual Instruction Tuning

> https://llava-vl.github.io/
