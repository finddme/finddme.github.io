---
title: "LLaVA(Large Language and Vision Assistant) : Visual Instruction Tuning"
category: Multimodal
tag: Multimodal
---







* 목차
{:toc}












# 1. Introduction

machine-generated instruction-following data를 사용하여 LLM을 instruction tuning하는 것이 새로운 task에 대한 zero-shot 성능을 향상시킨 다는 점은 이미 여러 연구를 통해 입증되었지만 이를 multimodal task에 적용하는 연구는 2023년 12월 11일 기준 아직 활발히 연구되고 있지 않다. 본 논문은  language-only GPT-4를 통해 multimodal language-image instruction-following data를 생성하고 이를 통해 instruction tuning을 진행하는 방법론을 제안한다. 이와 같이 생성된 모델을 LLaVA(Large Language and Vision Assistant)칭하며, 해당 모델은 vision encoder와 LLM을 end-to-end로 연결한 모델이다. Visual instruction following 연구를 위해 본 논문에서는 두 가지 evaluation benchmark를 만들었다. 해당 benchmark를 통해 GPT-4와 비교 평가한 결과, GPT-4에 준하는 성능을 보이는 것을 확인하였다.

본 논문의 contribution은 아래와 같이 요약할 수 있다:

<strong>- Visual instruction-tuning 방법론 제안</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual instruction-tuning 방법론을 제안함으로써 language-image multimodal 분야에 instruction-tuning을 적용을 통해 general-purpose visual assistant 구축<br>
<strong>- Multimodal instruction-following data 구축</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vision-language instruction-following data의 수량이 대규모 모델을 학습하기에 부족한 것이 본 연구의 큰 문제였다. 본 논문에서는 이 문제를 ChatGPT/GPT-4를 사용하여 image-text pair data를 instruction-following format으로 변환하는 pipeline을 구축함으로써 해결하였다.<br>
<strong>- Large multimodal model 개발</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 본 논문에서는  visual encoder로 CLIP을, language decoder로 Vicuna(LLaMa를 instruction tuning한 모델) 선정한 후 이를 연결하여 Large multimodal model을 개발하였고, 이 모델에 instructional vision-language data을 학습시켰다.<br>
<strong>- Multimodal instruction-following benchmark 생성(LLaVA-Bench)</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 본 논문에서는 두 가지 Multimodal instruction-following benchmark를 생성하여 공개하였다.<br>
<strong>- Open-source</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; multimodal instruction data, codebase, model checkpoints, visual chat demo를 모두 공개한다.<br>

> <strong>Instruction Tuning</strong><br>
> <br>
> Google이 발표한 FLAN을 통해 처음 제안된 방법론이다.
> 다양한 task가 instruction 형태로 들어가 있는 data(질문(prompt)-답변(completion) 쌍으로 구성)를 통해 Pre-trained model을 fine-tuning하는 방법론.<br>
> Instruction Tuning을 통해 사람이 원하는 대답을 생성할 수 있도록 모델의 weight를 업데이트 하는 방법이다.<br>
> Instruction Tuning은 자연어처리 분야에서 이루어진 많은 연구들을 통해 새로운 task에 대한 unseen task에 대한 성능을 향상시키는 것이 입증되었다. 즉, Instruction Tuning이 수행된 모델 prompt로 context나 예시를 입력하지 않아도 적절한 답변을 만들어내는 것이 확인되었다.<br>
> 하지만 많은 task에 대한 data를 수집하는 데에 많은 비용이 들고, 모델이 학습한 instrction data와 인간의 preference 간의 align이 잘 되지 않아 생기는 문제들도 있다. 그리고 hallucination(환각현상 : 거짓 정보 반환) 문제도 있다. hallucination 문제 완화를 위해 RLHF나 RAG 등이 사용되고 있다.<br>
> <br>
>> Lora 등의 기법을 사용하지 않으면 fine-tuning과 마찬가지로 모델 전체 weight가 update됨.<br>
>> 모델 자체가 엄청 크기 때문에 매우 많은 계산 자원과 시간이 필요하기 때문에 Lora와 같은 기법을 사용하거나 양자화를 수행하는 추세.<br>
>>  LoRA는 모델의 특정 파라미터를 저랭크(low-rank)로 분해하여 학습하는 방법으로, 효율적으로 파라미터 수를 줄여 메모리 사용량과 계산 비용을 감소시킴.<br>


# 2. GPT-assisted Visual Instruction Data Generation

CC, Laion과 같은 기존의 image-text pair dataset은 image와 caption text로 구성된 data이다. LLaVA 학습에는 multimodal instruction following data가 필요하다. 이 데이터를 human crowd-scouring으로 구축 하려니 시간도 많이 소요되고 less well-defined data(데이터 정의가 잘 되지 않은 데이터)가 구축될 가능성이 있어 연구진들은 ChatGPT/GPT-4를 활용한 데이터 구축 방법을 고안하였다.

연구진들은 기존에 공개된 Image-Text Pair data를 활용하여 image $X_v$와 그에 해당하는 caption $X_c$가 있을 때, ChatGPT/GPT-4에게 이미지 내용을 설명하도록 하는 질물 세트 $X_q$를 생성하라고 지시하여 instruction following data를 구축하였다. 이와 같은 방법은 저비용으로 dataset 구축을 가능하게 하지만 instruction과 response에 대해 다양성과 in-depth reasoning이 부족하다는 단점이 있다. 생성된 데이터는 아래와 같은 형태가 된다.

<center><img width="400" src="https://github.com/finddme/finddme.github.io/assets/53667002/bd6df9b4-4f8c-4bff-8009-122b41353b52"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

위 방법으로 Instruction-Following Dataset을 구축할 때 두 가지 문제가 있었다. 아래는 문제와 문제 완화를 위해 적용한 방법이다:

<strong>1. language-only GPT-4/ChatGPT를 사용했기 때문에 모델이 Visual Content를 인지할 수 없는 문제</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 이 문제 해결을 위한 대전제는 image를 LLM이 인식할 수 있는 시퀀스로 encoding 가능하게 하는 것이다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(i) Caption</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Caption을 GPT-4/ChatGPT에 함께 입력함으로써 language-only 모델이 visual scene을 알 수 있도록 한다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(ii) Bounding box</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; visual scene에서 특정 객체의 위치 정보를 좌표로 prompt에 넣어주어 language-only 모델이 객체를 인식할 수 있도록 한다.

<strong>2. Instruction-following data 유형 정의 및 유형별 포함 내용 정의 문제</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conversation, Detailed description, Complex reasoning 유형을 구축하여 세부 사항을 정의하여 총 158,000개의 data를 구축하였다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(i) Conversation (58,000개 생성)</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 주어진 이미지에 대해 Assistant와 Human이 대화하는 형태로 구성.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Assistant가 image를 보고 Human의 질문에 답하는 것과 같은 형식.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - image 내의 직관적인 시각적 정보에 대한 QAset으로 생성. (객체 유형, 객체 수, 객체 동작, 객체 위치, 객체 간의 상대적 위치 등)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(ii) Detailed description (23,000개 생성)</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - image의 caption과 box 정보를 제공한 후 이에 대한 상세 설명을 생성하라는 prompt를 입력하여 GPT-4가 image에 대한 상세 설명을 생성하도록 함.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - prompt는 다양하게 준비해 놓고 랜덤하게 입력<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(iii) Complex reasoning (77,000개 생성)</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 위 두 유형을 통해 시각적 내용 자체에 중점을 둔 data를 생성하한 후 이를 바탕으로 심층적 추론 질의응답셋을 GPT-4를 통해 생성한다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 답변에 단계별로 엄격한 논리를 포함하도록 prompt를 생성하고 이 prompt와 caption 그리고 box 정보를 모델에 함께 임력한다.<br>

# 3. Visual Instruction Tuning


# Reference

> Visual Instruction Tuning

> https://llava-vl.github.io/
