---
title: "LLaVA(Large Language and Vision Assistant) : Visual Instruction Tuning"
category: Multimodal
tag: Multimodal
---







* 목차
{:toc}












# 1. Introduction

machine-generated instruction-following data를 사용하여 LLM을 instruction tuning하는 것이 새로운 task에 대한 zero-shot 성능을 향상시킨 다는 점은 이미 여러 연구를 통해 입증되었지만 이를 multimodal task에 적용하는 연구는 2023년 12월 11일 기준 아직 활발히 연구되고 있지 않다. 본 논문은  language-only GPT-4를 통해 multimodal language-image instruction-following data를 생성하고 이를 통해 instruction tuning을 진행하는 방법론을 제안한다. 이와 같이 생성된 모델을 LLaVA(Large Language and Vision Assistant)칭하며, 해당 모델은 vision encoder와 LLM을 end-to-end로 연결한 모델이다. Visual instruction following 연구를 위해 본 논문에서는 두 가지 evaluation benchmark를 만들었다. 해당 benchmark를 통해 GPT-4와 비교 평가한 결과, GPT-4에 준하는 성능을 보이는 것을 확인하였다.

본 논문의 contribution은 아래와 같이 요약할 수 있다:

<strong>- Visual instruction-tuning 방법론 제안</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual instruction-tuning 방법론을 제안함으로써 language-image multimodal 분야에 instruction-tuning을 적용을 통해 general-purpose visual assistant 구축<br>
<strong>- Multimodal instruction-following data 구축</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vision-language instruction-following data의 수량이 대규모 모델을 학습하기에 부족한 것이 본 연구의 큰 문제였다. 본 논문에서는 이 문제를 ChatGPT/GPT-4를 사용하여 image-text pair data를 instruction-following format으로 변환하는 pipeline을 구축함으로써 해결하였다.<br>
<strong>- Large multimodal model 개발</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 본 논문에서는  visual encoder로 CLIP을, language decoder로 Vicuna(LLaMa를 instruction tuning한 모델) 선정한 후 이를 연결하여 Large multimodal model을 개발하였고, 이 모델에 instructional vision-language data을 학습시켰다.<br>
<strong>- Multimodal instruction-following benchmark 생성(LLaVA-Bench)</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 본 논문에서는 두 가지 Multimodal instruction-following benchmark를 생성하여 공개하였다.<br>
<strong>- Open-source</strong><br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; multimodal instruction data, codebase, model checkpoints, visual chat demo를 모두 공개한다.<br>

> <strong>Instruction Tuning</strong><br>
> <br>
> Google이 발표한 FLAN을 통해 처음 제안된 방법론이다.
> 다양한 task가 instruction 형태로 들어가 있는 data(질문(prompt)-답변(completion) 쌍으로 구성)를 통해 Pre-trained model을 fine-tuning하는 방법론.<br>
> Instruction Tuning을 통해 사람이 원하는 대답을 생성할 수 있도록 모델의 weight를 업데이트 하는 방법이다.<br>
> Instruction Tuning은 자연어처리 분야에서 이루어진 많은 연구들을 통해 새로운 task에 대한 unseen task에 대한 성능을 향상시키는 것이 입증되었다. 즉, Instruction Tuning이 수행된 모델 prompt로 context나 예시를 입력하지 않아도 적절한 답변을 만들어내는 것이 확인되었다.<br>
> 하지만 많은 task에 대한 data를 수집하는 데에 많은 비용이 들고, 모델이 학습한 instrction data와 인간의 preference 간의 align이 잘 되지 않아 생기는 문제들도 있다. 그리고 hallucination(환각현상 : 거짓 정보 반환) 문제도 있다. hallucination 문제 완화를 위해 RLHF나 RAG 등이 사용되고 있다.<br>
> <br>
>> Lora 등의 기법을 사용하지 않으면 fine-tuning과 마찬가지로 모델 전체 weight가 update됨.<br>
>> 모델 자체가 엄청 크기 때문에 매우 많은 계산 자원과 시간이 필요하기 때문에 Lora와 같은 기법을 사용하거나 양자화를 수행하는 추세.<br>
>>  LoRA는 모델의 특정 파라미터를 저랭크(low-rank)로 분해하여 학습하는 방법으로, 효율적으로 파라미터 수를 줄여 메모리 사용량과 계산 비용을 감소시킴.<br>


# 2. GPT-assisted Visual Instruction Data Generation

우선 instruction tuning을 위해 Text-Only GPT-4나 ChatGPT를 활용하여 visual instrction data를 생성한다. 생성 과정을 간략히 요약하자면 다음과 같다: Visual Content를 인지할 수 없는 GPT-4모델을 위해 Image Caption과 Bounding Box를 이용하여 image에 대한 설명을 제공하여 onversation, Detailed Description, Complex Reasoning 문답 형태로 총 158,000개의 데이터셋을 생성한다.

CC, Laion과 같은 기존의 image-text pair dataset은 image와 caption text로 구성된 data이다. LLaVA 학습에는 multimodal instruction following data가 필요하다. 이 데이터를 human crowd-scouring으로 구축 하려니 시간도 많이 소요되고 less well-defined data(데이터 정의가 잘 되지 않은 데이터)가 구축될 가능성이 있어 연구진들은 ChatGPT/GPT-4를 활용한 데이터 구축 방법을 고안하였다.

연구진들은 기존에 공개된 Image-Text Pair data를 활용하여 image $X_v$와 그에 해당하는 caption $X_c$가 있을 때, ChatGPT/GPT-4에게 이미지 내용을 설명하도록 하는 질물 세트 $X_q$를 생성하라고 지시하여 instruction following data를 구축하였다. 이와 같은 방법은 저비용으로 dataset 구축을 가능하게 하지만 instruction과 response에 대해 다양성과 in-depth reasoning이 부족하다는 단점이 있다. 생성된 데이터는 아래와 같은 형태가 된다.

<center><img width="400" src="https://github.com/finddme/finddme.github.io/assets/53667002/bd6df9b4-4f8c-4bff-8009-122b41353b52"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

위 방법으로 Instruction-Following Dataset을 구축할 때 두 가지 문제가 있었다. 아래는 문제와 문제 완화를 위해 적용한 방법이다:

<strong>1. language-only GPT-4/ChatGPT를 사용했기 때문에 모델이 Visual Content를 인지할 수 없는 문제</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 이 문제 해결을 위한 대전제는 image를 LLM이 인식할 수 있는 시퀀스로 encoding 가능하게 하는 것이다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(i) Caption</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Caption을 GPT-4/ChatGPT에 함께 입력함으로써 language-only 모델이 visual scene을 알 수 있도록 한다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(ii) Bounding box</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; visual scene에서 특정 객체의 위치 정보를 좌표로 prompt에 넣어주어 language-only 모델이 객체를 인식할 수 있도록 한다.

<strong>2. Instruction-following data 유형 정의 및 유형별 포함 내용 정의 문제</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conversation, Detailed description, Complex reasoning 유형을 구축하여 세부 사항을 정의하여 총 158,000개의 data를 구축하였다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(i) Conversation (58,000개 생성)</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 주어진 이미지에 대해 Assistant와 Human이 대화하는 형태로 구성.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Assistant가 image를 보고 Human의 질문에 답하는 것과 같은 형식.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - image 내의 직관적인 시각적 정보에 대한 QAset으로 생성. (객체 유형, 객체 수, 객체 동작, 객체 위치, 객체 간의 상대적 위치 등)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(ii) Detailed description (23,000개 생성)</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - image의 caption과 box 정보를 제공한 후 이에 대한 상세 설명을 생성하라는 prompt를 입력하여 GPT-4가 image에 대한 상세 설명을 생성하도록 함.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - prompt는 다양하게 준비해 놓고 랜덤하게 입력<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>(iii) Complex reasoning (77,000개 생성)</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 위 두 유형을 통해 시각적 내용 자체에 중점을 둔 data를 생성하한 후 이를 바탕으로 심층적 추론 질의응답셋을 GPT-4를 통해 생성한다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 답변에 단계별로 엄격한 논리를 포함하도록 prompt를 생성하고 이 prompt와 caption 그리고 box 정보를 모델에 함께 임력한다.<br>

# 3. Visual Instruction Tuning

## 3.1 Architecture

LLaVA의 visual encoder와 LLM($f_ϕ$)을 연결한 구조를 가진다. 현재는visual encoder와 LLM에 대해 매우 여러 조합의 버전으로 LLaVA가 나왔지만 해당 논문에 작성된 LLaVA는 visual encoder로 CLIP Vision Encoder인 VIT-Large 14와 LLM으로는 LLama를 instruction tuning한 Vicuna를 용하였다. visual encoder와 LLM은 단순한 linear layer로 연결한다. 즉, LLM의 Word Embedding Sapce와 Visual Encoder의 Fueature Space를 Linear 결합한다.

<center><img width="700" src="https://github.com/finddme/finddme.github.io/assets/53667002/9b5d042e-b546-47c4-8d83-993318935037"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

 
## 3.2 Training

각 image $X_v$에 대해 turn 수가 $T$개인 multi-turn conversation data $(X_1^q, X_1^a, \cdots, X_T^q, X_T^a)$를 생성한다. 이때 답변은 dataset 상에서 Assistant에 해당하는 것으로 하고, $t$번째 turn의 instrction $X_t^{instruct}$은 아래와 같이 정의한다. 첫 번째 turn에만 image $X_v$를 입력하고 나머지 turn들은 text이기 때문에 첫 turn에 입력되는 $X_v$와 $X_q$의 순서는 크게 상관 없다.

<center><img width="600" src="https://github.com/finddme/finddme.github.io/assets/53667002/2c1692a6-8804-4654-84ff-b0802d90bf15"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

multimodal instruction-following sequence는 아래 이미지와 같은 형식으로 표현된다.

<center><img width="700" src="https://github.com/finddme/finddme.github.io/assets/53667002/d74d0f0e-9bbc-4f9f-a542-d32dd5de475d"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>

LLM에 위 데이터를 넣어 original auto-regressive training task를 통해 token을 예측하며 학습을 수행한다. 위 예시는 두 turn만 나타내지만 실제 turn 수는 더 다양하다. 실제 구현은 사용된 모델에 따라 달라지겠지만 LLM으로 Vicuna를 사용할 때는 <STOP>을 ### 구분자로 설정했다. 모델은 assistant의 답변과 답변 중단 지점을 예측하도록 학습되기 때문에 위 이미지에서 초록색으로 표시된 sequence/tokens만 loss 계산에 사용된다.

sequence의 길이가 $L$일 때 정답 $X_a$에 대한 확률은 아래와 같이 계산된다. 이때 $X_{instruct<i}$는 현재 예측 token인 초록색 글씨 $x_i$ 이전의 모든 turn에 대한 instruction token을 의미하고, $X_{a<i}$는 현재 예측 토큰인 $x_i$ 이전의 모든 turn에 대한 answer token을 의미한다. 아래 수식은 image와 query에 대해 answer가 나올 확률을 계산할 때 image, instruction, answer에 대한 이전의 모든 token을 집어넣고 이를 모두 곱하여 확률값을 계산한다. 

<center><img width="500" src="https://github.com/finddme/finddme.github.io/assets/53667002/088baa57-451f-4781-885c-171f032f0e03"></center>
<center><em style="color:gray;">Visual Instruction Tuning</em></center><br>


LLaVA는 두 단계의 instruction-tuning 절를 거친다.

- **Stage 1. Pre-training for Feature Alignment:** CC3M의 subset을 기반으로 projection layer matrix만 업데이트한다.
- **Stage 2. Fine-tuning End-to-End:** 투영 행렬과 LLM 모두 두 가지 다른 사용 시나리오에 맞게 업데이트됩니다:
  - **Multimodal Chatbot(Visual Chat)**: LLaVA는 일상적인 사용자 지향 application(downstream) 수행을 위해 multimodal instruction-following data로 fine-tuning을 진행한다.
  - **Science QA**: LLaVA는 science domain을 위해 multimodal reasonsing dataset으로 로 fine-tuning을 진행한다.

### 3.2.1 Stage 1: Pre-training for Feature Alignment

Pre-Training 과정에서는 image Caption을 활용하여 Visual Tokenizer를 LLM에 호환 가능하도록 학습시키는 것이 주요 목적이다.

1) CC3M dataset을 595,000개의 image-text 쌍으로 filtering. 이때 instrction과 image, question을 입력으로 받고 원 데이터셋에서 caption에 해당하는 것을 예측하며 학습한다.
  
2) 학습 시에는 visual encoder와 LLM의 weight는 frozen시키고 trainable parameter인 $\theta = W$(projection matrix)에 대해서만 likelihood를 maximize한다. 이와 같은 과정을 통해 학습으 하면 image feature $H_v$가 pre-trained LLM word embedding에 align될 수 있다. 해당 단계는  frozen LLM에 대해 visual tokenizer를 학습시키는 것으로 볼 수 있다.


### 3.2.2 Stage 2: Fine-tuning End-to-End

Fine-tuing 과정에서는 LLM과 projection layer를 미세 조정하는 것이 주요 목적이다.

Fine-tuning 시, visual encoder weight는 frozen 시키고 projection layer와 LLM의 weight만 업데이트한다. 이 때 두 가지 scenario를 위한 fine-tuning을 수행한다.

**1) Multimodal Chatbot(Visual Chat)**<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 앞서 구축한 158K(15만 8천개) language-image instruction-following data로 fine-tuning

**2) Science QA**<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 과학 관련 질의응답 데이터로, 이미지 혹은 자연어로 질문이 들어가면 자연어로 답변을 반환하는 데이터이다. 해당 데이터의 question & context을 instruction으로 LLaVA에 입력하고 reasoning & answer를 output으로 설정하여 학습한다.


# 4. Experiments









# Reference

> Visual Instruction Tuning

> https://llava-vl.github.io/
