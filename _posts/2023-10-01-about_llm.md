---
title: "About LLM"
category: LLM / Multimodal
tag: NLP
---







* 목차
{:toc}











대부분의 LLM은 Transformer의 Decoder 구조를 기반으로 한다.

# Transformers 요약

## Encoder and Decoder

- Encoder
  - input를 representation을 변환하는 부분. 문장을 입력 받아 언어적 특징과 의미를 vector화한다.
  - Attention
    - 모델이 입력 sequence의 모든 token들이 서로의 관계를 학습할 수 있도록 Multi-Head Self-Attention을 사용한다. 이는 모델이 각 token의 context를 이해하는데 도움을 준다.

- Decoder
  - encoding 된 representation을을 받아 모델의 출력을 생성하는 부분.
  - input
    - 학습 시에는 정답 sequence(shifted right)가 입력되고, 추론 시에는 이전에 예측된 token이 입력된다.
  - Attention
    - Masked Multi-Head Self-Attention
      - 현재 시점 이후의 token들에 대해 masking 처리를 하여 Masked Multi-Head Self-Attention을 사용한다.
      - 이와 같은 처리는 현재 시점 이전 정보만을 가지고 현재 시점의 token을 예측하도록 한다.
    - Multi-Head Attention with Encoder Output
      - Encoder의 출력(input에 대한 representation)을 받아서 input sequnece와 decoder의 input으로 입력 받은 target sequnece 간의 관계를 매핑하며 학습하도록 돕는다.
  - output
    - token별로 softmax 함수를 거쳐 (현재 시점을 기준으로 다음 token으로) 예측된 단어의 확률 분포를 산출하고 가장 높은 확률의 token을 출력한다.
 
## Attention

# Transformers 기반 Model 요약

## Encoder-Only Models

- 대표 모델: BERT
- Pretraining Approach/Task: 기본적으로 Masked Language Modelling (MLM)을 통해 학습하며, 모델에 따라 이외에 추가적인 task를 함께 수행한다. BERT는 MLM 외에 NSP를 학습한다.
- Use Case: 일반적으로 classification task에 많이 활용된다.
  
## Decoder-Only Models
이전 time step의 출력이 다음 time step의 입력으로 들어가는 점이 자기 회기적이기 때문에 Auto-regressive model이라고도 불린다. 최근 LLM은 대부분 Decoder-Only 구조를 가진다.

- 대표 모델: GPT
- Pretraining Approach/Task: Next Token Prediction. Original Language Modeling이라고도 많이 불린다.
- Use Case: 일반적으로 Generative task에 많이 사용된다.
   
## Encoder-Decoder Models

- 대표 모델: T5, BART, Gemini
- Pretraining Approach/Task: Task에 따라 다르다.


# Factors to Consider When Choosing an Architecture of Models

