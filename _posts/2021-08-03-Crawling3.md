---
title: "Crawling3;CODE"
category: Python Basic Syntax
tag: Python Basic Syntax
---

**파이썬 크롤링 실습 코드 3: [https://github.com/finddme/Python/blob/master/Python_Crawling3.ipynb](https://github.com/finddme/Python/blob/master/Python_Crawling3.ipynb)**









* 목차
{:toc}

















## Continuous Crawling2

이제 제목과 내용을 함께 추출해보자


```python
import requests
from bs4 import BeautifulSoup
import re  
'''
일단 각 포스트로 넘어가는 url들을 다 추출하자 
함수를 만들어서 추출해보자

# 1. 제목을 titles1 배열에 담아줄 것이기 때문에 빈 배열을 미리 만들어 놓는다.
# 2. 카테고리 전체를 보여주는 페이지의 주소에 get방식으로 접근한다.
     해당 페이지까지 접근하는 요청 객체를 req1로 생성한다.
# 3. 위 페이지의 html문서를 가져온다.
# 4. soup1이라는 변수에 BeautifulSoup을 통해 해당 html문서(html1)를 파싱한 결과를 담는다.
# 5. 페이지 소스코드를 보면 제목들이 <div class="tags-expo-section">에 담기는 것을
     확인할 수 있다. 그래서 모든 'div'태그를 찾아내서 해당 태그의 'class' 속성의 값이
     tags-expo-section인 것들을 다 찾아라
# 6. 앞서 찾아낸 것들을 하나씩 돌면서 검색을 시작할거다
# 7. 내부에 존재하는 모든 'a'태그(여기에 제목 적혀있다)를 검색해서 links1에 담는다
# 8. links1에 담긴 a태그를 돌면서 
# 9. a태그의 속성값 중 'href'을 찾아 그 내용을 href1에 담는다
# 10. 아까 만든 titles1 배열에 차곡차곡 담는다
# 11. titles1을 반환한다.
'''
def get_all_categories():
    titles1 = [] # 1
    req1 = requests.get("https://finddme.github.io/") # 2
    html1 = req1.text # 3
    soup1 = BeautifulSoup(html1, "html.parser") # 4
    divs1 = soup1.findAll('div',{"class":"tags-expo-section"}) # 5
    for div1 in divs1: # 6
        links1 = div1.findAll('a') # 7
        for i in links1: # 8
            href1 = i.attrs['href'] # 9
            titles1.append(href1) # 10
    return titles1 # 11

result = get_all_categories()

print(result)

```

    ['/coreference%20resolution%20/%20zero-anaphora%20resolution/2021/04/24/CoreferenceResolution/', '/coreference%20resolution%20/%20zero-anaphora%20resolution/2019/10/28/Rearrangecategories3/', '/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/', '/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/', '/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/', '/natural%20language%20processing%20and%20linguistics/2021/03/30/Johnblog/', '/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/', '/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/', '/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/', '/natural%20language%20processing%20and%20linguistics/2019/10/29/Rearrangecategories/', '/natural%20language%20processing/2019/11/22/Bert/', '/natural%20language%20processing/2019/11/19/Transformer/', '/natural%20language%20processing/2019/11/12/Attention/', '/natural%20language%20processing/2019/11/11/Seq2Seq/', '/natural%20language%20processing/2019/11/07/GloVe(Global-Word-Vectors)/', '/natural%20language%20processing/2019/11/07/FastText/', '/natural%20language%20processing/2019/11/06/Skipgram/', '/natural%20language%20processing/2019/11/05/CBOW(Continuous-Bag-Of-Words-Model)/', '/natural%20language%20processing/2019/11/04/NPLM(Neural-Probabilistic-Language-Model)/', '/natural%20language%20processing/2019/11/03/Word-embedding(Distributed-Representation)/', '/natural%20language%20processing/2019/11/03/LSA(Latent-Sematic-Analysis)/', '/natural%20language%20processing/2019/11/01/Word-Vectors/', '/natural%20language%20processing/2019/10/30/NLP/', '/machine%20learning,%20deep%20learning/2021/08/03/KMeans/', '/machine%20learning,%20deep%20learning/2020/10/04/L1L2/', '/machine%20learning,%20deep%20learning/2020/10/01/TransferLearning/', '/linguistics%20%7C%20english/2021/04/04/Longdistance/', '/linguistics%20%7C%20english/2020/10/02/Rearrangecategories2/', '/conversation%20analysis/2020/11/07/RepairMechanisms/', '/conversation%20analysis/2020/11/06/SequenceExpansions/', '/conversation%20analysis/2020/11/05/AdjacencyPair/', '/conversation%20analysis/2020/11/04/Preference/', '/conversation%20analysis/2020/11/03/TurnTaking/', '/conversation%20analysis/2020/11/02/Intersubjectivity/', '/conversation%20analysis/2020/11/01/Conversation/', '/linguistik%20%7C%20germanistik/2020/12/14/Textlinguistik/', '/linguistik%20%7C%20germanistik/2020/12/13/Pragmatik/', '/linguistik%20%7C%20germanistik/2020/12/12/SemantikT/', '/linguistik%20%7C%20germanistik/2020/12/11/SemantikGB/', '/linguistik%20%7C%20germanistik/2020/12/10/Attribute/', '/linguistik%20%7C%20germanistik/2020/12/09/ErgaenzungenundAngaben/', '/linguistik%20%7C%20germanistik/2020/12/08/DependenzundValenz/', '/linguistik%20%7C%20germanistik/2020/12/07/TraditionelleSyntaxanalyse/', '/linguistik%20%7C%20germanistik/2020/12/06/Wordbildung/', '/linguistik%20%7C%20germanistik/2020/12/05/Morphologie/', '/linguistik%20%7C%20germanistik/2020/12/03/PhonetikundPhonologie/', '/linguistik%20%7C%20germanistik/2020/12/02/Semiotik/', '/linguistik%20%7C%20germanistik/2020/12/01/Linguistik/', '/language/2021/05/16/DeutschWendung2/', '/language/2021/04/25/DeutschWendung/', '/language/2021/01/02/DeutschGrammtik/', '/python/2021/08/03/Tensorflow/', '/python/2021/08/03/Crawling2/', '/python/2021/08/03/Crawling1/', '/python/2021/07/21/Pyfile/', '/python/2021/07/20/PyClass/', '/python/2021/07/19/PyModule/', '/python/2021/07/18/PyFunction/', '/python/2021/07/18/PyBuiltin/', '/python/2021/07/17/PyExeptionHandling/', '/python/2021/07/16/PyConditional_Iterative/', '/python/2021/07/15/PyDictionary/', '/python/2021/07/14/PyTuple/', '/python/2021/07/13/PyList/', '/python/2021/07/12/PyString/', '/python/2021/07/11/PyDataType/', '/python/2021/07/11/PyCalculation/', '/python/2021/01/03/Rearrangecategories4/', '/git/2021/07/10/Git3/', '/git/2021/07/08/Git2/', '/git/2021/07/05/Git1/', '/git/2021/01/04/Rearrangecategories3/', '/etc2021/03/14/comingsoon/', '/etc2021/01/24/ssss/']
    


```python
'''
[1], [2]

# 1. 하나의 포스트 제목에 대한 내용을 담을 class를 정의해보자
# 2. 이제 생성자를 만들어 준다. 
     제목과 내용을 담을 거니까 title과 contents를 변수로 구성한다.
     생성자는 특정한 클래스의 인스턴스를 초기화해주는 역할을 한다.
     생성자를 통해서 빠르게 각각의 객체를 생성할 수 있다.
     
이렇게 title과 contents를 받는 Posts객체를 하나 만들었다.

# 3. 생성자에서 정의한 해당 클래스의 내용 자체를 출력해주는 string함수를 사용한다.
     출력될 모양을 정해준다.
'''
class Posts: # 1
    def __init__(self,title, contents): # 2
        self.title = title
        self.contents = contents
    def __str__(self): # 3
        return "제목: " + self.title + "\n 포스트: " + self.contents
        
```


```python
'''
[1] 
# 1. 제목과 내용을 담을 빈 배열을 만든다.
# 2. 나중에 iteration을 몇 번째 도는지 출력할 것이기 때문에 i=0을 하나 만들어 놓는다.
# 3. 모든 url을 추출한 결과가 담긴 result(배열형태)에 하나씩 접근한다
# 4. 한번 접근할 때마다 i에 1을 더한다
# 5. 전체 reslult 개수 중에 몇 번째를 보고 있는지 출력한다.
# 6. 이 블로그는 특정 포스트에 접근할 때 
     https://finddme.github.io + url을 통해 접근하기 때문에 
     추출한 url들을 https://finddme.github.io뒤에 붙여서
     각 포스트에 get방식으로 접근하도록 한다.
# 7. 접근한 페이지의 HTML문서를 가져온다
# 8. 가져온 HTML문서를 파싱한다.
# 9. HTML문서에서 'div'태그를 다 찾고 class의 속성값이 post인 것들을 찾아서 tncs에 넣는다
     (<div class="post">에 제목부터 내용까지 다 담겨있기 때문이다)
# 10. tncs를 차례대로 돌면서 h1태그를 t에 담고
     (<h1 class="post-title">에 제목이 있어서)
     p태그를 찾아 c에 담는다 (<p>에 내용일 담겨있어서)
# 11. t 안에 담긴 내용을 t2txt에 담고, c 안에 담기 내용을 c2txt에 담는다.
# 12. 아까 만든 Posts클래스의 변수로 t2txt(title자리에)와 c2txt(contents자리에)를 넣는다
# 13. 그리고 그걸 아까 만든 빈 배열 contents에 넣는다.

이걸 계속 반복한다
'''

contents = [] # 1
i = 0 # 2

for sub in result: # 3
    i = i + 1 # 4
    print("(", i, "/", len(result),")", sub) # 5
    req2 = requests.get('https://finddme.github.io' + sub) # 6
    html2 = req2.text # 7
    soup2 = BeautifulSoup(html2, 'html.parser') # 8
    tncs = soup2.findAll('div',{'class': "post"}) # 9
    for tnc in tncs: # 10
        t = tnc.find('h1')
        c = tnc.find('p')
        t2txt = t.text # 11
        c2txt = c.text
        tc = Posts(t2txt, c2txt)
        contents.append(tc)
    #print("총", len(contents),"개의 포스트")
    if i == 5: # i가 5가되면 반복문을 나와라(74개 다 기다리기 귀찮으니까)
        break;

for cc in contents: # contents에 담긴 것들을 다 출력한다
    print(cc)

# 이 결과가 매우 이상하다. 이유는 <p>태그가 포스트 내용 각 줄마다 달려있기 때문이다.
# 그래서  <p>태그를 찾아서 c에 넣어주고 나면 그 포스트를 그만 읽어버려서 한줄만 읽고 끝나는 것이다.
# 이 문제를 [2]에서 해결해보자
```

    ( 1 / 74 ) /coreference%20resolution%20/%20zero-anaphora%20resolution/2021/04/24/CoreferenceResolution/
    ( 2 / 74 ) /coreference%20resolution%20/%20zero-anaphora%20resolution/2019/10/28/Rearrangecategories3/
    ( 3 / 74 ) /natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/
    ( 4 / 74 ) /natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/
    ( 5 / 74 ) /natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: coreference에 대한 세 가지 일반적인 아키텍쳐는 mention-pair, mention-rank, 그리고 entity-based로 각각 feature-based 혹은 neural classifier를 사용할 수 있다. 이번 장에서는 우선 feature-based 알고리즘에 대해서 설명하겠다.
    제목: ㅡ
     포스트: 카테고리정렬
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: 많은 논문들에서 BERT가 어떻게 언어를 이해하는지에 대해 다루는데 본 논문에서는 BERT가 통사적 지식을 어떻게 가지는지를 평가해보고자 한다. BERT의 통사적 지식은 한가지 방법으로만 평가될 수는 없다. 따라서 크게 두 가지 방법을 통해 실험을 진행한다.
    제목: Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))
     포스트: 본 연구에서는 영어의 장거리 의존문장을 여러 학습 모델에서 어떻게 처리하는지에 대해 실험한다. 장거리 의존 구문이란 자연어가 가지고 있는 sequential 한 속성과 더불어 hierarchical한 속성을 가지고 있다는 대표적인 예시이다. 영어에서 나타나는 대표적인 장거리 의존 구문은 WH-question, Relative clause, Topicalization이다. 장거리 의존 구문에 대한 자세한 설명은 Long-Distance Dependency 여기에 있다.
    제목: What does BERT learn about the structure of language?(Jawahar.G(2019))
     포스트: pre-trained된 BERT모델을 downstream NLP task에 사용된다. 일반적으로 BERT는 unsupervised learning으로 학습되지만 다른 task에 이용하기 위해 supervised learning으로도 학습하여 사용될 수도 있다. supervised learning으로 학습된 BERT가 GLUE benchmark의 11개 task에서 기존의 state-of-the-art를 뛰어넘었다고 한다. 그런데 BERT가 왜 잘 되는지 아직 잘 모른다고 한다.
    


```python
'''
[2]
[1]에서 나온 문제를 해결하기 위해 findAll을 사용해서 모든 <p>를 찾아줘야 한다.
근데 findAll은 해당 태그를 다 찾는 것이기 때문에 찾은 것을 리스트에 담아 반환한다.
리스트를 text로 바꾸기 위해서는 안에 들어있는 것들을 하나씩 돌면서 변환해줘야 한다.
'''
contents = []
i = 0

for sub in result:
    i = i + 1
    print("(", i, "/", len(result),")", sub)
    req2 = requests.get('https://finddme.github.io' + sub)
    html2 = req2.text
    soup2 = BeautifulSoup(html2, 'html.parser')
    tncs = soup2.findAll('div',{'class': "post"}) 
    for tnc in tncs:
        t = tnc.find('h1')
        t2txt = t.text
        
        c = tnc.findAll('p') # p태그를 전부 찾는다
        for p in c: # 전부 찾은 p태그가 배열 형태로 반환된 것을 담은 c를 돌면서 
            c2txt = p.text # 텍스트로 바꿔준다.
            tc = Posts(t2txt, c2txt)
            contents.append(tc)
    #print("총", len(contents),"개의 포스트")
    if i == 3:
        break;

for cc in contents:
    print(cc)

# 근데 포스트 내용 모든 줄에 p태그가 있어서 한 줄을 출력할 때마다 제목이 같이 출력된다.
# 마음에 안 든다
```

    ( 1 / 74 ) /coreference%20resolution%20/%20zero-anaphora%20resolution/2021/04/24/CoreferenceResolution/
    ( 2 / 74 ) /coreference%20resolution%20/%20zero-anaphora%20resolution/2019/10/28/Rearrangecategories3/
    ( 3 / 74 ) /natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: coreference에 대한 세 가지 일반적인 아키텍쳐는 mention-pair, mention-rank, 그리고 entity-based로 각각 feature-based 혹은 neural classifier를 사용할 수 있다. 이번 장에서는 우선 feature-based 알고리즘에 대해서 설명하겠다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 따라서 이번 장에서는 categorization을 사용한 시스템의 다양한 architecture를 개괄적으로 살펴본다. 알고리즘은 크게 entity-based(담화모델에서 각 개체를 나타내는 방식으로 coreference를 결정)인지, mention-based(각 mention을 독립적으로 고려하는 방식으로 coreference를 결정)인지 혹은 잠재적 선행사를 직접적으로 비교하기 위해서 ranking model을 사용하는지의 여부에 따라 구분된다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 먼저 mention-based algorithm인 mention-pair architecture에 대해서 살펴보겠다. mention-pair architecture는 이름에서도 알 수 있듯이 한 쌍의 mention, candidate anaphor그리고 candidate antecedent(후보선행사)가 주어지는 classifier를 기반으로 하며, binary classification(이항분류; corefering하는지 안 하는지)를 통해 결론을 도출한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 
    위 예문에 이 classifier의 task를 대입해보면 그림 22.2와 같이 된다. 이것은 예문에 있는 대명사 she와 그에 대한 잠재적 선행사 mention이 이루는 쌍에 대해 coreference link의 확률을 할당하는 것을 보여주는 그림이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 여기에서 각각의 Victoria Chen, Megabucks Banking, her과 같은 prior mention에 대해 binary classifier는 she의 선행사가 그 mention인지 아닌지 확률을 계산한다. 여기에서 우리는 이 확률을 실제 선행사(Victoria Chen, her, the 38-year-old)에 대해서는 높이고 선행사가 아닌 것(Megabucks Banking, her pay)에 대해서는 낮춰야 한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 훈련을 위해서는 training samples를 선택해야 하는데 이때 문서의 대부분 mention쌍은 corefernent가 아닌데 이것을 다 훈련 샘플로 선택하면 너무 많은 negative sample이 생기기 때문에 heuristic method가 필요하다. 가장 일반적인 heuristic은 positive example로 가장 가까운 선행사를 택하고, 그 사이에 있는 모든 쌍들은 negative example로 선택하는 것이다. 이것을 조금 formal하게 표현하자면, anaphor mention $m_i$에 대해서
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: Classifier가 훈련되고 나면 clustering단계에서 test sentence에 적용된다. clustering에는 두 가지 과정이 있다:  closest-first clustering과 best-first clustering.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: - closest-first clustering
    closest-first clustering에서 classifier는 오른쪽에서 왼쪽으로, 즉 mention $i-1$에서 mention 1까지로 실행된다. 그리고 확률 0.5가 넘는 first antecedent는 $i$와 연결된다. 선행사가 확률 0.5를 넘지 못하는 경우에는 $i$에 대한 선행사가 선택되지 않는다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: - best-first clustering
    best-first clustering에서 classifier는 모든 선행사들에 대해서 실행되는데, 가장 유력한 antecedent mention이 $i$의 선행사로 선택된다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: mention-pair model은 단순하다는 장점이 있지만 두 가지 주요한 문제가 있다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 1) classifier가 직접적으로 후보 선행사들을 비교하지 못한다. 따라서 두 선행사 사이에서 실제로 더 나은 것이 무엇인지 결정하는 훈련이 이루어지지 못한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 2) 담화 모델을 무시하며, entity가 아닌 단지 mention만을 살핀다. 각 classifier decistion은 동일한 entity의 다른 mention들을 고려하지 않고 한 쌍에 대해서만 완전히 국부적으로 이루어진다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: Mention-Rank Architecture는 mention-pair model의 결점을 보완한 model이다. mention-pair model의 단점 중 하나는 선행사 후보를 직접적으로 비교하지 않는다는 것이다. mention-ranking model은 이를 보완하여 선행사 후보를 직접적으로 비교하며 각 anaphor에 대해서 높은 점수의 선행사를 선택한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: mention-ranking systems에서는 mention(anaphor)에 $i-1$에서 1까지, 그리고 여기에 추가적으로 선행사가 없다는 것까지 고려하기 위한 dummy mention $\epsilon$까지의 범위에 걸쳐 관련된 랜덤 변수 $y_1$을 가진다. dummy mention $\epsilon$은 discourse-new나 새로운 coref chain의 시작 혹은 non-anaphoric을 나타낸다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 그림 22.3은 single candidate anaphor she에 대한 계산 예를 보여준다. mention-ranking system은 후보 anaphoric mention she에 대해 이전의 모든 mention들, 그리고 추가적으로 특수한 dummy mention $\epsilon$에 대해 소프트맥스를 계산하여 확률분포를 할당한다(각 후보 선행사들에 대한 확률을 구한다).
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 이전의 mention-pair model의 훈련은 단순했는데, mention-ranking model의 훈련은 비교적 까다롭다. 왜냐하면 mention-pair model에서는 positive-sample / negative-sample을 만들어서 훈련에 사용되는 sample pair를 다 알았는데, mention-ranking model은 각 anaphor에 대해서 훈련을 위해 사용할 수 있는 모든 gold antecedent들을 알 수 없기 때문이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 대신 각 mention에 대한 best antecedent는 잠재되어 있다; 즉 각 mention에 대해서 선택할 수 있는 legal gold antecednet의 전체 클러스터를 가지고 있다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 이런 잠재된 선행사를 모델링하는 다양한 방법들이 존재하는데, 이는 3장에서 상세히 다룰 것이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: Mention-ranking model 은 hand-build feature와 neural representation learning 둘 다를 통해 구현될 수 있느데, 전자는 2장에서, 후자는 3장에서 자세히 다룰 것이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 앞서 소개한 mention-pair model과 mention-ranking model 둘 다 mention-based algorithm이었고, 이번에 소개할 모델은 entity-based model이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: - entity-based model
    entity-based model은 각 mention을 이전 mention이 아닌 이전 담화 개체(mentions들의 cluster)에 연결한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: - entity-ranking model
    entity-ranking model은 단순히 mention-ranking model에서 classifier가 개별 mention들이 아니라 mention들의 cluster들에 대해 결정을 내리게 한 것이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: entity-based model도 mention-ranking model처럼 feature-based model과 neural model 둘 다를 통해 구현될 수 있는데, 이전의 모델들보다 표현력이 더 뛰어나지만 cluster-level 정보를 이용하는 것이 실제로 큰 성능 향상으로 이어지지 않았기 때문에 mention-ranking model이 여전히 더 많이 사용된다. 그래서 entity-based model은 간단하게만 소개하고 넘어가도록 하겠다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: feature-based model의 경우 클러스터에서 feature를 추출하여 이를 수행할 수 있다.
    Neural model은 cluster의 representation(cluster를 vector로 표현하는 것)을 자동으로 학습할 수 있다. 예를 들어 cluster representation에 해당하는 상태를 인코딩하기 위해 일련의 cluster mention에 대해 RNN을 이용하거나, mention pair의 학습된 representation에 대해 pooling하여 cluster쌍에 대한 distiributed(dense) representation을 학습한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: ‘Distiributed’라는 말이 붙는 이유는 하나의 정보가 여러 차원에 분산되어 표현되기 때문이다. 하나의 차원이 여러 속성들이 버무려진 정보를 들고 있다. 즉, 하나의 차원이 하나의 속성을 명시적으로 표현하는 것이 아니라 여러 차원들이 조합되어 나타내고자 하는 속성들을 표현하는 것이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 이번 장에서는 coreference resolution을 위해 logistic regression, SVM 또는 random forest classifier에서 일반적으로 사용되는 feature들에 대해 기술한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: anaphor mention과 potential antecedent mention이 있을 때 대부분의 feature based classifier는 세 가지 유형의 feature를 사용한다:
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 1) anaphor에 대한 feature,
    2) 선행사 후보에 대한 feature,
    3) 쌍 사이의 관계에 대한 feature.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: Entity-based model은 추가적으로 두 개의 클레스를 더 사용할 수 있다:
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 4) 선행사의 개체 클러스터로부터 온 모든 mention들에 대한 feature,
    5) 선행사 entity cluster의 mention과 anaphor간의 관계에 대한 feature.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 그림 22.4는 일반적으로 사용되는 feature들을 보여준다. 앞서 사용한 예문에서 잠재적 anaphor she와 잠재적 선행사 Victoria Chen에 대한 것이다. 노란색 박스로 표시된 것은 이전 연구에서 특히 유용하다고 밝혀진 feature이고 파란색 박스로 표시된 것은 neural model에 사용했을 때 도움이 될 수 있는 feature이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 위는 Aanphor나 선행 mention에 대한 feature들이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 위는 선행사 entity에 대한 feature(여기에서는 she에 대한 선행사니까 Victoria Chen, her, the 38-year-old에 대한 것; 그래서 Antecedent cluster size가 3)이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 위는 mention쌍에 대한 feature이다. 
    Longer anaphor는 anaphor가 선행사보다 기냐
    Paris of any features는 선행사 + anaphor 유형쌍에 대한 각 feature. 어떤 feature든 선행사 + anaphor쌍으로.
    Sentence distance는 선행사와 anaphor 사이에 있는 문장 수
    Mention distance는 선행사와 anaphor 사이에 있는 mention 수
    i-within-i는 어떠한 mention이 다른 mention에 포함되어 있으며 두 mention의 reference가 같은 것.
    Appositive 동격(앞의 내용을 명사구나 다른 명사를 사용해 정의하거나 설명하는 것). anaphor가 선행사와 동격관계에 있는 경우. 이것은 mention들이 동격어가 아닌 경우에도 유용하다. 동격어를 선행 헤드와 연결하는 것을 알기 위해.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 위는 entity쌍에 대한 feature이다.
    Exact String Match는 determiner와 modifier를 모두 포함하여 정확히 동일한 텍스트를 포함하는 경우에만 두 mention을 연관시키는 것이다. 즉 선행사와 anaphor cluster에 있는 아무 두 mention의 string(문자열)이 같으냐
    Head Word Match는 선행사 cluster에 있는 mention과 anaphor cluster의 mention이 같은 headword를 가지냐
    Word Inclusion는 선행사 cluster에 있는 단어가 anaphor cluster에 있는 모든 단어를 포함하냐
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 그 아래에 문서에 대한 feature는 장르가 뭔지에 대한 것이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: feature-based system에서 feature들의 결합을 사용하는 것은 중요하다고 한다. 한 실험에서는 classifier의 개별 feature들을 사용한 것과 다중 feature의 결합을 사용했을 때를 비교해 봤을 때 결합한 것의 F1 score가 4점 높았다고 한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 초기 classifier들은 2장에서 소개한 hand-built feature를 사용했고, 보다 최근의 classifier들은 지금 소개할 neural representation learning을 사용한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 이번 장에서는 Lee et al. (2017b)의 neural mention-ranking system에 대해 기술하겠다. 이 end-to-end system은 별도의 mention-detection단계를 가지고 있지 않다. 대신 가능한 모든 텍스트 범위를 설정된 length(예를 들어 길이 1, 2, 3, … N의 모든 n-gram)까지를 가능한 mention으로 간주한다(Lee의 논문에서는 length를 10으로 설정했다).
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 그림 22.5는 span representation과 mention score에 대한 계산을 보여준다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 그림 22.6은 그림 22.5의 예시 문장에서 the company의 가능한 세 가지 선행사에 대한 score $s$를 계산하는 것을 보여준다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 위 부분을 살펴보기 위해 나중에 계산될 span representations $g_i$에 대해서 잠시 얘기하도록 하겠다. span representations $g_i$는 스팬에서의 첫 번째 단어와 마지막 단어의 contextual representation 그리고 스팬에 있는 headword에 대한 representation 그리고 feature 하나로 구성된다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 여기에서 스팬의 첫 번째 단어와 마지막 단어의 contextual representation이 standard biLSTM으로 계산된다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 그리고 biLSTM은 ELMo와 같은 contextual word embedding을 기반으로 해서 각 단어에 대한 representation $w_t$를 input으로 받는다 (ELMo대신 BERT를 사용하면 성능이 훨씬 향상된다).
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 그러니까 biLSTM은 $w_t$를 input으로 받아서 output으로 $h_t$를 내는 것이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 위 부분은 스팬의 head를 나타내기 위한 부분이다. 시스템은 스팬의 head를 나타내기 위해 스팬의 단어들에 대해 attention을 사용하였다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: span representation $g_i$는 이전에 말한 것처럼 스팬의 시작과 끝의 hidden representation(hidden state 값), head 그리고 스팬의 $i$의 length같은 feature vector(이전 장에서 neural model에서 유용할 수 있는 feature 중 하나)를 모두 concatenate한 것이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 위 그림은 the company의 가능한 세 가지 선행사에 대한 score $s$를 계싼하는 것을 보여주는 그림이다. antecedent score부분을 보면 the company에 대해서 General Electric이 선행사인 경우와 the Postal Service가 선행사인 경우 각각이 들어가는 것을 볼 수 있고, 이들의 요소별 곱도 들어가며, 추가적으로 다른 feature들도 들어간다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: $m(i)$와 $c(i,j)$는 scoring 함수로, 각각 mention score, antecedent score를 나타내며 둘 다 스팬 $i$를 나타내는 백테 $g_i$를 기반으로 한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: FFNN(Feed Forward Neural Network). 가중치의 반복적인 업데이트. 
    1) 인풋 $x$를 받아서
    2) 이것의 $y=Wx+b$를 계산하고
    3) 여기에 activation function(sigmoid, tanh, ReLU, etc.)를 적용
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 선행사 score $c(i, j)$는 input으로 스팬 $i$와 $j$의 representation을 취하며, $g_i \circ g_j$($g_i$와 $g_j$의 요소별 곱)은 두 스팬의 요소별 유사도이다. 그리고 마지막에는 mention distances 그리고 화자와 장르에 대한 정보와 같은 유용한 feature들을 인코딩한 feature vector $\varphi(i, j)$가 들어간다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 이제 coreference score $s$를 보겠다. score $s(i, j)$는 세 가지 요소를 포함한다:
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: dummy 선행사의 경우에, score $s(i, \epsilon )$은 0으로 고정된다. 이 방법은 nondummy score가 양수면 모델이 가장 높은 점수의 선행사를 예측하지만 만약 모든 점수가 다 음수라면 제외하는 방법이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: task는 각 스팬 $i$에 선행사 $y_i$를 할당하고, 이전 스팬 그리고 special dummy token $\epsilon$에 랜덤 변수, 즉 확률 변수를 할당하는 것이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 여기 식을 보면 exponential score $i$, $y_i$의 summation 분의 exponential score $i$, $y_i$해서 선행사에 대한 확률값 $P(y_i)$가 나오는 것을 볼 수 있다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 만약 dummy token의 확률이 제일 높게 나온다면 $i$가 discourse-new이고 새로운 coreference chain을 시작하거나 nonanaphoric이기 때문에 선행사를 갖지 않는다는 것을 의미한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: \begin{matrix}
    \sum_{\hat{y}\in Y(i)\cap GOLD(i)}P(\hat{y})
    \end{matrix}
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 학습 시에 legal antecedent 의 coreference 확률의합을 최대화하는 손실함수를 사용한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 가능한 선행사 $Y(i)$를 가진 특정 mention $i$에 대해, $GOLD(i)$는 $i$를 포함하는 gold cluster의 mention 집합이라고 가정한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: $i$이전에 발생한 mention 집합은 $Y(i)$이기 때문에 $i$이전에 발생하는 gold cluster의 mention의 집합은 $Y(i)$와 $GOLD(i)$의 교집합이다. 따라서 이 확률을 최대화하는 방향으로 학습되어야 한다. mention $i$가 gold cluster에 없을 경우 $GOLD(i) = \epsilon$이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: \begin{matrix}
    L=\sum_{i-2}^{N} - log \sum_{\hat{y} \in Y(i) \cap GOLD(i)}P(\hat{y})
    \end{matrix}
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 방금 언급한 그 확률을 손실함수로 바꾸기 위해 위 수식처럼 확률에 $-log$를 취하고 그 값을 모두 더해서 corss-entropy loss function을 사용한다. 이 손실함수 값을 최소화 하는 방향으로 학습이 진행되어야 한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 손실함수: 모델의 출력값과 사용자가 원하는 출력값의 차이, 즉 오차를 말한다. 이 손실함수 값이 최소화 되도록 하는 가중치와 편향을 찾는 것이 학습이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 우리는 시스템이 생산한 일련의 체인이나 클러스터를 human labeling이나 gold나 reference chain 혹은 클러스터 집합과 비교하고, precisiton과 recall을 보고하며 이론적으로 evaluate coreference algorithm을 평가한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 이 비교를 수행하기 위한 다양한 방법이 있다. coreference algorithm을 평가하는데 사용되는 5가지 일반적인 metric(측정방법)은 다음과 같다:
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 1) MUC metric(link based)
    2) BLANC metric(link based)
    3) $B^3$ metric (mention based)
    4) CEAF metric (entity based)
    5) LEA metric (link based entity aware)
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: coreference와 밀접한 관련이 있는 entity linking task는 텍스트의 mention을 world에 있는 entity의 목록인 ontology에서 어떤 real-world entity의 representation과 연관시키는 것이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 이 task에 사용되는 가장 일반적인 온톨로지는 위키피디아이다. 위키피디아의 각 페이지는 특정 entity에 대한 고유한 id역할을 한다. 따라서 wikification의 entity linking task는 어떤 한 individual에 해당하는 Wikipedia page가 mention에 의해 지시되는지를 결정하는 작업이다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: entity linking은 두 단계로 이루어진다: mention detection과 mention disambiguation.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: Coreference가 올바른 위키피디아 페이지에 연결하기 위해 더 많은 간으한 surface form들을 제공하여 entity linking에 도움을 주기도 하지만 entity linking이 coreference resolution을 개선하기 위해 다른 방향으로도 사용될 수 있다. entity linking을 coreference에 통합하면 백과사전적 지식(Donald Tsang이 대통령이라는 사실처럼)을 이끌어내서 President에 대한 언급을 명확하게 하는 데에 도움이 될 수 있다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: Winograd는 위 예시를 제시하며 coreference의 일부 사례가 상당히 어려운 것으로 나타나 세계 지식이나 복잡한 추론을 요구하는 것으로 보인다고 지적했다. Winogard는 대부분의 독자들이 대명사 뒤에 이어지는 것에 대해서 선호하는 선행사는 (a)에서는 the city council이지만 (b)에서는 the demonstrators라는 것을 알아챘다. 그는 이것이 두 번째 절은 첫 번째 절의 설명으로 의도된 것임을 이해해야 하고, 또한 우리의 문화 프레임은 시의회가 아마도 시위대보다 폭력을 두려워할 가능성이 있고, 시위대는 폭력을 옹호할 가능성이 더 높다는 것을 시사한다. 이와 관련된 challenge task도 있고 Winograd와 coreference resolution problem이 있는 dataset들도 있다고 한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: Language processing의 다른 측면과 마찬가지로 coreference model도 성별과 기타 다른 편향들을 나타낸다. embedding은 그들의 training test에서 사회적 편향을 복제한다. 남성들은 의사와 같이 역사적으로 전형적인 남성 직업과 연관시키고, 여성은 비서와 같이 전형적인 여성 직업과 연관시킨다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 전형적인 남성직업 여성직업…?
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: WinoBias dataset은 Winograd Schema 패러다임의 변형을 사용하며 coreference algorithn이 문화적 고정 관념과 일치하는 선행사와 성별 대명사를 연결하는 방향으로 편향된 정도를 테스트하는데, 이 데이터셋은 전형적인 남성 그리고 전형적인 여성 직업에 해당하는 두 mention과 그 중 하나에 연결되어야 하는 성별 대명사를 포함한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: 위 예시에서 22.66은 pro-stereotypical이고 22.67은 anti-stereotypical하다. 이런 편향의 원인 중 하나는 데이터셋에 female entity가 매우 적다는 것인데, 이것을 해소하기 위해 남성과 여성 entity를 바꿔서 데이터셋을 바꾸고 기존 데이터셋과 합쳐서 비율을 맞추는 등의 시도가 있었다고 한다.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: Daniel Jurafsky and James H. Martin. 2019. Speech and Language Processing, 3rd Edition.
    제목: Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)
     포스트: https://web.stanford.edu/~jurafsky/slp3/
    제목: ㅡ
     포스트: 카테고리정렬
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: 많은 논문들에서 BERT가 어떻게 언어를 이해하는지에 대해 다루는데 본 논문에서는 BERT가 통사적 지식을 어떻게 가지는지를 평가해보고자 한다. BERT의 통사적 지식은 한가지 방법으로만 평가될 수는 없다. 따라서 크게 두 가지 방법을 통해 실험을 진행한다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: 1) negative polarity item(NPI) : NPI는 부정적인 언어환경에서 나오는 단어이다.
    2) five approaches:
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: language representation모델이 문법적 지식을 평가하지 못하고 있다. 최근 연구들에서도 probing task, Minimal pair, Boolean acceptability judgment 등으로 평가를 시도하고 있지만 모델들의 직접적인 비교는 하지 목하고 있다. 따라서 본 논문에서는 NPI를 통해 BERT와 같은 language representation모델을 평가해보고자 한다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: Boolean classification task, Minimal pair, probing task는 BERT의 encoder 부분을 평가하는 방법들이다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: NPI는 any처럼 부정적인 문장에서 받아들일 수 있는 단어이다. NPI는 licensor라는 환경이 구축되어야 사용될 수 있다. licensor는 일정의 언어적 환경으로, NPI를 쓰는 부정적인 환경이다. 그리고 Scope는 licensor가 미치는 영향의 범위이다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: 예문 (1)에서 hasn’t라는 licensor가 존재하기 때문에 부정적인 단어 any가 NPI로 나올 수 있다. 그리고 이러한 licensor의 영향 하에 NPI가 나왔기 때문에 scope가 맞게 되는 것이다. 예문 (2)에는 licensor가 없기 때문에 any라는 NPI가 나왔지만 이는 허용되지 않는 쓰임으로, 비문이 된다. 예문 (3)에서는 licensor가 있지만 any라는 NLP와 scope가 맞지 않아 비문이 된다. 즉, 부정적인 환경에서 NPI가 나와야 하는데 환경 영향 범위가 시작되기 이전에 나왔기 때문에 비문이 되는 것이다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: 예문 (4)는 DE(down entailing)이라는 환경과 관련된 예시이다. (4)-a에서 I haven’t been to France와 I have been to Paris에서 파리는 프랑스에 속해 있기 때문에 후자가 DE환경이다. 이처럼 어떠한 범위 안에 포함된 관계일 경우, 이를 DE환경이라고 부른다. 하지만 (4)-b에서는 뒷 문장에 부정적이 표현이 들어가지 않아 DE문장이 되지 않는다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: DE가 부정적인 것과 어떤 관련이 있나?
    최근 semantic에서 단조성(monotonicity: 한쪽 방향으로 쭉 진행되는 것)이 많이 연구되고 있다(downstream이랑 비슷한 개념이라고 생각하면 된다). 물 흘러가 듯이 cascade처럼 위에 나온 것이 참이라면 그 다음과 그 다음 다음 것도 참인 것이 되는, 단조적인 방법으로 진행되는 것을 말한다. 위 예시에서도 마찬가지로 프랑스를 간 적이 없는 것이 참이라면 파리를 간 적이 없는 것도 참인 것이다. 하지만 프랑스에 간 적이 있다해도 파리에 가지 않았을 수 있다. 따라서 a는 monotonic구조이고 b는 non-monotonic구조이다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: NPI는 다음과 같은 문맥에서 허용된다:
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: NPI는 자연언어에서 개념적으로 정의할 수는 있지만 나타나는 수많은 구성양상을 모두 설명하는 것은 불가능하다. 따라서 NPI는 언어학에서 매우 tricky한 주제이기 때문에 본 논문의 주제로 선정되었을 것이다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: CoLA는 데이터셋이다. 본 논문은 CoLA를 기반으로 생성한 데이터를 직접 구현하여 사용한다. CoLA는 1만개 이상의 example sentence를 지니는 데이터셋이며, supervised acceptability classifier를 수행하기 위해 사용된다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: 한국어 NPI는 영어보다 복잡하다. 왜냐하면 한국어의 NPI가 영어보다 많기 때문이다. 영어에서는 NPI가 명확한데 한국어에서는 이게 NPI인가 싶은 것들도 NPI인 경우가 많다. 그리고 한국어 NPI는 문맥상의 의미에 따라 여러 제약이 존재한다. 한국어 NPI를 연구하여 딥러닝 모델에 적용시킬 수 있다면 좋을텐데…
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: 본 연구는 다섯 가지 방식(Boolean acceptability, Minimal pair(Absolut, Gradient), Cloze Test, Feature probing을 통해 BERT가 NPI라는 환경에서 문법적인 지식을 이해할 수 있는지 평가한다.
    앞서 본 연구에서 사용된 데이터는 CoLA를 기반으로 생성하였다는 점을 언급한 바 있다. 데이터는 한 문장과 0과 1로 구성된 Boolean label로 구성되어 있으며, Boolean label에는 크게 세 가지의 meta-data(licensor, NPI, scope) 변수들이 존재한다. meta-data의 유무에 따라 0 혹은 1의 값이 주어진다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: 위 표에 나타난 Licensor, NPI, Scope가 meta-data 변수이다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: Licensor
    Table 2가 Questions environment에 대한 것이니까 Table 1에서 Question부분을 보면 whether가 licensor로 주어진 것을 확인할 수 있다. 다시 Table 2를 보면 licensor whether가 온 경우에는 1이 표시되어 있고 licensor가 존재하지 않을 경우에는 that이라는 licensor replacement가 오고 0으로 표시된다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: NPI
    Questions environment sample에서 NPI는 never이다. Table 2를 보면 NPI가 없을 경우에는 NPI replacement인 often이 오고 0으로 표시된 것을 확인할 수 있다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: Scope 
    Scope는 Licensor가 미치는 영향이다. Table 2를 보면 Licensor whether 뒤에 대괄호가 있는데 이것이 Licensor가 미치는 영향의 범위, 즉 Scope이다. 따라서 이 범위 안에 NPI가 존재할 경우에는 Scope가 1이되고 그렇지 않을 경우에는 0이 된다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: Boolean Acceptability는 언어 표준에 따라 문장이 만족스러운지를 평가하는 방식이다. 해당 모델이 특정 문장에 대해 acceptable하다 혹은 unacceptable하다를 잘 판단했는지 평가하는 것이다. Boolean Acceptability 판단을 위해 fine-tuning을 진행하게 되는데, BERT같은 경우에는 마지막 layer의 [CLS] embedding 상단에 classifier를 추가시킨다. Glove Bow의 경우에는 MLP classifier에 max pooling layer를 추가하여 classifier를 생성했다. 모델의 성능은 예측된 label과 정답 값의 label을 비교하는 MCC로 측정되었다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: Minimal Pair(최소대립쌍)은 두 개의 문장이 있을 때 두 문장에서 딱 하나의 token만 다르게 하여 해당 쌍을 비교할 수 있도록 하는 것이다. 여기에서는 Absolute와 Gradient Minimal pair에 대해 실험이 진행된다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: 본 실험에서는 두 문장이 paradigm(세 가지 meta-data 변수의 상황) 환경 내에서 NPI와 관련된 Boolean meta-data가 하나만 다를 경우에 최소 대립쌍을 형성하게 되는데, 이때 모델이 한 가지만 다른 것을 제대로 분류할 수 있는지를 확인한다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: 위 예시에서 (1)은 NPI licensor가 존재하고, (2)는 존재하지 않는다. 여기에서 not이라는 것이 NPI의 Licensor가 된다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: Gradient Minimal Pair는 Absolute Minimal Pair보다 rough한 버전의 방식이다. Absolute Minimal Pair는 NPI의 위치를 통해 모델이 정확한 분류를 해내야 하지만 Gradient Minimal Pair에서는 해당 문장이 acceptable할 확률이 unacceptable할 확률보다 높으면 올바르다고 간주한다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: Cloze Test는 어떤 문장이 있을 때 중간에 한 단어를 비워 놓고 어떤 단어가 오는지 예측하는 것이다. 하지만 본 실험에서는 BERT의 Masked token에 해당하는 단어를 예측하는 방식이 아닌 해당 문장에서 masking된 부분의 위치가 어디인지 알아낼 수 있는지를 실험하였다.
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: Feature Probing은 meta-data를 더 세분화한 방식이다. fine-tuning 유무와 관계없이 문장의 encoder부분을 freezing시키고 그 위에 lightweight classifier를 학습시켰다. lightweight classifier란 meta-data label을 예측하기 위해
    제목: Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))
     포스트: Warstadt et al.”Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs,”Association for Computational Linguistics.2019
    


```python
'''
[3]
[2]의 문제를 해결하기 위해 제목: + 포스트: 형태로 출력시켜주는 Posts 클래스를 빼본다
'''

i = 0

for sub in result:
    i = i + 1
    print("(", i, "/", len(result),")")
    req2 = requests.get('https://finddme.github.io' + sub)
    html2 = req2.text
    soup2 = BeautifulSoup(html2, 'html.parser')
    tncs = soup2.findAll('div',{'class': "post"})
    for tnc in tncs:
        t = tnc.find('h1')
        t2txt = t.text
        print("제목: ",t2txt, "\n")
        c = tnc.findAll('p')
        for p in c:
            c2txt = p.text
            print(c2txt)
            
    if i == 3:
        break;
```

    ( 1 / 74 )
    제목:  Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019) 
    
    coreference에 대한 세 가지 일반적인 아키텍쳐는 mention-pair, mention-rank, 그리고 entity-based로 각각 feature-based 혹은 neural classifier를 사용할 수 있다. 이번 장에서는 우선 feature-based 알고리즘에 대해서 설명하겠다.
    따라서 이번 장에서는 categorization을 사용한 시스템의 다양한 architecture를 개괄적으로 살펴본다. 알고리즘은 크게 entity-based(담화모델에서 각 개체를 나타내는 방식으로 coreference를 결정)인지, mention-based(각 mention을 독립적으로 고려하는 방식으로 coreference를 결정)인지 혹은 잠재적 선행사를 직접적으로 비교하기 위해서 ranking model을 사용하는지의 여부에 따라 구분된다.
    먼저 mention-based algorithm인 mention-pair architecture에 대해서 살펴보겠다. mention-pair architecture는 이름에서도 알 수 있듯이 한 쌍의 mention, candidate anaphor그리고 candidate antecedent(후보선행사)가 주어지는 classifier를 기반으로 하며, binary classification(이항분류; corefering하는지 안 하는지)를 통해 결론을 도출한다.
    
    위 예문에 이 classifier의 task를 대입해보면 그림 22.2와 같이 된다. 이것은 예문에 있는 대명사 she와 그에 대한 잠재적 선행사 mention이 이루는 쌍에 대해 coreference link의 확률을 할당하는 것을 보여주는 그림이다.
    여기에서 각각의 Victoria Chen, Megabucks Banking, her과 같은 prior mention에 대해 binary classifier는 she의 선행사가 그 mention인지 아닌지 확률을 계산한다. 여기에서 우리는 이 확률을 실제 선행사(Victoria Chen, her, the 38-year-old)에 대해서는 높이고 선행사가 아닌 것(Megabucks Banking, her pay)에 대해서는 낮춰야 한다.
    훈련을 위해서는 training samples를 선택해야 하는데 이때 문서의 대부분 mention쌍은 corefernent가 아닌데 이것을 다 훈련 샘플로 선택하면 너무 많은 negative sample이 생기기 때문에 heuristic method가 필요하다. 가장 일반적인 heuristic은 positive example로 가장 가까운 선행사를 택하고, 그 사이에 있는 모든 쌍들은 negative example로 선택하는 것이다. 이것을 조금 formal하게 표현하자면, anaphor mention $m_i$에 대해서
    Classifier가 훈련되고 나면 clustering단계에서 test sentence에 적용된다. clustering에는 두 가지 과정이 있다:  closest-first clustering과 best-first clustering.
    - closest-first clustering
    closest-first clustering에서 classifier는 오른쪽에서 왼쪽으로, 즉 mention $i-1$에서 mention 1까지로 실행된다. 그리고 확률 0.5가 넘는 first antecedent는 $i$와 연결된다. 선행사가 확률 0.5를 넘지 못하는 경우에는 $i$에 대한 선행사가 선택되지 않는다.
    - best-first clustering
    best-first clustering에서 classifier는 모든 선행사들에 대해서 실행되는데, 가장 유력한 antecedent mention이 $i$의 선행사로 선택된다.
    mention-pair model은 단순하다는 장점이 있지만 두 가지 주요한 문제가 있다.
    1) classifier가 직접적으로 후보 선행사들을 비교하지 못한다. 따라서 두 선행사 사이에서 실제로 더 나은 것이 무엇인지 결정하는 훈련이 이루어지지 못한다.
    2) 담화 모델을 무시하며, entity가 아닌 단지 mention만을 살핀다. 각 classifier decistion은 동일한 entity의 다른 mention들을 고려하지 않고 한 쌍에 대해서만 완전히 국부적으로 이루어진다.
    Mention-Rank Architecture는 mention-pair model의 결점을 보완한 model이다. mention-pair model의 단점 중 하나는 선행사 후보를 직접적으로 비교하지 않는다는 것이다. mention-ranking model은 이를 보완하여 선행사 후보를 직접적으로 비교하며 각 anaphor에 대해서 높은 점수의 선행사를 선택한다.
    mention-ranking systems에서는 mention(anaphor)에 $i-1$에서 1까지, 그리고 여기에 추가적으로 선행사가 없다는 것까지 고려하기 위한 dummy mention $\epsilon$까지의 범위에 걸쳐 관련된 랜덤 변수 $y_1$을 가진다. dummy mention $\epsilon$은 discourse-new나 새로운 coref chain의 시작 혹은 non-anaphoric을 나타낸다.
    그림 22.3은 single candidate anaphor she에 대한 계산 예를 보여준다. mention-ranking system은 후보 anaphoric mention she에 대해 이전의 모든 mention들, 그리고 추가적으로 특수한 dummy mention $\epsilon$에 대해 소프트맥스를 계산하여 확률분포를 할당한다(각 후보 선행사들에 대한 확률을 구한다).
    이전의 mention-pair model의 훈련은 단순했는데, mention-ranking model의 훈련은 비교적 까다롭다. 왜냐하면 mention-pair model에서는 positive-sample / negative-sample을 만들어서 훈련에 사용되는 sample pair를 다 알았는데, mention-ranking model은 각 anaphor에 대해서 훈련을 위해 사용할 수 있는 모든 gold antecedent들을 알 수 없기 때문이다.
    대신 각 mention에 대한 best antecedent는 잠재되어 있다; 즉 각 mention에 대해서 선택할 수 있는 legal gold antecednet의 전체 클러스터를 가지고 있다.
    이런 잠재된 선행사를 모델링하는 다양한 방법들이 존재하는데, 이는 3장에서 상세히 다룰 것이다.
    Mention-ranking model 은 hand-build feature와 neural representation learning 둘 다를 통해 구현될 수 있느데, 전자는 2장에서, 후자는 3장에서 자세히 다룰 것이다.
    앞서 소개한 mention-pair model과 mention-ranking model 둘 다 mention-based algorithm이었고, 이번에 소개할 모델은 entity-based model이다.
    - entity-based model
    entity-based model은 각 mention을 이전 mention이 아닌 이전 담화 개체(mentions들의 cluster)에 연결한다.
    - entity-ranking model
    entity-ranking model은 단순히 mention-ranking model에서 classifier가 개별 mention들이 아니라 mention들의 cluster들에 대해 결정을 내리게 한 것이다.
    entity-based model도 mention-ranking model처럼 feature-based model과 neural model 둘 다를 통해 구현될 수 있는데, 이전의 모델들보다 표현력이 더 뛰어나지만 cluster-level 정보를 이용하는 것이 실제로 큰 성능 향상으로 이어지지 않았기 때문에 mention-ranking model이 여전히 더 많이 사용된다. 그래서 entity-based model은 간단하게만 소개하고 넘어가도록 하겠다.
    feature-based model의 경우 클러스터에서 feature를 추출하여 이를 수행할 수 있다.
    Neural model은 cluster의 representation(cluster를 vector로 표현하는 것)을 자동으로 학습할 수 있다. 예를 들어 cluster representation에 해당하는 상태를 인코딩하기 위해 일련의 cluster mention에 대해 RNN을 이용하거나, mention pair의 학습된 representation에 대해 pooling하여 cluster쌍에 대한 distiributed(dense) representation을 학습한다.
    ‘Distiributed’라는 말이 붙는 이유는 하나의 정보가 여러 차원에 분산되어 표현되기 때문이다. 하나의 차원이 여러 속성들이 버무려진 정보를 들고 있다. 즉, 하나의 차원이 하나의 속성을 명시적으로 표현하는 것이 아니라 여러 차원들이 조합되어 나타내고자 하는 속성들을 표현하는 것이다.
    이번 장에서는 coreference resolution을 위해 logistic regression, SVM 또는 random forest classifier에서 일반적으로 사용되는 feature들에 대해 기술한다.
    anaphor mention과 potential antecedent mention이 있을 때 대부분의 feature based classifier는 세 가지 유형의 feature를 사용한다:
    1) anaphor에 대한 feature,
    2) 선행사 후보에 대한 feature,
    3) 쌍 사이의 관계에 대한 feature.
    Entity-based model은 추가적으로 두 개의 클레스를 더 사용할 수 있다:
    4) 선행사의 개체 클러스터로부터 온 모든 mention들에 대한 feature,
    5) 선행사 entity cluster의 mention과 anaphor간의 관계에 대한 feature.
    그림 22.4는 일반적으로 사용되는 feature들을 보여준다. 앞서 사용한 예문에서 잠재적 anaphor she와 잠재적 선행사 Victoria Chen에 대한 것이다. 노란색 박스로 표시된 것은 이전 연구에서 특히 유용하다고 밝혀진 feature이고 파란색 박스로 표시된 것은 neural model에 사용했을 때 도움이 될 수 있는 feature이다.
    
    위는 Aanphor나 선행 mention에 대한 feature들이다.
    위는 선행사 entity에 대한 feature(여기에서는 she에 대한 선행사니까 Victoria Chen, her, the 38-year-old에 대한 것; 그래서 Antecedent cluster size가 3)이다.
    위는 mention쌍에 대한 feature이다. 
    Longer anaphor는 anaphor가 선행사보다 기냐
    Paris of any features는 선행사 + anaphor 유형쌍에 대한 각 feature. 어떤 feature든 선행사 + anaphor쌍으로.
    Sentence distance는 선행사와 anaphor 사이에 있는 문장 수
    Mention distance는 선행사와 anaphor 사이에 있는 mention 수
    i-within-i는 어떠한 mention이 다른 mention에 포함되어 있으며 두 mention의 reference가 같은 것.
    Appositive 동격(앞의 내용을 명사구나 다른 명사를 사용해 정의하거나 설명하는 것). anaphor가 선행사와 동격관계에 있는 경우. 이것은 mention들이 동격어가 아닌 경우에도 유용하다. 동격어를 선행 헤드와 연결하는 것을 알기 위해.
    위는 entity쌍에 대한 feature이다.
    Exact String Match는 determiner와 modifier를 모두 포함하여 정확히 동일한 텍스트를 포함하는 경우에만 두 mention을 연관시키는 것이다. 즉 선행사와 anaphor cluster에 있는 아무 두 mention의 string(문자열)이 같으냐
    Head Word Match는 선행사 cluster에 있는 mention과 anaphor cluster의 mention이 같은 headword를 가지냐
    Word Inclusion는 선행사 cluster에 있는 단어가 anaphor cluster에 있는 모든 단어를 포함하냐
    그 아래에 문서에 대한 feature는 장르가 뭔지에 대한 것이다.
    feature-based system에서 feature들의 결합을 사용하는 것은 중요하다고 한다. 한 실험에서는 classifier의 개별 feature들을 사용한 것과 다중 feature의 결합을 사용했을 때를 비교해 봤을 때 결합한 것의 F1 score가 4점 높았다고 한다.
    초기 classifier들은 2장에서 소개한 hand-built feature를 사용했고, 보다 최근의 classifier들은 지금 소개할 neural representation learning을 사용한다.
    이번 장에서는 Lee et al. (2017b)의 neural mention-ranking system에 대해 기술하겠다. 이 end-to-end system은 별도의 mention-detection단계를 가지고 있지 않다. 대신 가능한 모든 텍스트 범위를 설정된 length(예를 들어 길이 1, 2, 3, … N의 모든 n-gram)까지를 가능한 mention으로 간주한다(Lee의 논문에서는 length를 10으로 설정했다).
    그림 22.5는 span representation과 mention score에 대한 계산을 보여준다.
    그림 22.6은 그림 22.5의 예시 문장에서 the company의 가능한 세 가지 선행사에 대한 score $s$를 계산하는 것을 보여준다.
    위 부분을 살펴보기 위해 나중에 계산될 span representations $g_i$에 대해서 잠시 얘기하도록 하겠다. span representations $g_i$는 스팬에서의 첫 번째 단어와 마지막 단어의 contextual representation 그리고 스팬에 있는 headword에 대한 representation 그리고 feature 하나로 구성된다.
    여기에서 스팬의 첫 번째 단어와 마지막 단어의 contextual representation이 standard biLSTM으로 계산된다.
    그리고 biLSTM은 ELMo와 같은 contextual word embedding을 기반으로 해서 각 단어에 대한 representation $w_t$를 input으로 받는다 (ELMo대신 BERT를 사용하면 성능이 훨씬 향상된다).
    그러니까 biLSTM은 $w_t$를 input으로 받아서 output으로 $h_t$를 내는 것이다.
    위 부분은 스팬의 head를 나타내기 위한 부분이다. 시스템은 스팬의 head를 나타내기 위해 스팬의 단어들에 대해 attention을 사용하였다.
    span representation $g_i$는 이전에 말한 것처럼 스팬의 시작과 끝의 hidden representation(hidden state 값), head 그리고 스팬의 $i$의 length같은 feature vector(이전 장에서 neural model에서 유용할 수 있는 feature 중 하나)를 모두 concatenate한 것이다.
    위 그림은 the company의 가능한 세 가지 선행사에 대한 score $s$를 계싼하는 것을 보여주는 그림이다. antecedent score부분을 보면 the company에 대해서 General Electric이 선행사인 경우와 the Postal Service가 선행사인 경우 각각이 들어가는 것을 볼 수 있고, 이들의 요소별 곱도 들어가며, 추가적으로 다른 feature들도 들어간다.
    $m(i)$와 $c(i,j)$는 scoring 함수로, 각각 mention score, antecedent score를 나타내며 둘 다 스팬 $i$를 나타내는 백테 $g_i$를 기반으로 한다.
    FFNN(Feed Forward Neural Network). 가중치의 반복적인 업데이트. 
    1) 인풋 $x$를 받아서
    2) 이것의 $y=Wx+b$를 계산하고
    3) 여기에 activation function(sigmoid, tanh, ReLU, etc.)를 적용
    선행사 score $c(i, j)$는 input으로 스팬 $i$와 $j$의 representation을 취하며, $g_i \circ g_j$($g_i$와 $g_j$의 요소별 곱)은 두 스팬의 요소별 유사도이다. 그리고 마지막에는 mention distances 그리고 화자와 장르에 대한 정보와 같은 유용한 feature들을 인코딩한 feature vector $\varphi(i, j)$가 들어간다.
    이제 coreference score $s$를 보겠다. score $s(i, j)$는 세 가지 요소를 포함한다:
    dummy 선행사의 경우에, score $s(i, \epsilon )$은 0으로 고정된다. 이 방법은 nondummy score가 양수면 모델이 가장 높은 점수의 선행사를 예측하지만 만약 모든 점수가 다 음수라면 제외하는 방법이다.
    task는 각 스팬 $i$에 선행사 $y_i$를 할당하고, 이전 스팬 그리고 special dummy token $\epsilon$에 랜덤 변수, 즉 확률 변수를 할당하는 것이다.
    여기 식을 보면 exponential score $i$, $y_i$의 summation 분의 exponential score $i$, $y_i$해서 선행사에 대한 확률값 $P(y_i)$가 나오는 것을 볼 수 있다.
    만약 dummy token의 확률이 제일 높게 나온다면 $i$가 discourse-new이고 새로운 coreference chain을 시작하거나 nonanaphoric이기 때문에 선행사를 갖지 않는다는 것을 의미한다.
    \begin{matrix}
    \sum_{\hat{y}\in Y(i)\cap GOLD(i)}P(\hat{y})
    \end{matrix}
    학습 시에 legal antecedent 의 coreference 확률의합을 최대화하는 손실함수를 사용한다.
    가능한 선행사 $Y(i)$를 가진 특정 mention $i$에 대해, $GOLD(i)$는 $i$를 포함하는 gold cluster의 mention 집합이라고 가정한다.
    $i$이전에 발생한 mention 집합은 $Y(i)$이기 때문에 $i$이전에 발생하는 gold cluster의 mention의 집합은 $Y(i)$와 $GOLD(i)$의 교집합이다. 따라서 이 확률을 최대화하는 방향으로 학습되어야 한다. mention $i$가 gold cluster에 없을 경우 $GOLD(i) = \epsilon$이다.
    \begin{matrix}
    L=\sum_{i-2}^{N} - log \sum_{\hat{y} \in Y(i) \cap GOLD(i)}P(\hat{y})
    \end{matrix}
    방금 언급한 그 확률을 손실함수로 바꾸기 위해 위 수식처럼 확률에 $-log$를 취하고 그 값을 모두 더해서 corss-entropy loss function을 사용한다. 이 손실함수 값을 최소화 하는 방향으로 학습이 진행되어야 한다.
    손실함수: 모델의 출력값과 사용자가 원하는 출력값의 차이, 즉 오차를 말한다. 이 손실함수 값이 최소화 되도록 하는 가중치와 편향을 찾는 것이 학습이다.
    우리는 시스템이 생산한 일련의 체인이나 클러스터를 human labeling이나 gold나 reference chain 혹은 클러스터 집합과 비교하고, precisiton과 recall을 보고하며 이론적으로 evaluate coreference algorithm을 평가한다.
    이 비교를 수행하기 위한 다양한 방법이 있다. coreference algorithm을 평가하는데 사용되는 5가지 일반적인 metric(측정방법)은 다음과 같다:
    1) MUC metric(link based)
    2) BLANC metric(link based)
    3) $B^3$ metric (mention based)
    4) CEAF metric (entity based)
    5) LEA metric (link based entity aware)
    coreference와 밀접한 관련이 있는 entity linking task는 텍스트의 mention을 world에 있는 entity의 목록인 ontology에서 어떤 real-world entity의 representation과 연관시키는 것이다.
    이 task에 사용되는 가장 일반적인 온톨로지는 위키피디아이다. 위키피디아의 각 페이지는 특정 entity에 대한 고유한 id역할을 한다. 따라서 wikification의 entity linking task는 어떤 한 individual에 해당하는 Wikipedia page가 mention에 의해 지시되는지를 결정하는 작업이다.
    entity linking은 두 단계로 이루어진다: mention detection과 mention disambiguation.
    Coreference가 올바른 위키피디아 페이지에 연결하기 위해 더 많은 간으한 surface form들을 제공하여 entity linking에 도움을 주기도 하지만 entity linking이 coreference resolution을 개선하기 위해 다른 방향으로도 사용될 수 있다. entity linking을 coreference에 통합하면 백과사전적 지식(Donald Tsang이 대통령이라는 사실처럼)을 이끌어내서 President에 대한 언급을 명확하게 하는 데에 도움이 될 수 있다.
    Winograd는 위 예시를 제시하며 coreference의 일부 사례가 상당히 어려운 것으로 나타나 세계 지식이나 복잡한 추론을 요구하는 것으로 보인다고 지적했다. Winogard는 대부분의 독자들이 대명사 뒤에 이어지는 것에 대해서 선호하는 선행사는 (a)에서는 the city council이지만 (b)에서는 the demonstrators라는 것을 알아챘다. 그는 이것이 두 번째 절은 첫 번째 절의 설명으로 의도된 것임을 이해해야 하고, 또한 우리의 문화 프레임은 시의회가 아마도 시위대보다 폭력을 두려워할 가능성이 있고, 시위대는 폭력을 옹호할 가능성이 더 높다는 것을 시사한다. 이와 관련된 challenge task도 있고 Winograd와 coreference resolution problem이 있는 dataset들도 있다고 한다.
    Language processing의 다른 측면과 마찬가지로 coreference model도 성별과 기타 다른 편향들을 나타낸다. embedding은 그들의 training test에서 사회적 편향을 복제한다. 남성들은 의사와 같이 역사적으로 전형적인 남성 직업과 연관시키고, 여성은 비서와 같이 전형적인 여성 직업과 연관시킨다.
    전형적인 남성직업 여성직업…?
    WinoBias dataset은 Winograd Schema 패러다임의 변형을 사용하며 coreference algorithn이 문화적 고정 관념과 일치하는 선행사와 성별 대명사를 연결하는 방향으로 편향된 정도를 테스트하는데, 이 데이터셋은 전형적인 남성 그리고 전형적인 여성 직업에 해당하는 두 mention과 그 중 하나에 연결되어야 하는 성별 대명사를 포함한다.
    위 예시에서 22.66은 pro-stereotypical이고 22.67은 anti-stereotypical하다. 이런 편향의 원인 중 하나는 데이터셋에 female entity가 매우 적다는 것인데, 이것을 해소하기 위해 남성과 여성 entity를 바꿔서 데이터셋을 바꾸고 기존 데이터셋과 합쳐서 비율을 맞추는 등의 시도가 있었다고 한다.
    Daniel Jurafsky and James H. Martin. 2019. Speech and Language Processing, 3rd Edition.
    https://web.stanford.edu/~jurafsky/slp3/
    ( 2 / 74 )
    제목:  ㅡ 
    
    카테고리정렬
    ( 3 / 74 )
    제목:  Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019)) 
    
    많은 논문들에서 BERT가 어떻게 언어를 이해하는지에 대해 다루는데 본 논문에서는 BERT가 통사적 지식을 어떻게 가지는지를 평가해보고자 한다. BERT의 통사적 지식은 한가지 방법으로만 평가될 수는 없다. 따라서 크게 두 가지 방법을 통해 실험을 진행한다.
    1) negative polarity item(NPI) : NPI는 부정적인 언어환경에서 나오는 단어이다.
    2) five approaches:
    language representation모델이 문법적 지식을 평가하지 못하고 있다. 최근 연구들에서도 probing task, Minimal pair, Boolean acceptability judgment 등으로 평가를 시도하고 있지만 모델들의 직접적인 비교는 하지 목하고 있다. 따라서 본 논문에서는 NPI를 통해 BERT와 같은 language representation모델을 평가해보고자 한다.
    Boolean classification task, Minimal pair, probing task는 BERT의 encoder 부분을 평가하는 방법들이다.
    NPI는 any처럼 부정적인 문장에서 받아들일 수 있는 단어이다. NPI는 licensor라는 환경이 구축되어야 사용될 수 있다. licensor는 일정의 언어적 환경으로, NPI를 쓰는 부정적인 환경이다. 그리고 Scope는 licensor가 미치는 영향의 범위이다.
    예문 (1)에서 hasn’t라는 licensor가 존재하기 때문에 부정적인 단어 any가 NPI로 나올 수 있다. 그리고 이러한 licensor의 영향 하에 NPI가 나왔기 때문에 scope가 맞게 되는 것이다. 예문 (2)에는 licensor가 없기 때문에 any라는 NPI가 나왔지만 이는 허용되지 않는 쓰임으로, 비문이 된다. 예문 (3)에서는 licensor가 있지만 any라는 NLP와 scope가 맞지 않아 비문이 된다. 즉, 부정적인 환경에서 NPI가 나와야 하는데 환경 영향 범위가 시작되기 이전에 나왔기 때문에 비문이 되는 것이다.
    예문 (4)는 DE(down entailing)이라는 환경과 관련된 예시이다. (4)-a에서 I haven’t been to France와 I have been to Paris에서 파리는 프랑스에 속해 있기 때문에 후자가 DE환경이다. 이처럼 어떠한 범위 안에 포함된 관계일 경우, 이를 DE환경이라고 부른다. 하지만 (4)-b에서는 뒷 문장에 부정적이 표현이 들어가지 않아 DE문장이 되지 않는다.
    DE가 부정적인 것과 어떤 관련이 있나?
    최근 semantic에서 단조성(monotonicity: 한쪽 방향으로 쭉 진행되는 것)이 많이 연구되고 있다(downstream이랑 비슷한 개념이라고 생각하면 된다). 물 흘러가 듯이 cascade처럼 위에 나온 것이 참이라면 그 다음과 그 다음 다음 것도 참인 것이 되는, 단조적인 방법으로 진행되는 것을 말한다. 위 예시에서도 마찬가지로 프랑스를 간 적이 없는 것이 참이라면 파리를 간 적이 없는 것도 참인 것이다. 하지만 프랑스에 간 적이 있다해도 파리에 가지 않았을 수 있다. 따라서 a는 monotonic구조이고 b는 non-monotonic구조이다.
    NPI는 다음과 같은 문맥에서 허용된다:
    NPI는 자연언어에서 개념적으로 정의할 수는 있지만 나타나는 수많은 구성양상을 모두 설명하는 것은 불가능하다. 따라서 NPI는 언어학에서 매우 tricky한 주제이기 때문에 본 논문의 주제로 선정되었을 것이다.
    CoLA는 데이터셋이다. 본 논문은 CoLA를 기반으로 생성한 데이터를 직접 구현하여 사용한다. CoLA는 1만개 이상의 example sentence를 지니는 데이터셋이며, supervised acceptability classifier를 수행하기 위해 사용된다.
    한국어 NPI는 영어보다 복잡하다. 왜냐하면 한국어의 NPI가 영어보다 많기 때문이다. 영어에서는 NPI가 명확한데 한국어에서는 이게 NPI인가 싶은 것들도 NPI인 경우가 많다. 그리고 한국어 NPI는 문맥상의 의미에 따라 여러 제약이 존재한다. 한국어 NPI를 연구하여 딥러닝 모델에 적용시킬 수 있다면 좋을텐데…
    본 연구는 다섯 가지 방식(Boolean acceptability, Minimal pair(Absolut, Gradient), Cloze Test, Feature probing을 통해 BERT가 NPI라는 환경에서 문법적인 지식을 이해할 수 있는지 평가한다.
    앞서 본 연구에서 사용된 데이터는 CoLA를 기반으로 생성하였다는 점을 언급한 바 있다. 데이터는 한 문장과 0과 1로 구성된 Boolean label로 구성되어 있으며, Boolean label에는 크게 세 가지의 meta-data(licensor, NPI, scope) 변수들이 존재한다. meta-data의 유무에 따라 0 혹은 1의 값이 주어진다.
    위 표에 나타난 Licensor, NPI, Scope가 meta-data 변수이다.
    Licensor
    Table 2가 Questions environment에 대한 것이니까 Table 1에서 Question부분을 보면 whether가 licensor로 주어진 것을 확인할 수 있다. 다시 Table 2를 보면 licensor whether가 온 경우에는 1이 표시되어 있고 licensor가 존재하지 않을 경우에는 that이라는 licensor replacement가 오고 0으로 표시된다.
    NPI
    Questions environment sample에서 NPI는 never이다. Table 2를 보면 NPI가 없을 경우에는 NPI replacement인 often이 오고 0으로 표시된 것을 확인할 수 있다.
    Scope 
    Scope는 Licensor가 미치는 영향이다. Table 2를 보면 Licensor whether 뒤에 대괄호가 있는데 이것이 Licensor가 미치는 영향의 범위, 즉 Scope이다. 따라서 이 범위 안에 NPI가 존재할 경우에는 Scope가 1이되고 그렇지 않을 경우에는 0이 된다.
    Boolean Acceptability는 언어 표준에 따라 문장이 만족스러운지를 평가하는 방식이다. 해당 모델이 특정 문장에 대해 acceptable하다 혹은 unacceptable하다를 잘 판단했는지 평가하는 것이다. Boolean Acceptability 판단을 위해 fine-tuning을 진행하게 되는데, BERT같은 경우에는 마지막 layer의 [CLS] embedding 상단에 classifier를 추가시킨다. Glove Bow의 경우에는 MLP classifier에 max pooling layer를 추가하여 classifier를 생성했다. 모델의 성능은 예측된 label과 정답 값의 label을 비교하는 MCC로 측정되었다.
    Minimal Pair(최소대립쌍)은 두 개의 문장이 있을 때 두 문장에서 딱 하나의 token만 다르게 하여 해당 쌍을 비교할 수 있도록 하는 것이다. 여기에서는 Absolute와 Gradient Minimal pair에 대해 실험이 진행된다.
    본 실험에서는 두 문장이 paradigm(세 가지 meta-data 변수의 상황) 환경 내에서 NPI와 관련된 Boolean meta-data가 하나만 다를 경우에 최소 대립쌍을 형성하게 되는데, 이때 모델이 한 가지만 다른 것을 제대로 분류할 수 있는지를 확인한다.
    위 예시에서 (1)은 NPI licensor가 존재하고, (2)는 존재하지 않는다. 여기에서 not이라는 것이 NPI의 Licensor가 된다.
    Gradient Minimal Pair는 Absolute Minimal Pair보다 rough한 버전의 방식이다. Absolute Minimal Pair는 NPI의 위치를 통해 모델이 정확한 분류를 해내야 하지만 Gradient Minimal Pair에서는 해당 문장이 acceptable할 확률이 unacceptable할 확률보다 높으면 올바르다고 간주한다.
    Cloze Test는 어떤 문장이 있을 때 중간에 한 단어를 비워 놓고 어떤 단어가 오는지 예측하는 것이다. 하지만 본 실험에서는 BERT의 Masked token에 해당하는 단어를 예측하는 방식이 아닌 해당 문장에서 masking된 부분의 위치가 어디인지 알아낼 수 있는지를 실험하였다.
    Feature Probing은 meta-data를 더 세분화한 방식이다. fine-tuning 유무와 관계없이 문장의 encoder부분을 freezing시키고 그 위에 lightweight classifier를 학습시켰다. lightweight classifier란 meta-data label을 예측하기 위해
    Warstadt et al.”Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs,”Association for Computational Linguistics.2019
    



