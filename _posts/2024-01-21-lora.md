---
title: "Parameter-Efficient Finetuning (PEFT) methods : LoRA, QLoRA, DoRA"
category: LLM / Multimodal
tag: NLP
---







* 목차
{:toc}








최근 Large Language Model이 많이 나오며 대규모 모델을 효율적으로 학습시킬 수 있는 방법에 대한 연구가 활발히 진행되고 있다. 해당 연구는 주로 LLM tuning 이후의 성능을 더 좋게 하거나 모델을 보다 빠르게 훈련시킬 수 있도록 하는 방향으로 진행되고 있다.  이러한 연구의 대표로는 Low-Rank Adaptation (LoRA)가 있는데 LoRA란, parameter 수를 줄여 LLM을 빠르고 효과적으로 학습시킬 수 있도록 한다. 최근 이와 관련된 연구가 많이 진행되는 만큼 LoRA의 변형들도 많이 나온 상황이다. 

> LLM tuning으로 사용되는 일반적인 방법은 LLM의 일부 layer를 원하는 task에 맞게 학습시키는 것이다.

# LoRA

<center><img width="400" src="https://github.com/finddme/finddme.github.io/assets/53667002/87e0634d-0680-48b7-a751-84cd8c098886"></center>
<center><em style="color:gray;">Low-Rank Adaption (LoRA)</em></center><br>

위 이미지처럼 LoRA는 pre-trained LLM layer의 parameter weight matrix $W$를 frozen 시키고 $W$ 외에 "adapter"라 불리는 두 개의 행렬 A와 B를 추가한다. 추가된 두 행렬은 $W$보다 작다. 예를 들어 $W$의 크기가 $d x d$일 때 A와 B의 크기는 $d x r$과 $r x d$이며, $r$($rank$)은 일반적으로 100 이하의 매우 작은 크기이다. $r$의 크기가 클 수록 더 많은 parameter를 학습시키게 된다. 더 많은 parameter를 학습시킨 다는 것은 더 좋은 성능으로 이어질 가능성이 있지만 학습 시간이 길어진다는 단점이 있다. 

학습이 진행 될 때 frozen된 $W$와 B\*A에 동일한 값을 입력한 후 B\*A의 출력을 original matrix $W$의 출력에 추가한다. 즉, 일부 parameter를 학습시키고 그 출력을 original prediction에 더하여 모델에 영향을 주는 방식으로 작동된다. 처음에 행렬 A는 평균이 0인 랜덤 값들로 구성된다(Random values of mean zero). 즉, 랜덤 변수의 값들이 평균적으로 0을 중심으로 분포된다. 그리고 B는 완전히 0으로 초기화된다. 이와 같은 처리를 통해 adpater 행렬이 original matrix $W$의 출력을 완전 초기부터 변경시키지 않도록 한다. A와 B의 parameter가 적절한 방향으로 tuning될 때 A와 B의 출력이 $W$의 출력에 영향을 미치도록 조정하기 위함이다. 

LoRA는 끝쪽에서만 이루어져야 하는 것은 아니고 neural network 내부의 깊은 layer에도 적용 가능하다.

# QLoRA

<center><img width="600" src="https://github.com/finddme/finddme.github.io/assets/53667002/f3978e20-6572-41c7-aa3d-471c9f07d4bb"></center>
<center><em style="color:gray;">Low-Rank Adaption (LoRA)</em></center><br>

QLoRA는 4-bit Quantized language model into Low Rank Adapters의 줄임말로, 양자화를 거친 모델에 대해 LoRA를 활용해 tuning하는 기법을 뜻한다. 계산량 및 메모리 감축을 위해 QLoRA의 저자들은 크게 세 가지 기법을 사용했다.

- 4-bit NormalFloat
  
Pretrained Model의 weight를 4-bit로 양자화시켰을 때 그 데이터 타입을 NormalFloat이라고 한다.

> 일반적으로 컴퓨터는 실수를 32-bit로 floating(fp32)하고 모델들은 보통 FP16으로 경량화되어 사용된다.

- Double Quantized

이 기법은 이중 양자화로, 양자화된 것을 또 양자화하는 것이다. 앞서 4-bit NormalFloat으로 양자화 된 weight를 또 양자화하는 것이 아니라 weight를 양자화하면서 발생한 quantization constant(양자화 상수)를 양자화하여 조금 더 가볍게 만드는 것이다.

- Paged Optimizer

이 기법은 모델 자체에 대한 경량화는 아니지만 제한된 컴퓨터 자원에서 모델을 사용할 때 GPU가 사용하는 VRAM page를 CPU의 RAM에도 일부 저장할 수 있게 해서 메모리를 최대한 사용핟로록 하도록 하는 방법이다. 이 기법의 적용으로 동일한 크기의 모델에 대해 OOM 발생을 피할 수 있는  같다.

> process를 memory에 올릴 때 process를 page 단위로 나누어 처리함.


# DoRA

<center><img width="400" src="https://github.com/finddme/finddme.github.io/assets/53667002/1269df50-6c25-4d26-a605-de98c19ef4b9"></center>
<center><em style="color:gray;">Low-Rank Adaption (LoRA)</em></center><br>

DoRA(Weight-Decomposed Low-Rank Adaption)는 LoRA의 변형 중 하나로, 행렬이 행렬의 길이/크기(magnitude)와 방향(direction)의 곱으로 분해(decompose)될 수 있다는 개념에서 출발한 방법론으로, weight matrix $W$는 matrix 크기 m과 방향 v로 분해되어 각각 독립적으로 tuning한다. 즉, pretrained matrix $W$를 magnitude vector m (size $1xd$)와 direction matrix V로 분리하여 magnitude와 direction을 독립적으로 학습시킨다. direction matrix V는 standard LoRA approach와 같이 B\*A matirx에 의해 강화되고, m은 또 따로 그대로 훈련된다. 이와 같이 direction 강화/업데이트에 집중하여 학습이 진행되기 때문에 Full Fine-Tuning에 준하는 tuning 방법이 될 수 있다고 DoRA 저자는 주장한다. 또한 LoRA가 학습에 사용하는 parameter보다 더 적은 수의 parameter를 사용하면서도 더 좋은 성능을 낸다고 한다. 

