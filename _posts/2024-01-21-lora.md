---
title: "Variants of Low-Rank Adaptation : LoRA, DoRA"
category: Natural Language Processing
tag: NLP
---







* 목차
{:toc}








최근 대규모 모델이 많이 나오며 대규모 모델을 효율적으로 학습시킬 수 있는 방법에 대한 연구가 활발히 진행되고 있다. 해당 연구는 주로 대규모 모델 tuning 성능을 더 좋게 하거나 모델을 보다 빠르게 훈련시킬 수 있도록 하는 방향으로 진행되고 있다. 이러한 연구의 대표로는 Low-Rank Adaptation (LoRA)가 있는데 LoRA의 변형들도 많이 나온 상황이다. 



# LoRA

<center><img width="400" src="https://github.com/finddme/finddme.github.io/assets/53667002/87e0634d-0680-48b7-a751-84cd8c098886"></center>
<center><em style="color:gray;">Low-Rank Adaption (LoRA)</em></center><br>


위 이미지처럼 pre-trained weight matrix $W$는 frozen 시키고 $W$ 옆에 $W$보다 작은 low-rank matrix A와 B를 추가한다. 



# DoRA
