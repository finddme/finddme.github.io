---
title: "Prompt-Based Learning 2 | P-tuniung: GPT Understands, Too"
category: Natural Language Processing
tag: NLP
---







* 목차
{:toc}








# 1. Introduction / Motivation

GPT 계열의 모델들은 fine-tuning 방식으로 NLU task를 수행했을 때 bert계열의 모델들보다 좋은 성능을 보이지 않는다. 이는 GPT 계열의 일방향적 특성 때문이라고 알려져 있다. GPT3의 경우에는 적절하게 만든 handcrafted prompt를 통해 NLU task를 풀긴 하지만 best-performing prompt를 handcrafting으로 찾는 것은 사막에서 바늘 찾는 것과 같다. 그리고 pattern을 manual하게 조정하게 되면 pattern(prompt)에 따라 성능 차이가 크다는 문제도 존재한다. Table 1은 prompt에 따라 성능차이가 심한 것을 보여준다.

<center><img width="600" src="https://user-images.githubusercontent.com/53667002/208833410-9cf5c9e5-f445-4984-811b-54b4a9e66a6b.png"></center>

> 그리고 GPT3와 같이 거대한 모델의 치명적인 단점은 poor transferability이다. many-shot setting 하에서fine-tuning을 한다 해도 거대 모델들은 fine-tuning sample을 빠르게 담아내기에 모델 자체가 너무 커서 제대로 되지 않는다.

이러한 이유로 prompt를 자동으로 생성하는 연구들이 진행되었다. 하지만 이들은 모두 이산적인 prompt를 생성하는데 neural network 자체가 continuous하기 때문에 discrete한 정보는 neural network에게 sub-optimal하다. 따라서 본 논문의 저자들은 P-tuning을 제안하며 연속적인 공간에서 prompt를 자동으로 생성하여 GPT계열 모델들의 NLU application 수행문제를 해결하고자 하였다.

> Discrete: token/문자로 이루어져 이산적인 상태
> 
> Continuous: token이 embedding되어 연속적으로 존재하는 상태

본 논문에서는 P-tuning을 이용했을 때 GPT 계열의 모델도 비슷한 크기의 BERT 계열 모델만큼 NLU task를 잘 수행할 수 있다는 것을 보여준다. 그리고 BERT도 p-tuning을 했을 때 fine-tuning보다 성능이 향상되었다고 한다.

BERT와 같이 MLM task를 학습한 모델도 prompt를 활용하여 task를 해결할 수 있는 방법을 제안한 PET, iPET의 경우 labeled data가 많이 필요한 fine-tuning의 문제점을 완화시킬 방법을 제안했지만 두 방법론의 경우에는 unlabeled data가 많이 필요하고, 앞서 언급한 GPT의 prompt 생성과 동일하게 pattern을 manual하게 생성 및 조정해야 한다는 문제와 생성한 prompt가 discrete하다는 문제가 있다. 따라서 BERT 계열의 prompt based learning에서도 p-tuning을 통해 성능 향상이 이루어진 것으로 보인다

# 2. Method: P-tuning

## 2.1. Architecture

<center><img width="800" src="https://user-images.githubusercontent.com/53667002/208844126-f58fb9c7-19e4-40c1-9280-7b3091816b86.png"></center>

Figure 2의 (a)는 PET, iPET, AdaPrompt, AutoPrompt와 같이 P-tuning 이전에 선행된 Prompt searching 방식을 표현한 것인데 모두 LM의 loss를 reward로 삼아 prompt generator를 통해 이산적인 prompt를 생성한다. 앞서 말했듯이 이산적인 정보는 neural network에게 sub-optimal하다.

P-tuning은 선행 연구들의 문제를 해결하기 위해 Figure 2의 (b)와 같이 bi-LSTM으로 구성된 prompt encoder가 LM loss를 받아 학습하여 연속적인 prompt를 생성한다.  prompt encoder 학습 시에는 PLM의 전체 weight에 대해 parameter update(fine-tuning)를 하는 것이 아니라 continuous prompt embedding만 tuning한다.

Figure 2의 (a)는 PET, iPET, AdaPrompt, AutoPrompt와 같이 P-tuning 이전에 선행된 Prompt searching 방식을 표현한 것인데 모두 LM의 loss를 reward로 삼아 prompt generator를 통해 이산적인 prompt를 생성한다. 앞서 말했듯이 이산적인 정보는 neural network에게 sub-optimal하다.
P-tuning은 선행 연구들의 문제를 해결하기 위해 Figure 2의 (b)와 같이 bi-LSTM으로 구성된 prompt encoder가 LM loss를 받아 학습하여 연속적인 prompt를 생성한다.  prompt encoder 학습 시에는 PLM의 전체 weight에 대해 parameter update(fine-tuning)를 하는 것이 아니라 continuous prompt embedding만 tuning한다. 

P-tuning 관련 notation은 아래와 같다.

$$
\begin{aligned} 
\mathcal{M}\text{ : pre-trained language model}
\end{aligned}
$$

$$
\begin{aligned} 
\text{x}_{1:n}\text{= }\left\{ \text{x}_{0}, \text{x}_{1},\cdots, \text{x}_{n} \right\} \text{: a sequence of discrete input tokens}
\end{aligned}
$$

$$
\begin{aligned} 
\mathbf{e}\in\mathcal{M}\text{: pretrained embedding layer}
\end{aligned}
$$

$$
\begin{aligned} 
\left\{ e(\text{x}_{0}), e(\text{x}_{1}),\cdots, e(\text{x}_{n}) \right\} \text{: input embeddings mapped by }\mathbf{e}\in\mathcal{M}\
\end{aligned}
$$

input token을 PLM의 embedding layer에 넣어 input embedding값을 만든다. pre-training의 경우 $text{x}$는 unmasked tokens이고 $text{y}$는 \text([MASK]) token일 것이고 sentence classification task에서 $text{x}$는 sentence token, $text{y}$는 \text([CLS]) token일 것이다.

$$
\begin{aligned} 
\mathbf{p}\text{: function for creating prompts }
\end{aligned}
$$

$$
\begin{aligned} 
\mathbf{x}\text{: context}
\end{aligned}
$$

$$
\begin{aligned} 
\mathbf{y}\text{: target}
\end{aligned}
$$

$$
\begin{aligned} 
\mathit{T}\text{: template}
\end{aligned}
$$

prompt를 만드는 함수 $\mathbf{p}$에 $\mathbf{x}$와 $\mathbf{y}$를 넣어 template을 만든다. 예를 들어 수도를 예측하는 task가 있다고 가정했을 때, “The capital of ... is ... .”라는 prompt에 “Britain”이라는 context와 “[MASK]”라는 target을 넣으면 template “The capital of Britain is [MASK].”이 만들어진다.

$$
\begin{aligned} 
\mathcal{V}\text{: vocabulary of a language model }\mathcal{M}
\end{aligned}
$$

$$
\begin{aligned} 
\text{[}\text{P}_{i}\text{]}\text{: i}^{th}\text{prompt token in a template }\mathit{T}
\end{aligned}
$$

$$
\begin{aligned} 
\mathit{T}\text{=}\left\{\text{ [}\text{P}_{0:i}\text{]}, \mathbf{x}, \text{[}\text{P}_{i+1:m}\text{]}, \mathbf{y}\right\}
\end{aligned}
$$

LM의 vocab을 $\mathcal{V}$, template을 구성하는 prompt의 $\text{i}$번째 token을 $\text{[}\text{P}_{i}\text{]}$라 하고 주어진 template이 위와 같을 때 discrete prompt search 방식의 경우 $\text{[}\text{P}_{i}\text{]}\in\mathcal{V}​$이기 때문에(prompt를 구성하는 token이 LM vocab 안에 있기 때문에) discrete prompt template은 아래와 같이 표현될 수 있다.

<center><img width="400" src="https://user-images.githubusercontent.com/53667002/209091196-608c4eeb-ac0d-44ff-afe7-ad7a682e1069.png"></center>

P-tuning의 경우에는 prompt token $$\text{[}\text{P}_{i}\text{]}$$에 해당하는 것들이 LM의 vocab에 한정되지 않는 trainable embedding tensor \text{h}_{i}\text{(0}\leq \text{i}\leq \text{m)} pseudo token으로 교체된다. 이를 통해 LM의 vocab을 넘어 더 나은 continuous prompts를 찾을 수 있다.

<center><img width="400" src="https://user-images.githubusercontent.com/53667002/209091276-85ed5041-8d7a-4b3c-bed3-2e3f926a9ca4.png"></center>

이런 식으로 template에 대한 embedding값을 뽑으면 그걸 LM에 넣어 mask token($\mathbf{y}$)에 대해 LM loss를 계산한 후에 그걸로 prompt encoder를 update시킨다. 즉, LM으로 추론하고 그걸 기반으로 prompt encoder(bi-LSTM)을 학습시킨다. 이때 LM은 parameter update를 하지 않기 때문에 연산량과 메모리를 줄일 수 있다. LM이 이 과정에서 학습을 하게 될 경우 backpropagation을 위해 gradient를 저장해 두어야 하고 방향을 기억(retain_graph=True)해야 하는데 이것이 필요 없기 때문이다. 

## 2.2. Optimization

continuous prompt 학습에는 두 가지 문제점이 있다.

1) Discreteness
LM의 original word embedding $\mathbf{e}$는 이미 pre-training 이후에 highly discrete된 상태이다. $\mathit{h}$를 random initialized시킨 이후 SGD로 최적화하게 되면 optimizer는 local minima에 빠지기 쉽다.

2) Association
직관적으로 생각해 봤을 때 prompt embedding $\mathit{h}_{i}$의 value들은 독립적이지 않고 서로 의존적이어야 한다. 따라서 prompt embedding 값들을 서로 연결시킬 어떤 mechanism이 있어야 한다.

저자는 위 두 문제를 해결하기 위해 lite한 neural network로 구성된 Prompt encoder를 통해 $\mathit{h}_{i}$를 sequence로 modeling하는 방안을 제안한다. 본 논문에서는 bi-LSTM과 함께 ReLU activated two-layer multilayer perceptron (MLP)이 사용되었다.

<center><img width="400" src="https://user-images.githubusercontent.com/53667002/209091480-744c748e-de2d-4a03-ad25-b2fd2d165853.png"></center>



# 3. Experiments

본 논문은 두 가지 NLU task benchmark에 대해 실험을 진행하였다: LAMA knowledge probing, SuperGlue.

## 3.1. Knowledge Probing

LAMA knowledge probing은 LM이 real-world knowledge에 대해 얼마나 잘 학습했는지 확인하는 task이다.

<center><img width="400" src="https://user-images.githubusercontent.com/53667002/209890359-5cd0cad3-6e21-4d99-91b4-c3095ac2ba57.png"></center>

실험 결과 MP(Manual prompt), discrete prompt보다 p-tuning 성능이 더 좋다.  Table 2의 오른쪽 표에서 P-tuning 부분에 표기된 붉은 글씨는 MP와의 차이이다. 그리고 MP+FT(Manual prompt augmented fine-tuning)보다도 p-tuning 성능이 높다. Megatron의 경우는 parameter가 너무 많아서 fine-tuning이 불가해 관련 결과가 없지만 MP보다 p-tuning 결과가 좋은 것을 확인할 수 있다.


<center><img width="400" src="https://user-images.githubusercontent.com/53667002/209890394-2a521b80-d835-4b4f-9545-ac399e404670.png"></center>

Table3는 bert size와 비슷한 gpt2 모델을 사용하여 SuperGLUE에 대해 평가를 진행한 결과이다. GPT2-base의 P-tuning 부분에 표기된 붉은 글씨는 bert base와의 차이이다. 위 결과를 보면 전반적으로 bert보다 GPT2-base가 좋은 것을 확인할 수 있는데 이는 NLU task에 bert가 gpt계열보다 좋은 성능을 보이는 일반적인 결과와는 다르다. 그리고 앞선 실험 결과와 마찬가지로 fine-tuning보다 p-tuning 성능이 좋다.


<center><img width="400" src="https://user-images.githubusercontent.com/53667002/209890422-53306791-c406-4b60-8f5b-6377f89bfca8.png"></center>

Table 4는 bert-large와 gpt2-medium과의 비교이다. base 모델의 결과와 유사하다.

<center><img width="400" src="https://user-images.githubusercontent.com/53667002/209890450-abc6f584-c13c-4d02-a891-975332485055.png"></center>

Table 5는 few-shot setting하에 진행된 실험 결과이다. 여기에서도 p-tuning 결과가 좋다.

# Reference

> Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang. "GPT Understands, Too,"2021

