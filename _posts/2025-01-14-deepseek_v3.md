---
title: "DeepSeek-V3 (작성 중)"
category: LLM / Multimodal
tag: Multimodal
---


 




* 목차
{:toc}













# DeepSeek-V3 요약 
- Open Source Model 
- Architecture : Mixture-of-Experts(MoE) 구조
- Parameter:
  - total : 6,710억개
  - activated per token: 370억개
- Speed: 초당 약 60개의 토큰을 처리할 수 있어 이전 버전(DeepSeek-V2)보다 3배 빨라졌다고 한다.
- Specialized task: 교육, 코딩, 수학적 추론


# Architecture 

DeepSeek 모델도 Transformer Decoder 구조를 기반으로 하지만 여기에 몇 가지 설계적 요소를 추가하여 성능을 향상시켰다.

## Multi-head Latent Attention (MLA)
