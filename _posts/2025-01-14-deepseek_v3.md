---
title: "DeepSeek-V3 (작성 중)"
category: LLM / Multimodal
tag: Multimodal
---


 




* 목차
{:toc}













# DeepSeek-V3 요약 
- Open Source Model 
- Architecture : Mixture-of-Experts(MoE) 구조
- Parameter:
  - total : 6,710억개
  - activated per token: 370억개
- Speed: 초당 약 60개의 토큰을 처리할 수 있어 이전 버전(DeepSeek-V2)보다 3배 빨라졌다고 한다.
- Specialized task: 교육, 코딩, 수학적 추론


# Architecture 

DeepSeek 모델도 Transformer Decoder 구조를 기반으로 하지만 여기에 몇 가지 설계적 요소를 추가하여 성능을 향상시켰다.

## Multi-head Latent Attention (MLA)

기존 transformer attenetion mechanism을 개선한 attention으로, 이 방법론을 도입함으로써 token 처리 속도가 크게 개선되었다고 한다. 

- MLA 작동 방식:<br>
  1. **low-rank key-value joint compression**<br>
     key-value에 대해 저차원 공동 압축을 수행. 즉, key와 value를 함께 압축하여 kv cache를 줄이는 방법이다. <br>
  2. 압축된 latant vector를 cache에 저장<br>
  3. key-value를 동적으로 재구성하여 원래 정보 유지시킨 후 attention을 계산한다. <br>

- MLA로 인한 이점
  - 메모리 사용량 감소: KV cache를 줄여 memory 사용량을 감소시킨다.
  - 긴 sequence 처리: 메모리 요구량이 감소하여 더 긴 sequence나 더 큰 batch를 처리할 수 있음
  - 추론 속도 향상: 효율적인 메모리 사용으로 추론 속도가 향상
 
## Mixture-of-Experts (MoE)

DeepSeek-V3는 총 6710억 개의 parameter를 가진 모델이지만, MOE를 통해 370억개의 parameter만 활성화하여 각 token을 처리한다. 이렇게 필요한 부분만 활성화 시킴으로써 계산 자원을 감소시키고 효율성을 증대시킨다.

또한 MoE는 여러 expert들을 두어 대규모 모델의 능력을 유지하면서도 효율적인 추론이 가능하게 한다. 

1) expert module: "expert"라 불리는 작은 모델들로 구성된다. 각 expert들은 특정 유형의 데이터나 task에 특화되어 있다. (Deepseek-v3은 매우 세분화된 expert들을 두고 있다고 한다.)<br>
2) router module: 입력 데이터를 분석하여 가장 적합한 expert들을 선택한다. (모든 expert들이 작동하지 않고 특정 입력에 가장 적합한 expert들만 선택적으로 활성화된다.)<br>
3) **Load Balancing**: 각 expert의 작업 부하를 모니터링하고 조정하여 균형을 유지한다. 과부하된 expert의 조정값을 줄이고, 과부하되지 않은 expert의 조정값을 늘린다.<br>
4) Shared Experts / Routed Experts: expert들은 모든 token에 접근 가능한 shared expert와 router에 의해 선택되는 routed expert로 구분된다.<br>


- 전통적인 MOE와의 차별점:<br>
  1. Expert 세분화<br>
    - 일반적인 MOE architecture에 비해 expert들의 FFNN을 줄여 각 expert들의 크기를 줄인다. <br>
    - expert들의 크기가 작아진 만큼, 활성화되는 expert들의 수를 늘려 계산 비용을 일정하게 유지한다. -> parameter 수와 계산 비용을 일정하게 유지하면서도 성능을 향상시킨다.<br>
    - 더 많은 expert들이 활성화됨으로써 각 token 처리 시 매우 유연한 expert 조합이 가능해진다. -> 더 많은 수의 expert들이 활성화되어 다양한 지식을 더 정밀하게 분석할 수 있다. <br>
2. Shared expert isolation<br>
    - expert 중 일부($Ks$개)를 shared experts로 지정하여 shared experts와 routed experts를 분리한다.<br>
    - shared experts들은 routing 결과와 무관하게 항상 활성화된다. 즉, 모든 입력 token은 shared expert들에 항상 할당된다. <br>
    - shared expert들은 다양한 맥락에서 공통적으로 필요한 정보를 capture하고 통합하는 역할을 한다.<br>
    - 이는 공통 지식을 shared expert에 집중시켜 다른 routed expert들 간의 지식 중복이 없도록 한다.<br>
    - routing 되는 expert들의 수는 $mN-Ks$로 조정된다.<br>
3. Load Balancing<br>
    - expert별 bias를 조정하여 균형을 맞추는 것이다.<br>
    - 예를 들어, "금융 사기 탐지 과제를 위한 LLM 기반 솔루션"이라는 문장이 들어왔을 때, routing된 expert들의 affinity score이 다음과 같을 때:<br>
       `{"기술 expert":0.7,"금융 expert":0.4, "사회 expert": 0.2}`<br>
      위 경우, 기술 expert가 과부하된 상태이다. expert들의 활용도를 균형있게 만들기 위해 위 점수에 bias가 추가된다. <br>

