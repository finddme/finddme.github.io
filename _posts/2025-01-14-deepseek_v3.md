---
title: "DeepSeek-V3 (작성 중)"
category: LLM / Multimodal
tag: Multimodal
---


 




* 목차
{:toc}













# DeepSeek-V3 요약 
- Open Source Model 
- Architecture : Mixture-of-Experts(MoE) 구조
- Parameter:
  - total : 6,710억개
  - activated per token: 370억개
- Speed: 초당 약 60개의 토큰을 처리할 수 있어 이전 버전(DeepSeek-V2)보다 3배 빨라졌다고 한다.
- Specialized task: 교육, 코딩, 수학적 추론


# Architecture 

DeepSeek 모델도 Transformer Decoder 구조를 기반으로 하지만 여기에 몇 가지 설계적 요소를 추가하여 성능을 향상시켰다.

## Multi-head Latent Attention (MLA)

기존 transformer attenetion mechanism을 개선한 attention으로, 이 방법론을 도입함으로써 token 처리 속도가 크게 개선되었다고 한다. 

- MLA 작동 방식:
  1) **low-rank key-value joint compression**<br>
     key-value에 대해 저차원 공동 압축을 수행. 즉, key와 value를 함께 압축하여 kv cache를 줄이는 방법이다. 
  2) 압축된 latant vector를 cache에 저장
  3) key-value를 동적으로 재구성하여 원래 정보 유지시킨 후 attention을 계산한다. 

- MLA로 인한 이점
  - 메모리 사용량 감소: KV cache를 줄여 memory 사용량을 감소시킨다.
  - 긴 sequence 처리: 메모리 요구량이 감소하여 더 긴 sequence나 더 큰 batch를 처리할 수 있음
  - 추론 속도 향상: 효율적인 메모리 사용으로 추론 속도가 향상
 
## Mixture-of-Experts (MoE)

DeepSeek-V3는 총 6710억 개의 parameter를 가진 모델이지만, MOE를 통해 370억개의 parameter만 활성화하여 각 token을 처리한다. 이렇게 필요한 부분만 활성화 시킴으로써 계산 자원을 감소시키고 효율성을 증대시킨다.

또한 MoE는 여러 expert들을 두어 대규모 모델의 능력을 유지하면서도 효율적인 추론이 가능하게 한다. 

1) expert module: "expert"라 불리는 작은 모델들로 구성된다. 각 expert들은 특정 유형의 데이터나 task에 특화되어 있다. (Deepseek-v3은 매우 세분화된 expert들을 두고 있다고 한다.)
2) router module: 입력 데이터를 분석하여 가장 적합한 expert들을 선택한다. (모든 expert들이 작동하지 않고 특정 입력에 가장 적합한 expert들만 선택적으로 활성화된다.)
3) **Load Balancing**: 각 expert의 작업 부하를 모니터링하고 조정하여 균형을 유지한다. 과부하된 expert의 조정값을 줄이고, 과부하되지 않은 expert의 조정값을 늘린다.
4) Shared Experts / Routed Experts: expert들은 모든 token에 접근 가능한 shared expert와 router에 의해 선택되는 routed expert로 구분된다.




