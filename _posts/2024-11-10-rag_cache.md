---
title: "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation (작성 중)"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}









본 논문은 베이징 대학교와 ByteDance가 효율적인 RAG 수행을 위해 연구한 내용을 담고 있다. 최근 LLM 추론 시간 자체에 대한 가속화 연구가 많이 진행되고 있다. RAG 작동 효율 향상을 위해 LLM 추론 자체가 빠른 것도 도움되지만 RAG 시스템 특성을 적절히 고려한 최적화도 필요하다.

RAG 시스템에서 보이는 주요한 문제는 아래와 같다:

**1. Performance Bottleneck**<br>
RAG 시스템의 주요 병목 지점은 외부 문서의 통합으로 인해 LLM이 처리할 sequence가 길이가 너무 길어지는 부분이다.

**2. Access Pattern**<br>
Retrieval request는 매우 집중적인 패턴을 보인다. 예를 들어 전체 검색 요청의 60%이상이 전체 문서의 3%에 대한 질문이다. 따라서 이와 같은 패턴은 cache 최적화가 필요하다.

**3. Optimized Spac**<br>
자주 사용되는 문서들에 대해 중간 계산 단계를 caching함으로써 computational overhead를 크게 줄일 수 있다. 즉, LLM 이 처리해야 할 중간 계산 결과들을 미리 계산해서 cache에 저장해 둠으로써 시간을 크게 줄일 수 있다는 것이다. 이렇게 되면 자주 사용되는 문서를 처리할 때 처음부터 모든 계산을 다시 수행할 필요가 없어 결과적으로 처리 시간이 단축되게 된다. 실험 결과에 따르면 이 방법을 도입한 후 pre-population 시간이 11.5배 감소했다고 한다.

# RAGCache

RAGCache의 핵심 아이디어는 multi-layer dynamic caching system을 통해 검색된 문서들에 대한 중간 계산 상태를 효율적으로 caching하고 재사용함으로써 RAG을 효율적으로 수행하는 데에 있다. 

<center><img width="500" src="https://github.com/user-attachments/assets/e90613eb-356c-4153-8696-af7e135b0ea5"></center>
<center><em style="color:gray;">https://arxiv.org/pdf/2404.12457</em></center><br>

## Knowledge Tree Structure

RAGCache는 caching된 문서들의 state를 구조화 하기 위해 knowledge tree 구조를 설계했다. 이와 같은 tree 형태의 구조는 RAG 시스템에서 문서 검색의 순서 민감성 문제를 해결한다.

예를 들어 아래 그림에서 [D1,D3]와 [D2,D3] 두 개의 문서 sequence가 있을 때, D3가 두 sequence 모두에 나타나지만 앞에 나온 sequence가 다르기 때문에 각 sequence에서의 key-value tensor 값이 서로 다르다. knowledge tree 구조는 이와 같은 변화들을 효율적으로 관리하여 문서들의 순서를 유지함과 동시에 빠른 검색을 가능하게 한다. 

<center><img width="500" src="https://github.com/user-attachments/assets/8d175c09-faf9-44c9-b918-ce2b98167a7e"></center>
<center><em style="color:gray;">https://arxiv.org/pdf/2404.12457</em></center><br>

## Prefix-aware Greedy Double Size Frequency (PGDSF) replacement policy 

RAGCache는 여러 요소들을 고려하는 복잡한 caching replacement 전략을 사용한다. 이 전략은 가장 가치있는 문서 state들이 cache에 유지되도록하며, cache hit rate를 최대화하고 중복 계산을 최소화 한다.

