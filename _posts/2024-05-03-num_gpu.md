---
title: "Estimate Memory Usage and Trainable Parameters(original,lora,quantization)"
category: LLM / Multimodal
tag: NLP
---







* 목차
{:toc}










# Calculate GPU Requirements
## Regarding Training (Full Training)

-batch size 1일 때의 수식으로 보임.

<center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/dc962a28-d216-4cda-bc4c-49db2caabbd4"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

num_gpu=(model parameters in billions * 18 * 1.25)/GPU size in GB

- **model parameters in billions**: 일반적으로 모델 크기를 나타낼 때 사용되는 "B" 단위 (모델 총 parameter 수를 10억으로 나눈 값)
- **18**: 학습에 사용되는 요소들을 고려한 대략적인 계수. (optimizer state: 8 + gradients: 4 + weights: 6)
- **1.25**: 입력된 데이터를 모델이 처리할 때 변하는 dynamic data structure에 25% 메모리를 추가한 값. 입력데이터의 크기가 크면 이 값이 커 질 수도 있을 것 같다.

```python
import math

def calculate_gpus(model_params_in_B, gpu_size):
    gpus_required = (model_params_in_B * 18 * 1.25) / gpu_size
    return math.ceil(gpus_required)
```

예를 들어 A100 80GB로 LLaMa3 7B를 학습할 때 위 공식에 따르면,

(7 * 18 * 1.25)/80

약 2개의 GPU가 필요하다.

## Regarding Training (LoRA / Quantization)
- LoRA나 quantization이 적용된 경우, Trainable Parameter를 계산하여 적용.
  - [LoRA](https://finddme.github.io/llm%20/%20multimodal/2024/01/21/lora/#lora)는 original model의 parameter가 frozen된 상태로 추론처럼 사용되기 때문에 해당 모델이 [추론에 소요되는 GPU수](https://finddme.github.io/llm%20/%20multimodal/2024/05/03/num_gpu/#gpus-needed-for-inference) +  LoRA를 적용한 Trainable Parameter에 대한 필요 GPU 수로 계산하면 될 것으로 보임.

### LoRA

```python
import math

def calculate_gpus_lora(model_params_in_B, lora_trainable_params_in_B, gpu_size):
    gpus_required_frozen_metrix = (model_params_in_B * 2 * 1.25) / gpu_size
    gpus_required_lora_metrix = (lora_trainable_params_in_B * 18 * 1.25) / gpu_size
    gpus_required= gpus_required_frozen_metrix+gpus_required_lora_metrix
    return math.ceil(gpus_required)
```

## GPUs needed for Inference

<center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/c3330777-4d97-4152-bc1a-5c29e3e0709c"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

num_gpu=(model parameters in billions * 2 * 1.25)/GPU size in GB

```python
import math

def calculate_gpus_inference(model_params_in_B, gpu_size):
    gpus_required = (model_params_in_B * 2 * 1.25) / gpu_size
    return math.ceil(gpus_required)
```

#  Trainable Parameters

Trainable Parameters는 모델이 학습단계에서 값이 계산되고, 없데이트 되는 parameter이다.  

```python
def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}"
    )
```
# Reference

> https://medium.com/@plthiyagu/calculate-gpu-requirements-for-your-llm-training-7122a3700547

> https://medium.com/illuminations-mirror/how-many-gpus-do-you-really-need-for-model-training-ce0cacf3ce9b
