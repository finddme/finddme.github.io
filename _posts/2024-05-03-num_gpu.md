---
title: Number of GPUs needed for training
category: LLM / Multimodal
tag: NLP
---







* 목차
{:toc}











# Regarding Training (Full Training)

<center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/dc962a28-d216-4cda-bc4c-49db2caabbd4"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

num_gpu=(model parameters in billions * 18 * 1.25)/GPU size in GB

- **model parameters in billions**: 일반적으로 모델 크기를 나타낼 때 사용되는 "B" 단위 (모델 총 parameter 수를 10억으로 나눈 값)
- **18**: 학습에 사용되는 요소들을 고려한 대략적인 계수. (optimizer state: 8 + gradients: 4 + weights: 6)
- **1.25**: 입력된 데이터를 모델이 처리할 때 변하는 dynamic data structure에 25% 메모리를 추가한 값.

```python
import math

def calculate_gpus(model_params_in_B, gpu_size):
    gpus_required = (model_params_in_B * 18 * 1.25) / gpu_size
    return math.ceil(gpus_required)
```

예를 들어 A100 80GB로 LLaMa3 7B를 학습할 때 위 공식에 따르면,

(7 * 18 * 1.25)/80

약 2개의 GPU가 필요하다.

# GPUs needed for Inference

<center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/c3330777-4d97-4152-bc1a-5c29e3e0709c"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

num_gpu=(model parameters in billions * 2 * 1.25)/GPU size in GB

```python
import math

def calculate_gpus(model_params_in_B, gpu_size):
    gpus_required = (model_params_in_B * 2 * 1.25) / gpu_size
    return math.ceil(gpus_required)
```

# Reference

> https://medium.com/@plthiyagu/calculate-gpu-requirements-for-your-llm-training-7122a3700547

> https://medium.com/illuminations-mirror/how-many-gpus-do-you-really-need-for-model-training-ce0cacf3ce9b
