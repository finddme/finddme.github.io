---
title: "Model Parameters and Memory Usage"
category: LLM / Multimodal
tag: NLP
---







* 목차
{:toc}









# Model Parameter
## Parameter types

1. float: 32-bit floating point, 4 bytes

2. half/BF16: 16-bit floating point, 2 bytes

3. int8: 8-bit integer, 1 byte

4. int4: 4-bit integer, 0.5 bytes

```
1 Billion Parameters = 4 x 10E9 bytes = 4GB

1B = 10억개 Parameter
```

LLM이 70B라면 이는 모델이 조정 가능한 parameter를 700억개 가지고 있다는 것을 의미한다. parameter가 많을수록 모델 내부 신경망은 더 복잡해지며, 더 많은 데이터를 잘 학습할 가능성이 높아진다. 하지만 parameter가 많아 내부 신경망이 복잡할수록 계산량이 많아져 모델 학습 혹은 추론에 많은 비용이 요구된다.

## Trainable Parameters

Trainable Parameters는 모델이 학습단계에서 값이 계산되고, 없데이트 되는 parameter이다.  

```python
def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}"
    )
```

# Estimate Memory Usage
Model Parameter 1B를 기준으로 model parameter에 따라 대략 아래 표와 같은 memory가 요구된다. 

그리고 학습 시에는 model parameter로 인해 요구되는memory 외에 학습 시 gradients, optimizer states, weight 계산 등 추가적으로 memory가 할당되어야 하는 요소들이 존재한다.

- Optimizer states를 위해 요구되는 memory 사용량은 optimizer 유형에 따라 다르다.<br>
  - AdamW optimizer: model parameter 1B당 요구되는 memory의 2배<br>
  - SGD optimizer: model parameter 1B당 요구되는 memory와 동일

- Gradients: model parameter 1B당 요구되는 memory와 동일

<html>
<table border="0" cellpadding="0" cellspacing="0" width="374" style="">
  <thead>
    <tr height="23" style="height:17.4pt">
      <th colspan='3'>Inference</th>
    </tr>
  </thead>
 <colgroup><col width="152" span="2" style="mso-width-source:userset;mso-width-alt:4864;
 width:114pt">
 <col width="70" style="width:53pt">
 </colgroup>
  <tbody>
    <tr height="23" style="height:17.4pt">
      <td rowspan="3" height="69" class="xl66" style="height:52.2pt">Model parameter 1B</td>
      <td class="xl65">float</td><td>4B</td>
    </tr>
    <tr height="23" style="height:17.4pt">
      <td height="23" class="xl65" style="height:17.4pt">BF16</td><td class="xl65">2GB</td>
    </tr>
    <tr height="23" style="height:17.4pt">
      <td height="23" class="xl65" style="height:17.4pt">int8</td><td class="xl65">1GB</td>
    </tr>
  </tbody>
</table>
  
## Estimate Memory Usage for Training (Full Training)

- Model parameter type: float, Train batch size=1

<center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/dc962a28-d216-4cda-bc4c-49db2caabbd4"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

num_gpu=(model parameters in billions * 18 * 1.25)/GPU size in GB

- **model parameters in billions**: 일반적으로 모델 크기를 나타낼 때 사용되는 "B" 단위 (모델 총 parameter 수를 10억으로 나눈 값)
- **18**: 학습에 사용되는 요소들을 고려한 대략적인 계수. (optimizer state: 8 + gradients: 4 + weights: 6)
- **1.25**: 입력된 데이터를 모델이 처리할 때 변하는 dynamic data structure에 25% 메모리를 추가한 값. (입력데이터의 크기가 크면 이 값이 커 질 수도 있을 것 같다.)

```python
import math

def calculate_gpus(model_params_in_B, gpu_size):
    gpus_required = (model_params_in_B * 18 * 1.25) / gpu_size
    return math.ceil(gpus_required)
```

예를 들어 A100 80GB로 LLaMa3 7B를 학습할 때 위 공식에 따르면,

(7 * 18 * 1.25)/80

약 2개의 GPU가 필요하다.

## Estimate Memory Usage for Training (LoRA / Quantization)
- LoRA나 quantization이 적용된 경우, Trainable Parameter를 계산하여 적용.
  - [LoRA](https://finddme.github.io/llm%20/%20multimodal/2024/01/21/lora/#lora)는 original model의 parameter가 frozen된 상태로 추론처럼 사용되기 때문에 해당 모델이 [추론에 소요되는 GPU수](https://finddme.github.io/llm%20/%20multimodal/2024/05/03/num_gpu/#gpus-needed-for-inference) +  LoRA를 적용한 Trainable Parameter에 대한 필요 GPU 수로 계산하면 될 것으로 보임.

### LoRA

```python
import math

def calculate_gpus_lora(model_params_in_B, lora_trainable_params_in_B, gpu_size):
    gpus_required_frozen_metrix = (model_params_in_B * 2 * 1.25) / gpu_size
    gpus_required_lora_metrix = (lora_trainable_params_in_B * 18 * 1.25) / gpu_size
    gpus_required= gpus_required_frozen_metrix+gpus_required_lora_metrix
    return math.ceil(gpus_required)
```

## Estimate Memory Usage for Inference

일반적으로 모델 parameter를 BF16로 불러오기 때문에 GPU 요구량은 아래와 같이 계산될 수 있다

<center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/c3330777-4d97-4152-bc1a-5c29e3e0709c"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>
</html>
num_gpu=(model parameters in billions * 2 * 1.25)/GPU size in GB

```python
import math

def calculate_gpus_inference(model_params_in_B, gpu_size):
    gpus_required = (model_params_in_B * 2 * 1.25) / gpu_size
    return math.ceil(gpus_required)
```

# Reference

> https://medium.com/@plthiyagu/calculate-gpu-requirements-for-your-llm-training-7122a3700547

> https://medium.com/illuminations-mirror/how-many-gpus-do-you-really-need-for-model-training-ce0cacf3ce9b
