---
title: "Llama 2: Open Foundation and Fine-Tuned Chat Models"
category: Natural Language Processing
tag: NLP
---







* 목차
{:toc}








# 1. LLAMA 2

- 상업적 용도로 사용할 수 있는 무료LLM
- Pretrained(Foundation) model과 Fine-Tuned For Chat use cases model 공개
- 7B, 13B, 70B version

<center><img width="1000" src="https://github.com/facebookresearch/codellama/assets/53667002/8e4378d7-b1cf-4c12-8581-5eef030b3800"></center>

# 2. LLAMA 2 : Foundation Model

- auto-regressive transformer architectur수정하여 사용
- robust data cleaning
- Total Training Token 40% 증가  (1.4T tokens → 2T tokens)
- Context Length 2배 증가 (2K → 4K)
- 추론 성능 향상을 위한 GQA (Grouped-query attention) 적용

<center><img width="1000" src="https://github.com/facebookresearch/codellama/assets/53667002/6835da0a-3508-4a04-963b-3c1d4ae18a4a"></center>

## 2.1 LLAMA 2 : Foundation Model - pretraining methodology

### 1) Pretraining Data

- Meta의 제품 또는 서비스의 데이터를 포함하지 않은 공개적으로 사용 가능한 소스를   학습 corpus 추가
- 개인 정보가 포함된 특정 사이트들의 데이터 제거
- 2 trillion tokens 학습
- 최대한 사실적인 출처를 up-sampling함으로서 hallucinations을 줄임

### 2) Training Details

- Standard transformer architecture의 부분을 수정하여 사용
    - Pre-normalization using RMSNorm
    - SwiGLU activation function
    - Rotary positional embeddings
- LLAMA1과의 주된 차이점:
    - Context length 증가 (2048 tokens → 4096 tokens)
    - 추론 성능 향상을 위한  GQA (Grouped-Query Attention) 적용
- Tokenizer
    - bytepair encoding (BPE) algorithm 사용
    - total vocabulary size는 32k tokens

### 2-1) Training Details : Context length 조정 결과

- 더 긴 context window는 더 많은 정보 처리를 가능하게 한다:
    - longer histories in chat applications
    - various summarization tasks
    - understanding longer documents
- 아래 두 표는 각각 context length만 다른 모델이 
   long-context benchmarks와 general tasks를 수행한 결과이다. 

<center><img width="1000" src="https://github.com/facebookresearch/codellama/assets/53667002/73fa6d99-83a3-4393-a4d7-5dec2a7daa49"></center>


