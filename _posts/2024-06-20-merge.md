---
title: "Merge algorithms (작성 중)"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}










Model merging 기법은 여러 모델을 하나의 모델로 결합하는 기술이다. 최근 [open llm leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)에도 많은 merge model들이 올라오고 있다. Mergekit 혹은 LazyMergekit을 통해 간편히 Model merge를 할 수 있다.



# 1. Task Vector Arithmetic

"task vectors"라는 것을 통해 신경망의 동작을 변형/수정하는 방법이다. task vectors는 pre-trained model의 weight space에서 특정 task의 성능 향상을 가리키는 방향이다. 이 기법에서는 부정(negation) 혹은 덧셈과 같은 연산을 통해 vector의 수정/변형이 이루어진다. (model의 targeted behavior에 맞게)

- modifying “task vectors”
  <center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/6e7ee199-3412-4497-9809-1380d66e0367"></center>
  
  - Forgeting via negation : task vector에 대한 부정을 통해 target task의 성능은 줄이되 control task에 대해서는 성능을 유지할 수 있다.
  - Learning via addition : task  vector에 대한 덧셈을 통해 여려 task들에 대한 성능을 향상시킬 수 있다.
  - Task analogiy : 유사한 task의 vector의 결합을 통해 추가 task에 대한 데이터 학습 없이도 해당 task 성능을 향상시킬 수 있다.

- 장점
  - Efficient Model Editing : 간단하고 효율적으로 효과적인 결합을 할 수 있다.
  - Versatility Across Models and Tasks : 다양한 model과 task에 대해 잘 작동하는 것이 많은 사례를 통해 입증되었다.


# 2. SLERP

Spherical Linear Interpolation (SLERP)는 두 vector에 대한 부드러운 interpolate(보간법/내삽)을 위해 사용되는 방법이다. 이는 고차원 공간에서 두 모델의 고유한 특성과 곡률을 보존하면서 모델을 혼합시킨다.

> interpolate<br>
> 데이터 지점의 고립점 내에서 새로운 데이터 지점을 구성하는 방식

- 구현 단계
  <center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/f3182c6f-7acc-4fd7-a2ff-0ab6bbd4985c"></center>
  
  1. **Normalization** : vector의 크기보다는 방향에 집중하여 정규화 하기 위해 입력 벡터를 단위 길이로 정규화한다. <br>
                    가중치 방향의 변화는 변화의 크기보다 더 의미 있는 정보(feature learning, representation 등)를 나타내는 경우가 많다.
  2. **Angle Calculation** : vector의 dot product(내적)을 통해 두 vector 간의 각도를 계산한다. interpolation factor(보간 계수)와 vector 간의 각도를 기반으로 scale factor(scale 계수)를 계산한다.
  3. **Vector Weighing and Summation** : 계산된 계수로 기존 vector를 가중하고, 이를 합산하여 보간된 vector를 구한다.

- 장점
  - Smooth Transitions : 부드러운 parameter 전환. 고차원 vector interpolate 시 부드럽게 parameter를 전환시킨다.
  - Preservation of Characteristics : SLERP은 모델의 특성을 잘 보존하며 결합이 진행된다. 즉, 결합하는 모델들의 고유한 특성과 곡률을 유지한다.
  - Nuanced Blending : 세밀한 혼합이 가능하다. vector space의 여러 속성들을 고려하여 두 모델의 특성을 세밀하게 반영하여 혼합한다.

- 단점
  - 한번에 두 모델만 결합할 수 있다.

[EmbeddedLLM/Mistral-7B-Merge-14-v0.1](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1)에서 시도된 것처럼 계층적으로 둘 이상의 모델을 결합할 수도 있다. 해당 모델의 merge.yml은 [https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1/blob/main/merge.yml](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1/blob/main/merge.yml)에서 확인 가능하다. .yml 파일에서 $t$는 보간 계수이다.
 
# 3. TIES

Task Vector Arithmetic과 SLERP는 비교적 오래된 모델 병합 기법인 반면 TIES는 LLM이 유행하며 새롭게 등장한 기법이다. 기존의 방법들은 각 model parameter간의 간섭을 처리하는데에 문제가 발생하곤 하는데 이 간섭은 모델 병합 이후 모델 성능 저하의 요인이 된다.  간섭의 종류에는 아래와 같이 두 가지가 있다.

- Redundancy in model parameters : 모델 parameter의 중복 방지. fine-tuning 중 변경된 parameter들 중 가장 중요한 상위 k%의 변경 사항을 식별하고 나머지는 버림으로써 모델 내의 중복 parameter를 식별하고 제거한다.
- Disagreement between parameter signs : 병합하려는 모델들이 동일한 parameter에 대해 상반된 결과를 지니고 있을 대 충돌이 발생하는데 TIES 기법에서는 병합하고자 하는 모든 모델들에서 가장 우세한 변화 방향(다수결 느낌)으로 vector을 생성하여 충돌 문제를 해결한다. 


TIES는 아래와 같이 세 단계를 통해 위와 같은 간섭을 해결한다.

<center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/0f0e6fdb-e65d-4351-a5b0-6409f3747cdc"></center>

1. **Trim** : parameter를 초기화한다. fine-tuning을 통해 변경된 parameter를 초기화하여 task별 model parameter의 중복을 줄인다. 이는 중요한 parameter인 density parameter만 유지하고 나머지는 0으로 초기화함으로써 수행된다.
2. **Resolve conflicts** : model 간 parameter 값의 부호(sign)가 상이한 경우 발생하는 충돌을 해결한다. 앞서 언급한 바와 같이 모델들의 우세한 방향으로 부호를 통일시킨다.
3. **Merge** : 최종 합의된 부호와 일치하는 parameter만 병합한다.

TIES는 기존의 병합법들보다 뛰어난 성능을 보였고, 특히 부호 간섭 문제를 효과적으로 해결하여 모델의 전반적인 성능을 향상시킨다고 알려져 있다.

```
models:
  - model: mistralai/Mistral-7B-v0.1
    # no parameters necessary for base model
  - model: OpenPipe/mistral-ft-optimized-1218
    parameters:
      density: 0.5
      weight: 0.5
  - model: mlabonne/NeuralHermes-2.5-Mistral-7B
    parameters:
      density: 0.5
      weight: 0.3
merge_method: ties
base_model: mistralai/Mistral-7B-v0.1
parameters:
  normalize: true
dtype: float16
```

위 구성은 mistralai/Mistral-7B-v0.1와 OpenPipe/mistral-ft-optimized-1218를 TIES기법으로 병합하는 예시이다.

- mistral-ft-optimized-1218(50%) + NeuralHermes-2.5-Mistral-7B(30%)
  - weight의 합이 100이 되지 않지만 나머지 20은 "normalize: true" 를 통해 parameter를 내부적으로 자동 정규화시킨다.
- density는 두 모델 모두 parameter 중 50%만 유지. 나머지 50%는 base_model에서 가져옴.

# 4. DARE
# 5. Passthrough
# 6. MoE

# Reference

> [Geometric Algebra - Linear and Spherical Interpolation (LERP, SLERP, NLERP)](https://www.youtube.com/watch?v=ibkT5ao8kGY)
> https://slgero.medium.com/merge-large-language-models-29897aeb1d1a
