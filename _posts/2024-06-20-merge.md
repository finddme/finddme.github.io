---
title: "Merge algorithms"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}










Model merging 기법은 여러 모델을 하나의 모델로 결합하는 기술이다. 최근 [open llm leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)에도 많은 merge model들이 올라오고 있다. Mergekit 혹은 LazyMergekit을 통해 간편히 Model merge를 할 수 있다.



# 1. Task Vector Arithmetic

"task vectors"라는 것을 통해 신경망의 동작을 변형/수정하는 방법이다. task vectors는 pre-trained model의 weight space에서 특정 task의 성능 향상을 가리키는 방향이다. 이 기법에서는 부정(negation) 혹은 덧셈과 같은 연산을 통해 vector의 수정/변형이 이루어진다. (model의 targeted behavior에 맞게)

- modifying “task vectors”
  <center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/6e7ee199-3412-4497-9809-1380d66e0367"></center>
  
  - Forgeting via negation : task vector에 대한 부정을 통해 target task의 성능은 줄이되 control task에 대해서는 성능을 유지할 수 있다.
  - Learning via addition : task  vector에 대한 덧셈을 통해 여려 task들에 대한 성능을 향상시킬 수 있다.
  - Task analogiy : 유사한 task의 vector의 결합을 통해 추가 task에 대한 데이터 학습 없이도 해당 task 성능을 향상시킬 수 있다.

- 장점
  - Efficient Model Editing : 간단하고 효율적으로 효과적인 결합을 할 수 있다.
  - Versatility Across Models and Tasks : 다양한 model과 task에 대해 잘 작동하는 것이 많은 사례를 통해 입증되었다.


# 2. SLERP

Spherical Linear Interpolation (SLERP)는 두 vector에 대한 부드러운 interpolate(보간법/내삽)을 위해 사용되는 방법이다. 이는 고차원 공간에서 두 모델의 고유한 특성과 곡률을 보존하면서 모델을 혼합시킨다.

> interpolate<br>
> 데이터 지점의 고립점 내에서 새로운 데이터 지점을 구성하는 방식

- 구현 단계
  <center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/f3182c6f-7acc-4fd7-a2ff-0ab6bbd4985c"></center>
  
  1. **Normalization** : vector의 크기보다는 방향에 집중하여 정규화 하기 위해 입력 벡터를 단위 길이로 정규화한다. <br>
                    가중치 방향의 변화는 변화의 크기보다 더 의미 있는 정보(feature learning, representation 등)를 나타내는 경우가 많다.
  2. **Angle Calculation** : vector의 dot product(내적)을 통해 두 vector 간의 각도를 계산한다. interpolation factor(보간 계수)와 vector 간의 각도를 기반으로 scale factor(scale 계수)를 계산한다.
  3. **Vector Weighing and Summation** : 계산된 계수로 기존 vector를 가중하고, 이를 합산하여 보간된 vector를 구한다.

- 장점
  - Smooth Transitions : 부드러운 parameter 전환. 고차원 vector interpolate 시 부드럽게 parameter를 전환시킨다.
  - Preservation of Characteristics : SLERP은 모델의 특성을 잘 보존하며 결합이 진행된다. 즉, 결합하는 모델들의 고유한 특성과 곡률을 유지한다.
  - Nuanced Blending : 세밀한 혼합이 가능하다. vector space의 여러 속성들을 고려하여 두 모델의 특성을 세밀하게 반영하여 혼합한다.

- 단점
  - 한번에 두 모델만 결합할 수 있다.

[EmbeddedLLM/Mistral-7B-Merge-14-v0.1](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1)에서 시도된 것처럼 계층적으로 둘 이상의 모델을 결합할 수도 있다. 해당 모델의 merge.yml은 [https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1/blob/main/merge.yml](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1/blob/main/merge.yml)에서 확인 가능하다. .yml 파일에서 $t$는 보간 계수이다.
 
# 3. TIES

Task Vector Arithmetic과 SLERP는 비교적 오래된 모델 병합 기법인 반면 TIES는 LLM이 유행하며 새롭게 등장한 기법이다. 기존의 방법들은 각 model parameter간의 간섭을 처리하는데에 문제가 발생하곤 하는데 이 간섭은 모델 병합 이후 모델 성능 저하의 요인이 된다.  간섭의 종류에는 아래와 같이 두 가지가 있다.

- Redundancy in model parameters : 모델 parameter의 중복 방지. fine-tuning 중 변경된 parameter들 중 가장 중요한 상위 k%의 변경 사항을 식별하고 나머지는 버림으로써 모델 내의 중복 parameter를 식별하고 제거한다.
- Disagreement between parameter signs : 병합하려는 모델들이 동일한 parameter에 대해 상반된 결과를 지니고 있을 대 충돌이 발생하는데 TIES 기법에서는 병합하고자 하는 모든 모델들에서 가장 우세한 변화 방향(다수결 느낌)으로 vector을 생성하여 충돌 문제를 해결한다. 


TIES는 아래와 같이 세 단계를 통해 위와 같은 간섭을 해결한다.

<center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/0f0e6fdb-e65d-4351-a5b0-6409f3747cdc"></center>

1. **Trim** : parameter를 초기화한다. fine-tuning을 통해 변경된 parameter를 초기화하여 task별 model parameter의 중복을 줄인다. 이는 중요한 parameter인 density parameter만 유지하고 나머지는 0으로 초기화함으로써 수행된다.
2. **Resolve conflicts** : model 간 parameter 값의 부호(sign)가 상이한 경우 발생하는 충돌을 해결한다. 앞서 언급한 바와 같이 모델들의 우세한 방향으로 부호를 통일시킨다.
3. **Merge** : 최종 합의된 부호와 일치하는 parameter만 병합한다.

TIES는 기존의 병합법들보다 뛰어난 성능을 보였고, 특히 부호 간섭 문제를 효과적으로 해결하여 모델의 전반적인 성능을 향상시킨다고 알려져 있다.

```
models:
  - model: mistralai/Mistral-7B-v0.1
    # no parameters necessary for base model
  - model: OpenPipe/mistral-ft-optimized-1218
    parameters:
      density: 0.5
      weight: 0.5
  - model: mlabonne/NeuralHermes-2.5-Mistral-7B
    parameters:
      density: 0.5
      weight: 0.3
merge_method: ties
base_model: mistralai/Mistral-7B-v0.1
parameters:
  normalize: true
dtype: float16
```

위 구성은 mistralai/Mistral-7B-v0.1와 OpenPipe/mistral-ft-optimized-1218를 TIES기법으로 병합하는 예시이다.

- mistral-ft-optimized-1218(50%) + NeuralHermes-2.5-Mistral-7B(30%)<br>
  - weight의 합이 100이 되지 않지만 나머지 20은 "normalize: true" 를 통해 parameter를 내부적으로 자동 정규화시킨다.
- density는 두 모델 모두 parameter 중 50%만 유지. 나머지 50%는 base_model에서 가져옴.

# 4. DARE

DARE는 주로 유사한 모델들 간의 병합에 많이 사용된다. DARE는 TIES와 접근 방식은 유사하지만 두 가지 큰 차이점이 있다.

- Pruning(가지치기) : fine-tuning된 weight를 무작위로 base model 값으로 재설정한다. 
- Rescaling : model의 output의 기대값을 대체로 동일하게 유지하기 위해 weight를 rescaling한다. 둘 이상의 병합 모델의 rescaling된 weight를 base model에 scale factor와 함께 추가한다.

DARE는 아래와 같이 구현된다:

1. Pruning
2. Merging : 여러 모델의 parameter를 평균화하여 단일 모델을 만든다.
3. Rescaling : 병합된 모델의 가중치를 조정하여 예상 성능을 유진한다. 

<center><img width="800" src="https://github.com/finddme/finddme.github.io/assets/53667002/df8b0e0b-26d4-4095-90c9-e466f2449918"></center>

```
models:
  - model: mistralai/Mistral-7B-v0.1
    # No parameters necessary for base model
  - model: samir-fama/SamirGPT-v1
    parameters:
      density: 0.53
      weight: 0.4
  - model: abacusai/Slerp-CM-mist-dpo
    parameters:
      density: 0.53
      weight: 0.3
  - model: EmbeddedLLM/Mistral-7B-Merge-14-v0.2
    parameters:
      density: 0.53
      weight: 0.3
merge_method: dare_ties
base_model: mistralai/Mistral-7B-v0.1
parameters:
  int8_mask: true
dtype: bfloat16
```

위 구성은 dare_ties기법을 통해 Mistral-7B를 기반으로 세 가지 모델을 병합하는 예시이다. (Mergekit에는 TIES의 부호 선택 단계를 포함한 DARE(dare_ties)과 포함하지 않는 DARE(dare_linear)이 있다.)

- 위 구성에서 병합 세 모델의 weight 합계는 1이된다. (합계는 0.9에서 1.1 사이여야 합니다)
- DARE 논문에서는 density parameter를 <0.5로 권장한다. 이 값을 지킬 때 더 일관된 결과를 얻는다고 한다.(위 구성에서는 0.5를 넘는다.)

# 5. Passthrough

기존의 모든 방법들과는 다른 방법이다. 7B model 두 개를 병합하여 9B 모델을 생성하는... 특이한 기법이다. SOLAR-10.7B-v1.0에서 사용된 depth-up 스케일링과 유사한 아이디어에서 나온 기법이다. 

```
slices:
  - sources:
    - model: OpenPipe/mistral-ft-optimized-1218
      layer_range: [0, 32]
  - sources:
    - model: mlabonne/NeuralHermes-2.5-Mistral-7B
      layer_range: [24, 32]
merge_method: passthrough
dtype: bfloat16
```

- OpenPipe/mistral-ft-optimized-1218의 32개 layer와 mlabonne/NeuralHermes-2.5-Mistral-7B의 8개 layer를 병합한다.
- 병합 결과, 총 40개의 layer와 8.99B 모델이 생성된다.

# 6. MoE

관련 포스트 : [MoE, MoA : MoE(Mixtures of Experts)](https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/#moemixtures-of-experts)

# 7. MoA

관련 포스트 : [MoE, MoA : MoA(Mixture-of-Agents)](https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/#moamixture-of-agents)

# Reference

> [Geometric Algebra - Linear and Spherical Interpolation (LERP, SLERP, NLERP)](https://www.youtube.com/watch?v=ibkT5ao8kGY)
> 
> https://slgero.medium.com/merge-large-language-models-29897aeb1d1a
> 
> https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54
