---
title: A Structural Probe for Finding Syntax in Word Representations(John.H(2019))
category: Natural Language Processing and Linguistics
tag: NLP & Linguistics
---

## 1. Introduction

본 논문은 통사적 정보를 찾기 위한 structure prob를 다루며 word representation 내에 syntactic tree가 반영되어 있는지 확인하는 방법에 대해 논의한다. 이를 위해 두 가지 문제제기를 할 수 있다: 

(1) ELMo와 [BERT]( https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/)가 문맥에 따라 영어의 dependency tree를 encoding할 수 있나.  
(2) vector representation이 tree를 encoding한다는 것을 어떻게 알 수 있나.  

## 2. Are vector spaces and trees reconcilable?

이 논문은 인간이 구사하는 문장이 chunk로 구성되어 있고, 그 chunk들이 결합되는 순서로 tree structure를 만들어볼 수 있다는 것을 기본 전제로 하고 있다. 신경망을 통해 단어들은 아래와 같이 vector로 표현될 수 있다:

<center><img width="330" alt="2021-03-21 (2)" src="https://user-images.githubusercontent.com/53667002/112918977-12d85000-9141-11eb-8357-2ba3bff94c4f.png"></center>

<center><img width="266" alt="2021-03-21" src="https://user-images.githubusercontent.com/53667002/112919033-2e435b00-9141-11eb-9437-d3b27c75302d.png"></center>

<center><img width="252" alt="2021-03-21 (1)" src="https://user-images.githubusercontent.com/53667002/112919077-461adf00-9141-11eb-88cc-a15e99b692b2.png"></center>

## 3. Methods

- **Goal**:  
본 논문에서는 vector space에 존재하는 vector(ELMo나 BERT로 학습된)로 tree structure를 찾아내고, ELMo와 BERT가 parse tree를 얼마나 인간과 유사하게 encode하는지 확인하고자 한다. 즉, neural network가 sentence의 parse tree를 잘 embedding할 수 있는지 확인하는 것을 목표로 한다. 

- **Definition**:  
그래프를 embedding한다는 것은 vector space의 기하학(norm)이 그래프의 기하학에 유사하도록 각 node의 vector representation을 학습하는 것이다(Hamilton et al., 2017)  
직관적으로, parse tree distance와 depth가 syntax와 무슨 상관이 이는 것일까? 단어와 단어사이의 distance metric은 node u,v와 거리 d_T(u,v)=1가 연결괸다는 것을 확인함으로써 tree를 포함할 수 있다. node는 norm의 크기에 따라 parents node냐 child node냐가 구분된다.  
distance metric은 hierarchical behavior를 설명한다. Linzen et al., 2016에서는 주어와 동사 사이에 나타나는 attractor가 존재한다고 언급되었는데, 동사는 tree 내에서 어떤 attractor보다 주어와 가깝다. 이러한 관계를 통해 hierarchical한 구조가 embedding된다는 것을 알 수 있다.

<center><img width="206" alt="2021-03-28" src="https://user-images.githubusercontent.com/53667002/112919166-7f534f00-9141-11eb-95b3-dc0eb4c5ea40.png"></center>

nn이 parse tree를 embedding하면 매우 많은 종류의 정보를 encoding해야 하기 때문에 아마 representation space 전체를 사용하지 않을 것이다. 본 연구의 probe는 word representation space의 선형 변형을 학습하는데 변형된 공간은 모든 문장에 대한 parse tree를 embedding한다. 이는 syntax를 encoding하데에 사용된 representation space의 일부분을 찾는 것과 같다. 즉, 이는 tree metric에 가장 잘 맞는 original space에서의 distance를 찾는 것이다.


### 3.1 The structural probe

본 논문에서 제안하는 structural probe는 pares tree가 어떻게 신경망의 hidden state에서 embedding될 수 있는지에 대해 간단한 가설을 통해 진행된다. parse tree는 아래와 같이 생겼다:


<center><img width="326" alt="2021-03-21 (3)" src="https://user-images.githubusercontent.com/53667002/112919316-d8bb7e00-9141-11eb-87df-4bdadb271128.png"></center>

### 3.1.1 Trees as distances and norms

본 연구에서는 불연속 구조인 parse tree가 연속적인 vector의 sequence로 encoding되는지를 결정하는 것이 핵심이다. 여기에서 제시되는 가설은 우선 vector distance가 pares tree로 encoding되어 word representation의 linear transformation이 존재한다는 것이다. parse tree에는 두 단어에 대한 path metric d(w_i, w_j)가 있다. 따라서 tree를 embedding하는 것은 tree에 의해 정의된 distance metric을 embedding하는 것과 같다.
따라서 nn을 통해 생성된 vector의 distance가 parse tree의 distance와 같다면 이는 nn이 문장 내 잠재된 tree를 embedding한 것으로 볼 수 있다. 

<center><img width="330" alt="2021-03-21 (4)" src="https://user-images.githubusercontent.com/53667002/112919342-e7099a00-9141-11eb-83bf-ad9cf7ed9f73.png"></center>

위 그림을 보면 distance-1에 chef와 was가 걸리는 것을 볼 수 있다. 따라서 여기에서 chef와 was가 각각 w_i, w_j가 되고 두 단어 사이의 관계를 edge를 통해 embedding하는 것이다. chef가 was에 대한 직접적인 피지배소이기 때문에 chef와 was는 distance-1이지만 store과 was는 was에서 chef를 거쳐 ran과 to를 지나 이어지기 때문에 distance-4가된다. 즉, distance-4가 되기 위해서는 arc를 타고 들어가야 distance를 확인할 수 있다.

> The syntax distance hypothesis  
syntax distance 가설은 선형 변형(lenear transformation) $B$에 대한 것으로, 이는 L2 distance를 통해vector간의 거리가 parse tree를 표상한다는 것이다(${B^T}B$). 여기에서 L2 distance를 사용한 이유는 그냥 이게 제일 잘 돼서. 아래는 위 예시가 vector space에 표상된 그림으로, 3차원 word representaion을 2차원 공간에 encoding한 것이다.($B \in \mathbb{R}^{2\times3}$)  
<img width="228" alt="2021-03-27 (1)" src="https://user-images.githubusercontent.com/53667002/112919879-ddccfd00-9142-11eb-9d0e-ede02fbbdf53.png">  
2차원 공간에 encoding된 것을 선형 변형시키면 tree가 보인다:  
<img width="222" alt="2021-03-27 (2)" src="https://user-images.githubusercontent.com/53667002/112919925-ee7d7300-9142-11eb-96a3-093cfb672fb7.png">

### 3.1.2 Finding a parse tree-encoding distance metric
잠재적 tree-encoding distance는 선형변형($B \in \mathbb{R}^{k\times n}$)을 통해 parameter화 된다.

\begin{matrix}
{\lVert  h_i-h_j \rVert}_B^2=(B(h_i-h_j))^T(B(h_i-h_j))
\end{matrix}

$Bh$는 word representation의 선형변형된 것으로, parse tree node representation과 같다. 이는 L2 distance를 positive semi-definite matrix(vector x값에 따라 0이상의 양수값으로 만드는 행렬) ($A={B^T}B$) 로 parameter화된 vector space에서 찾는 것과 같다.

\begin{matrix}
{\lVert  h_i-h_j \rVert}_A^2=(h_i-h_j)^TA(h_i-h_j)
\end{matrix}

인간이 만든 parse tree의 distance와 nn이 예측한 parse tree distance의 차이를 최소화한다.

코퍼스 내 문장 $s_\ell$과 $\frac{1}{\left| s_{\ell} \right|}^2$ index$\ell$은 각 문장의 단어 쌍 수를 normalize한다. 중요한 것은 tree distance와 squared distance $\lVert {h_i}-{h_j} \rVert_B^2$ 의 차이를 최소화해야 한다는 것이다. (실제 vector 거리 ${\lVert h_i - h_j \rVert}_{B}$가 항상 실제 parse tree distance에서 벗어나지만 encoding된 tree정보는 일치하며, squared distance로 최적화하는 것이 가장 효과적이기 때문이다.)

### 3.1.3 Finding a parse depth-encoding norm

parse tree에서의 edge는 parse tree에 있는 단어들의 depth에 의해 결정된다. governance relationship(지배관계)에서 더 깊은 node는 지배 단어이다. parse tree에서의 depth는 norm과 같다. 이는 tree의 node를 결정한다. 이 tree depth norm을 ${\lVert w_i \rVert}$라고 한다.

우리는 squared L2 vector norm ${\lVert Bh_i \rVert}\_{2}^{2}$ 으로 tree depth norm이 encoding되는 선형 변형이 존재한다는 것을 전제한다. 그리고 거리 가설처럼 depth norm가설은 best-approxiamted된 선형 변형을 찾을 수 있다.

\begin{matrix}
\underset{B}{\min}\sum_{\ell}\frac{1}{\left| s_{\ell} \right|}\sum_i(\lVert w_i \rVert-{\lVert Bh_i \rVert}^2)
\end{matrix}
