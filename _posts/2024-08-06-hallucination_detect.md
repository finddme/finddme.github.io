---
title: "Semantic Entropy: Summary of Two Papers on LLM <span style='font-weight: bold; font-family: Computer Modern;'>Hallucination Detection</span> ğŸ˜±"
category: LLM / Multimodal
tag: Multimodal
---







* ëª©ì°¨
{:toc}










# [1st Paper] Detecting hallucinations in large language models using semantic entropy

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” entropy-based uncertainty estimatorsë¥¼ í†µí•´ modelì´ hallucinationì„ ì–¼ë§ˆë‚˜ ë°˜í™˜í•˜ëŠ” modelì¸ì§€ ê°ì§€í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤. 

ì´ë•Œ ë¶ˆí™•ì‹¤ì„±ì€ ì˜ë¯¸ ìˆ˜ì¤€ì—ì„œ ê³„ì‚°ë˜ì–´ì•¼ í•œë‹¤. ì„œë¡œ ë‹¤ë¥¸ token sequenceë„ ë™ì¼í•œ ì˜ë¯¸ë¥¼ ê°–ëŠ” ê²½ìš°ê°€ ìˆê¸° ë•Œë¬¸ì— ì¶œë ¥ëœ token ìˆ˜ì¤€ì—ì„œ ë¶ˆí™•ì‹¤ì„±ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ì í•©í•˜ì§€ ì•Šë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "Paris", "It's Paris", "The capital of France is Paris"ë¼ëŠ” ë‹µë³€ì€ ëª¨ë‘ ë™ì¼í•œ ì˜ë¯¸ë¥¼ ë‚´í¬í•˜ëŠ”ë° ì´ë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ì´ ì„¸ ë¬¸ì¥ì„ ì¶œë ¥í•œ ëª¨ë¸ì˜ ë¶ˆí™•ì‹¤ì„±ì€ ë†’ê²Œ ì¸¡ì •ë  ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ë¬¸ì—ì„œëŠ” generationë“¤ì„ ì˜ë¯¸ì ìœ¼ë¡œ clusteringí•œ ì´í›„ ì˜ë¯¸ì  ê³µê°„ì—ì„œ ë¶ˆí™•ì‹¤ì„±ì„ ì¶”ì •í•˜ëŠ” semantic entropy (SE)ë¥¼ ì œì•ˆí–ˆë‹¤.

ì¦‰, ëª¨ë¸ì´ ìì‹  ìˆê²Œ ëŒ€ë‹µí•˜ì§€ë§Œ í‘œí˜„ì€ ë‹¤ì–‘í•˜ê²Œ í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì—ì„œëŠ” ë¶ˆí™•ì‹¤ì„±ì„ ë‚®ê²Œ ë³´ê³ , ë‹µë³€ì˜ ì˜ë¯¸ ìì²´ê°€ ì—¬ëŸ¬ ê°€ì§€ë¡œ í•´ì„ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì—ì„œëŠ” ë¶ˆí™•ì‹¤ì„±ì„ ë†’ê²Œ í‰ê°€í•˜ëŠ” ë°©ì‹ì´ë‹¤.

## 1. Semantic Entropy

ëª¨ë¸ì˜ generationì—ëŠ” ì•„ë˜ì™€ ê°™ì€ ë‘ ê°€ì§€ ë¶ˆí™•ì‹¤ì„±ì´ ìˆë‹¤:

- Lexical and Syntactic Uncertainty: ì–´íœ˜ ë° êµ¬ë¬¸ì  ë¶ˆí™•ì‹¤ì„±ì´ë‹¤. ì´ëŠ” ë¬¸ì¥ì„ ì–´ë–¤ ë‹¨ì–´ë¡œ ì–´ë–»ê²Œ êµ¬ì„±í• ì§€ì™€ ê´€ë ¨ëœ ë¶ˆí™•ì‹¤ì„±ì´ë‹¤.
- Semantic Uncertainty: ì˜ë¯¸ì  ë¶ˆí™•ì‹¤ì„±ì´ë‹¤. ì´ëŠ” ìƒì„±ëœ ë¬¸ì¥ì´ ë‚´í¬í•˜ëŠ” ì˜ë¯¸ì™€ ê´€ë ¨ëœ ë¶ˆí™•ì‹¤ì„±ì´ë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‘ ë¶ˆí™•ì‹¤ì„±ì„ ì •í™•íˆ êµ¬ë¶„í•´ì•¼í•œë‹¤ê³  ê°•ì¡°í•˜ë©° Semantic Entropyë¥¼ ì œì•ˆí•˜ì˜€ë‹¤. Semantic EntropyëŠ” LLMì´ ìƒì„±í•˜ëŠ” í…ìŠ¤íŠ¸ì—ì„œ ë°œìƒí•˜ëŠ” ì˜ë¯¸ì  ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤.

êµ¬í˜„ ì½”ë“œëŠ” [https://github.com/jlko/semantic_uncertainty/blob/master/semantic_uncertainty/uncertainty/uncertainty_measures/semantic_entropy.py](https://github.com/jlko/semantic_uncertainty/blob/master/semantic_uncertainty/uncertainty/uncertainty_measures/semantic_entropy.py)ì— ê¹”ë”í•˜ê²Œ ì •ë¦¬ë˜ì–´ ìˆë‹¤.

Semantic Entropyê³„ì‚°ì€ í¬ê²Œ 3 stepìœ¼ë¡œ ì§„í–‰ëœë‹¤.

### 1.1 Generation
   - ì´ ë‹¨ê³„ì—ì„œëŠ” inputì— ëŒ€í•´ [different random seed(sampling parameter)](https://finddme.github.io/llm%20/%20multimodal/2024/08/06/hallucination_detect/#additional-information-sampling-parameters)ë¡œ ì—¬ëŸ¬ outputì„ ìƒì„±í•œë‹¤. ì£¼ë¡œ temperature, top-K samplingì„ ì‚¬ìš©í–ˆë‹¤.
   - ìƒì„±ëœ ê° outputì— ëŒ€í•œ í™•ë¥ ë„ í•¨ê»˜ ê¸°ë¡í•œë‹¤.
     
### 1.2 Semantic Clustering
   - ì´ ë‹¨ê³„ì—ì„œëŠ” ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ ë¬¸ì¥ë“¤ì„ clusteringí•œë‹¤.  

   - $s_a$ì™€ $s_b$ë¼ëŠ” ë¬¸ì¥ì´ ìˆì„ ë•Œ ë‘ ë¬¸ì¥ì´ ì–‘ë°©í–¥ìœ¼ë¡œ ì„œë¡œ í•¨ì˜í•œë‹¤ë©´ ë‘ ë¬¸ì¥ì€ ë™ì¼í•œ ì˜ë¯¸ë¥¼ ì „ë‹¬í•œë‹¤ê³  ê°€ì •í•œë‹¤. ì´ëŸ¬í•œ ê°€ì • í•˜ì— ì„œë¡œ í•¨ì˜ ê´€ê³„ì— ë†“ì€ ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ clusterë¡œ ë¬¶ëŠ”ë‹¤.
   - ìš°ì„  ì—¬ëŸ¬ê°œì˜ ë¬¸ì¥ì„ LLMìœ¼ë¡œë¶€í„° ìƒì„±í•˜ê³  ì´ë“¤ì„ ì˜ë¯¸ì ìœ¼ë¡œ clusteringí•œ í›„ ê° clusterë“¤ì˜ ë°œìƒ í™•ë¥ ì„ ê³„ì‚°í•œë‹¤. (ì‹¤í—˜ ì‹œì—ëŠ” Monte Carlo ë°©ì‹ìœ¼ë¡œ Nê°œì˜ ë¬¸ì¥ì„ samplingí•˜ê³  ì´ì— ëŒ€í•œ clusterë¥¼ ìƒì„±í•˜ì—¬ ì§„í–‰í•œë‹¤.)
   - cluster í™•ë¥ ì€ í•´ë‹¹ clusterì— ì†í•œ ëª¨ë“  generation $s$ì˜ í™•ë¥ ì„ í•©í•œ ê°’ì´ë‹¤. generation $s$ì˜ í™•ë¥ ì€ input $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ìƒì„±ëœ tokenë“¤($t_1$,...$t_n$)ì´ ì§€ë‹Œ í™•ë¥  ê°’ì˜ ê³±ì´ëœë‹¤.

### 1.3 Semantic Entropy Estimation
   > entropyëŠ” ì •ë³´ ì´ë¡ ì—ì„œ ì£¼ì–´ì§„ í™•ë¥  ë¶„í¬ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œì´ë‹¤.
   
   - ì´ ë‹¨ê³„ì—ì„œëŠ” ì˜ë¯¸ì  ë¶ˆí™•ì‹¤ì„±ì„ ê³„ì‚°í•œë‹¤.
   - modelì´ ë‹¤ì–‘í•œ ì˜ë¯¸ë¥¼ ìƒì„±í• ìˆ˜ë¡ ë¶ˆí™•ì‹¤ì„±ì€ ì»¤ì§„ë‹¤. ìƒì„±ëœ ë¬¸ì¥ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±ì´ í´ìˆ˜ë¡ ëª¨ë¸ì´ hallucinationì„ ë‚¼ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.
   - input $x$ì— ëŒ€í•œ LLMì˜ generationë“¤ì˜ ì˜ë¯¸ê°€ ì–¼ë§ˆë‚˜ ë¶ˆí™•ì‹¤í•œì§€ ì •ëŸ‰í™”í•œë‹¤.
   - ìš°ì„  ì—¬ëŸ¬ê°œì˜ ë¬¸ì¥ì„ LLMìœ¼ë¡œë¶€í„° ìƒì„±í•˜ê³  ì´ë“¤ì„ ì˜ë¯¸ì ìœ¼ë¡œ clusteringí•œ í›„ ê° clusterë“¤ì˜ ë°œìƒ í™•ë¥ ì„ ê³„ì‚°í•œë‹¤. cluster ë°œìƒ í™•ë¥ ì€ clusterì— ì†í•œ ë¬¸ì¥ë“¤ì˜ í™•ë¥ ì˜ í•©ì´ë‹¤.
   - semantic cluster $C$ì˜ ë°œìƒ í™•ë¥  ê¸°ë°˜ìœ¼ë¡œ ê° clusterì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •í•˜ëŠ”ë° ì´ë•Œ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ entropyì´ë‹¤. clusterë“¤ì´ì´ ì–¼ë§ˆë‚˜ ê³¨ê³ ë£° ë¶„í¬ë˜ëŠ”ì§€ì— ë”°ë¼ entropyì˜ ê°’ì´ ë‹¬ë¼ì§„ë‹¤.

## 2. Entailment estimator

Entailment estimatorëŠ” ì•ì„œ ê¸°ìˆ í•œ Semantic Entropyê³„ì‚° ê³¼ì • ì¤‘ Semantic Clusteringì— ì‚¬ìš©ëœë‹¤. ì˜ë¯¸ì ìœ¼ë¡œ ë¬¸ì¥ì„ clusteringí•˜ê¸° ìœ„í•´ì„œëŠ” ê° ë¬¸ì¥ë“¤ì´ ì„œë¡œ entailment([í•¨ì˜](https://finddme.github.io/linguistik%20%7C%20germanistik/2020/12/13/Pragmatik/#--implikationen%ED%95%A8%EC%9D%98)) ê´€ê³„ì— ìˆëŠ”ì§€ ì•Œì•„ì•¼ í•œë‹¤.

í•¨ì˜ ê°ì§€ ë°©ë²•ì€ ë‘ ê°€ì§€ì´ë‹¤.:

### 2.1 Instruction-tuned LLM ì‚¬ìš©
  LLaMA 2, GPT-3.5 (Turbo 1106), GPT-4ì™€ ê°™ì€ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‘ ë‹µë³€ ì‚¬ì´ì˜ í•¨ì˜ë¥¼ ì˜ˆì¸¡í•œë‹¤. ì‚¬ìš©ëœ promotëŠ” ì•„ë˜ì™€ ê°™ë‹¤. ì§ˆë¬´ì— ëŒ€í•œ ë‘ ë‹µë³€ì„ ì œì‹œí•˜ê³  ë‘ ë‹µë³€ì˜ ê´€ê³„ë¥¼ ë¬»ëŠ”ë‹¤. ë‘ ë‹µë³€ì˜ ê´€ê³„ëŠ” entailment(í•¨ì˜), contradiction(ëª¨ìˆœ), neutral(ì¤‘ë¦½) ì¤‘ í•˜ë‚˜ë¡œ ì„ íƒí•˜ë„ ìš”ì²­í•œë‹¤.

  ```
  We are evaluating answers to the question {question}
  Here are two possible answers:
  Possible Answer 1: {text1}
  Possible Answer 2: {text2}
  Does Possible Answer 1 semantically entail Possible Answer 2?
  Respond with entailment, contradiction, or neutral.
  ```

### 2.2  embedding similarity ì¸¡ì • ë°©ì‹ ì‚¬ìš©

embedding similarityë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‘ ë¬¸ì¥ì´ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ íŒë‹¨í•˜ëŠ” taskë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” MNLI datasetì„ í•™ìŠµí•œ DeBERTa-large modelì„ í†µí•´ Q-A1, Q-A2 ì‚¬ì´ì— í•¨ì˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•œë‹¤. í•´ë‹¹ ë°©ë²•ì€ LLMì„ ì‚¬ìš©í•œ ë²™ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ë‚®ë‹¤.


## 3. Experiment

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‘ ê°€ì§€ taskì— ëŒ€í•´ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤.

### 3.1 QA and math task
  - dataset:
    - BioASQ: ìƒëª…ê³¼í•™ ë¶„ì•¼ì˜ QA dataset
    - SQuAD: ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” dataset
    - TriviaQA: í€´ì¦ˆ í˜•ì‹ì˜ ì§ˆë¬¸-ë‹µë³€ dataset
    - SVAMP: ìˆ˜í•™ì  ì¶”ë¡ ì´ í•„ìš”í•œ ì´ˆë“±í•™êµ ìˆ˜ì¤€ì˜ ë‹¨ì–´ ë¬¸ì œ dataset
    - NQ-Open: êµ¬ê¸€ ê²€ìƒ‰ì—ì„œ ì¶”ì¶œëœ ì‹¤ì œ ì§ˆë¬¸ë“¤ì„ ë‹¤ë£¬ dataset
  - model: FalconInstruct (7B/40B), LLaMA 2 Chat(7B/13B/70B), Mistral Instruct (7B)
  - Entailment estimator: LLM
  - method: modelì—ê²Œ context ì—†ì´ ë‹µë³€í•˜ë„ë¡ ìš”ì²­í•˜ì—¬ hallucinationì„ í•œ í›„ ë¶ˆí™•ì‹¤ì„± ì˜ˆì¸¡ ì‹¤í—˜ ì§„í–‰.
  - result:
    <center><img width="600" src="https://github.com/user-attachments/assets/cdf6f747-c52b-4b8f-bfdd-23b9c9583040"></center>
    <center><em style="color:gray;">paper</em></center><br>

### 3.2 biography-generation task
  - dataset: FactualBio(ì‹¤ì¡´ ì¸ë¬¼ë“¤ì˜ ì „ê¸°ë¥¼ ë‹´ì€ dataset)
  - Entailment estimator: DeBERTa
  - method:
    1. specific factual claimì„ ê¸°ì¤€ìœ¼ë¡œ datasetì„ ë¶„í•´. ì´ë•Œ ì‚¬ìš©ëœ promptëŠ” ì•„ë˜ì™€ ê°™ë‹¤:
      ```
      "Please list the specific factual propositions included in the answer above. Be complete and do not leave any factual claims out. Provide each claim as a separate sentence in a separate bullet point."
      ```
    2. ë¶„í•´ëœ ì‚¬ì‹¤ì  ì£¼ì¥ì— ëŒ€í•´ í•´ë‹¹ ë¬¸ì¥ë“¤ì´ ìƒì„±ë  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ 6ê°œë¥¼ ìƒì„±
    3. ëª¨ë¸ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ë‹µë³€ì„ 3ë²ˆ ìƒì„±
      ```    
      "You see the sentence: {proposition}. Generate a list of three questions, that might have generated the sentence in the context of the preceding original text."
      ```
    4. semantic entropy ê³„ì‚°-> ë‹µë³€ 3ê°œ + original factual claimì— ëŒ€í•´ semantic entropyë¥¼ ê³„ì‚°
  - result:
    <center><img width="600" src="https://github.com/user-attachments/assets/a0cdb4cb-e6f2-416f-9cbb-f8d926cd8391"></center>
    <center><em style="color:gray;">paper</em></center><br>
    

# [Additional Information] Sampling Parameters

Semantic Entropyê³„ì‚° ê³¼ì • ì¤‘ Generateion ë‹¨ê³„ì— ë“±ì¥í•œ "different random seed(sampling parameter)" ë¶€ë¶„ì˜ ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ í•´ë‹¹ ë‚´ìš©ì„ ì •ë¦¬í•œ ë¶€ë¶„ì´ë‹¤. 

Sampling Parameterë¡œëŠ” ëŒ€í‘œì ìœ¼ë¡œ Temperatureê°€, Top-K, Top-Pê°€ ìˆëŠ”ë° ì´ë“¤ì€ LLMì˜ ì¶œë ¥ì„ ì œì–´í•˜ëŠ” parameterì´ë‹¤. í•´ë‹¹ parameterë¥¼ í†µí•´ ëª¨ë¸ ì¶œë ¥ì˜ ì¼ê´€ì„±<->ë‹¤ì–‘ìƒ ì •ë„ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤.

## Temperature

TemperatureëŠ” log probabilityë¥¼ ì¡°ì •í•˜ì—¬ ì˜ˆì¸¡ ì‹ ë¢°ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” parameterì´ë‹¤. ì¦‰, log probability scalingì„ í†µí•´ model predictionì— ëŒ€í•œ ë¬´ì‘ìœ„ì„±<->ì‹ ë¢°ì„±ì˜ ì •ë„ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆëŠ” parameterì´ë‹¤. Temperatureì˜ ê°’ì´ ë‚®ì„ìˆ˜ë¡ ë” ì§‘ì¤‘ì ì´ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë‹µë³€ì„ í•œë‹¤.

- 1 ì´í•˜ì˜ Temperature
  - ëª¨ë¸ì´ íŠ¹ì • tokenì„ ë” í™•ì‹  ìˆê²Œ ì˜ˆì¸¡í•˜ì—¬ í™•ë¥  ë¶„í¬ê°€ ë§¤ìš° ê·¹ëª…í•˜ê²Œ ë‚˜íƒ€ë‚˜ê²Œ ëœë‹¤.
  - temperatureê°€ ë‚®ì„ìˆ˜ë¡ next tokenìœ¼ë¡œ ì˜ˆì¸¡í•œ tokenì˜ probability massê°€ ì§‘ì¤‘ëœë‹¤.
  - ì´ ê²½ìš°, ë” ì¼ê´€ì„± ìˆëŠ” output ìƒì„±ì€ ê°€ëŠ¥í•˜ì§€ë§Œ ë‹¤ì–‘ì„±ê³¼ ì°½ì˜ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤.

- 1 ì´ìƒì˜ Temperature
  - ëª¨ë¸ì˜ í™•ì‹ ì´ ê°ì†Œí•œë‹¤. ë”°ë¼ì„œ next tokenìœ¼ë¡œ ì˜ˆì¸¡í•œ tokenë“¤ì— ëŒ€í•œ í™•ë¥  ì§ˆëŸ‰ì´ ê³ ë¥´ê²Œ í¼ì ¸ í™•ë¥  ë¶„í¬ê°€ í‰íƒ„í•´ì§„ë‹¤.
  - ì´ ê²½ìš°, ëª¨ë¸ì´ ë‹¤ì–‘í•œ tokenì„ samplingí•˜ê¸° ë•Œë¬¸ì— ë‹¤ì–‘í•œ output ìƒì„±ì´ ê°€ëŠ¥í•˜ë‹¤.
  - í•˜ì§€ë§Œ temperatureê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ ë¬¸ë§¥ì— ë§ì§€ ì•ŠëŠ” ë¬¸ì¥ì„ ìƒì„±í•  ìˆ˜ë„ ìˆë‹¤. 

- Temperature scaling ì‘ë™ ê³¼ì •:
  1. modelì€ modelì´ ì‚¬ìš©í•˜ëŠ” tokenizerì˜ vocabì— ìˆëŠ” tokenì„ í˜„ì¬ time stepì— ëŒ€í•œ next tokenìœ¼ë¡œ ì˜ˆì¸¡í•œë‹¤. ì´ë•Œ ê° tokenë“¤ì—ê²ŒëŠ” ì •ê·œí™” ë˜ì§€ ì•Šì€ log probability scoreê°€ ë¶€ì—¬ëœë‹¤.
  2. ì´ log probability scoreë¥¼ ì„¤ì •ëœ Temperature ê°’ìœ¼ë¡œ ë‚˜ëˆˆë‹¤. $log_prob_scaled = log_prob / temperature$
  3. ìœ„ ê²°ê³¼, Temperatureê°€ 1 ì´í•˜ì¼ ê²½ìš°ì—ëŠ” log probabilityê°€ ê·¹ë‹¨ì ìœ¼ë¡œ ë³€í•œë‹¤. ì¦‰, ì˜ˆì¸¡ê°’ì´ ë†’ì•˜ë˜ ì• ë“¤ì€ ë” ë†’ì•„ì§€ê³  ë‚®ì•˜ë˜ ì• ë“¤ì€ ë” ë‚®ì•„ì§„ë‹¤. ì´ì™€ ê°™ì´ ë†’ì€ í™•ë¥ ê³¼ ë‚®ì€ í™•ë¥  ê°’ì˜ ì°¨ì´ë¥¼ ì¦í­ì‹œì¼œ ê°€ëŠ¥ì„± ë†’ì€ ëª‡ëª‡ tokenì— í™•ë¥ ì„ ë” ì§‘ì¤‘ì‹œí‚¨ë‹¤.
  4. Temperatureê°€ 1 ì´ìƒì¼ ê²½ìš°ì—ëŠ” ë°˜ëŒ€ë¡œ log probabilityê°€ í‰íƒ„í•˜ê²Œ ë³€í•˜ì—¬ ëŒ€ë¶€ë¶„ 0ì— ê°€ê¹Œì›Œì§„ë‹¤. ì´ë¥¼ í†µ tokenê°„ì˜ log probabilityë¥¼ ì¤„ì—¬ ë¹„êµì  ë‚®ì€ í™•ë¥ ì„ ê°€ì§„ tokenë“¤ì—ê²Œë„ ê¸°íšŒë¥¼ ì£¼ê²Œ ëœë‹¤.
  5. Temperature scaling ì´í›„ softmax í•¨ìˆ˜ë¥¼ í†µê³¼ì‹œì¼œ scalingëœ log probabilityì˜ í•©ì´ 1ì´ ë˜ë„ë¡ í•œë‹¤.


## Top-K

tokenì„ ì˜ˆì¸¡í•˜ëŠ” ê° stepë§ˆë‹¤ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²°ê³¼ì— ëŒ€í•´ í™•ë¥ ì„ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ kê°œì˜ token ì•ˆì—ì„œ ì¶œë ¥í•œ tokenì„ ì„ íƒí•˜ë„ë¡ ì œí•œí•˜ëŠ” ê²ƒì´ë‹¤. ì´ì™€ ê°™ì´ ëª¨ë¸ì˜ vocabì„ ì œí•œí•¨ìœ¼ë¡œì¨ ë¹„ë…¼ë¦¬ì ì´ê±°ë‚˜ ë¬´ì˜ë¯¸í•œ ì¶œë ¥ì„ ì œì–´í•  ìˆ˜ ìˆë‹¤. 

K=5ì¸ ê²½ìš°:
- ëª¨ë¸ ì˜ˆì¸¡ tokenì˜ í™•ë¥  ë¶„í¬ì—ì„œ ìƒìœ„ 5ê°œì˜ tokenìœ¼ë¡œ vocabì„ ì œí•œ
- 5ê°œì˜ tokenì— ëŒ€í•´ í™•ë¥ ì„ re-normalizeí•˜ì—¬ ê° tokenì— ëŒ€í•œ í™•ë¥ ê°’ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ í•œë‹¤.
- ì¬ì •ê·œí™”ëœ ë¶„í¬ì—ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ samplingí•œë‹¤.

## Top-P (Nucleus sampling)

ê°„ë‹¨í•˜ê¸° ë§í•˜ìë©´ samplingëœ tokenë“¤ì´ ì§€ë‹Œ í™•ë¥ ê°’ì˜ í•©ì— ëŒ€í•´ thresholdë¥¼ ê±°ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì™€ ê°™ì€ tokenì´ ìˆì„ ë•Œ Top-Pê°€ 0.8ì´ë©´ (ê°ˆë¹„ì°œ, í†µíƒœì „, ì¡ì±„) ì¤‘ì— ë‹¤ìŒ ë‹¨ì–´ë¥¼ samplingí•˜ëŠ” ê²ƒì´ë‹¤. 

```
ê°ˆë¹„ì°œ: 0.4
ì¡ì±„: 0.1
í†µíƒœì „: 0.3
ë–¡ê°ˆë¹„: 0.05
```

## Sampling Parameter ì‘ë™ ë°©ì‹

> ì´ parameterë¥¼ ì˜ ì¡°ì •í•˜ê¸° ìœ„í•´ Greedy Samplingê³¼ Random Sampling ê°œë…ì„ ë³µê¸°í•˜ëŠ” ê²ƒì´ ë„ì›€ëœë‹¤.<br>
> - Greedy Sampling<br>
> <br>
> ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ê²ƒì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ ê²½ìš°ëŠ” `Top-K=1, Temperature=1.0`ì´ë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë‹¤ìŒ tokenìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ê°€ì¥ ë†’ì€ í™•ë¥ ì¸ ê²ƒì„ ì„ íƒí•œë‹¤. ì´ì™€ ê°™ì€ ë°©ë²•ì€ ëª¨ë¸ ì¶œë ¥ì˜ ì¼ê´€ì„±ì€ ìœ ì§€í•  ìˆ˜ ìˆì§€ë§Œ ì°½ì˜ì„± í˜¹ì€ ë‹¤ì–‘ì„±ì„ ê¸°ëŒ€í•˜ê¸° ì–´ë µë‹¤. <br>
> <br>
> - Random Sampling<br>
> <br>
> ë§ ê·¸ëŒ€ë¡œ ë¬´ì‘ìœ„ ì„ íƒì´ë‹¤. ì´ ê²½ìš°ëŠ” `Top-K=50~100, Temperature=1.5~2.0`ì´ë‹¤. ì´ëŠ” ëª¨ë¸ì´ ì˜ˆì¸¡í•œ token ì¤‘ í™•ë¥  ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ top 50~100 ì‚¬ì´ì˜ tokenì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ëŠ” ë°©ë²•ì´ë‹¤.<br>

```
Temperature=0.8, Top-K=40, Top-P=0.8
```

1. modelì´ ì „ì²´ vocabì— ëŒ€í•´ ì •ê·œí™”ë˜ì§€ ì•Šì€ log probabilityë¥¼ ê³„ì‚°í•œë‹¤. (vocab ì „ì²´ì— ëŒ€í•´ í™•ë¥ ê°’ì´ í• ë‹¹ë¨.)
2. Temperature ì ìš©
   - ëª¨ë“  tokenë“¤ì˜ log probabilityë¥¼ 0.8ë¡œ ë‚˜ëˆ„ì–´ scalingí•œë‹¤.
   - 1ë³´ë‹¤ ì‘ì€ ê°’ìœ¼ë¡œ ë‚˜ëˆ„ì—ˆê¸° ë•Œë¬¸ì— ê·¹ë‹¨ì ì¸ í™•ë¥  ë¶„í¬ê°€ ëœë‹¤.
3. Top-K ì ìš©
   - temperatuer scaling ì ìš©ëœ í™•ë¥ ì¤‘ ìƒìœ„ 40ê°œì˜ tokenì„ ì„ íƒí•œë‹¤.
4. Top-P ì ìš©
   - ì„ íƒëœ 40ê°œì˜ tokenì— ëŒ€í•´ ìƒìœ„ í™•ë¥  tokenë¶€í„° í•©ì‚°í•˜ì—¬ í™•ë¥  ì§ˆëŸ‰ì´ 80%ì´ ë˜ëŠ” tokenê¹Œì§€ë§Œ sample tokenì— í¬í•¨ì‹œí‚¨ë‹¤.
5. ìœ„ scaling ê³¼ì •ì„ ëª¨ë‘ ê±°ì¹œ í›„ ë‚¨ì€ tokend 20ê°œë¼ê³  ê°€ì •í•œë‹¤ë©´, 20ê°œì˜ tokenë“¤ì— ëŒ€í•œ log probabilityë¥¼ renormalizeí•˜ì—¬ í™•ë¥ ì´ 1ì´ ë˜ë„ë¡ ë§Œë“ ë‹¤.
6. ì´ í™•ë¥ ë¶„í¬ì—ì„œ next tokenì„ samplingí•œë‹¤.

# [2nd Paper] Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs

Farquhar et al.ì˜ ë…¼ë¬¸ê³¼ ë™ì¼í•˜ê²Œ token ìˆ˜ì¤€ì´ ì•„ë‹Œ ì˜ë¯¸ ìˆ˜ì¤€ì—ì„œì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •í•˜ì§€ë§Œ Farquhar et al.ì´ ì œì•ˆí•œ ë°©ë²•ì€ input queryì— ëŒ€í•´ ì—¬ëŸ¬ë²ˆì˜ model generation ê²°ê³¼ë¥¼ ìš”êµ¬í•˜ëŠ”ë° ì¼ë°˜ì ìœ¼ë¡œ 5-10ë²ˆ ì‚¬ì´ì˜ generation ê²°ê³¼ê°€ í•„ìš”í•˜ë‹¤. ì´ëŠ” 5-10ë°°ì˜ ê³„ì‚° ë¹„ìš©ì´ ì¶”ê°€ë¡œ ë°œìƒí•¨ì„ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì— ì‹¤ìš©ì ì´ì§€ ì•Šë‹¤. ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë³´ë‹¤ ì‹¤ìš©ì ì¸ LLM hallucination detection ë°©ì‹ì„ ì œì•ˆí•œë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” ë°©ë²•ë¡ ì˜ í•µì‹¬ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:

- hidden stateì—ì„œ semantic entropyë¥¼ í¬ì°©í•˜ë„ë¡ í•™ìŠµëœ linear probe Semantic Entropy Probes (SEPs)ë¥¼ ì œì•ˆí•œë‹¤.
- semantic entropyê°€ single model generationì˜ hidden sate ìƒíƒœì— encodingë˜ì–´ ìˆìœ¼ë©°, ì´ë¥¼ linear probeë¥¼ í†µí•´ ì¶”ì¶œ ê°€ëŠ¥í•¨ì„ ì…ì¦í•œë‹¤.
- ablation studyë¥¼ í†µí•´ ë‹¤ì–‘í•œ model, task, layer, token positionì— ëŒ€í•´ SEP ì„ ëŠ¥ì„ ì‹¤í—˜í•œë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ëª¨ë¸ ë‚´ë¶€ stateë“¤ì´ tokenì„ ìƒì„±í•˜ê¸° ì „ì—ë„ ì˜ë¯¸ì  ë¶ˆí™•ì‹¤ì„±ì„ ì•”ë¬µì ìœ¼ë¡œ í¬ì°©í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤ê³  í•œë‹¤.
- SEPê°€ hallucinationì„ ì˜ˆì¸¡í•˜ëŠ”ë° ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë©°, ì´ì „ ì—°êµ¬(Farquhar et al.)ì—ì„œ ì œì•ˆëœ probeë³´ë‹¤ ë” ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë°©ë²•ì´ë‹¤.

## 1.  Semantic Entropy Probes

ì•ì„œ ì„¤ëª…í•œ Semantic Entropy(Farquhar et al.)ëŠ” computational costê°€ ë†’ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì„ í˜• íšŒê·€ ëª¨ë¸(Linear Probes)ì„ ì‚¬ìš©í•˜ì—¬ hidden stateë¥¼ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ì  Entropyë¥¼ ì˜ˆì¸¡í•œë‹¤. ì´ëŠ” hidden stateë‹¨ì—ì„œ í•´ê²°ë˜ê¸° ë•Œë¬¸ì— multiple response samplingì´ ìš”êµ¬ë˜ì§€ ì•Šì•„ ê³„ì‚° ë¹„ìš©ì´ ì¤„ì–´ë“ ë‹¤.

### 2.1 Training SEPs

SE probsë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ model generationì˜ hidden stateì™€ semantic entropy ìŒì˜ dataset $(h_p^l(x), H_SE(x))$ë¥¼ ìˆ˜ì§‘í•œë‹¤. ì´ë•Œ $x$ëŠ” inputì´ê³ , $h_p^l(x) \in \mathbb{R}^d$ì€ input $x$ì— ëŒ€í•œ ì¶œë ¥ì„ ìƒì„±í•  ë•Œì˜ tokenë“¤ì˜ ìœ„ì¹˜ $p$ì™€ layer $l$ì´ê³ ($d$ëŠ” hidden state dimension), $H_SE(x)\in \mathbb{R}$ì€ semantic entropyì´ë‹¤. dataset ìˆ˜ì§‘ ì‹œ, input $x$ì— ëŒ€í•œ high-likelihood model responseë“¤ì„ greedy samplingí•˜ê³  ê° layerì™€ token positionì— í•´ë‹¹í•˜ëŠ” hidden stateë“¤ì„ ì €ì¥í•œë‹¤. high temperature (T = 1)ì¸ ìƒíƒœì—ì„œ sample(N=10)ì„ modelë¡œë¶€í„° ìƒì„±í•˜ì—¬ $H_SE(x)$ë¥¼ ì‚°ì¶œí•œë‹¤. 

ìœ„ì™€ ê°™ì´ ìˆ˜ì§‘ëœ datasetì€ logistic regression classifierë¥¼ í•™ìŠµí•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤. Semantic entropy scoreëŠ” real numberì¸ë° ì–´ë–»ê²Œ classifier í•™ìŠµì— ì‚¬ìš©ë ê¹Œ? binary ê³¼ì •ì„ ê±°ì¹œ í›„ì— í•™ìŠµì— ì‚¬ìš©ëœë‹¤. real numberì¸ entropy scoreë¥¼ ì´ì§„í™”í•˜ê¸° ìœ„í•´ threshold $\gamma$ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë†’ì€ entropy, ë‚®ì€ entropyë¡œ ë¶„ë¥˜í•˜ì˜€ë‹¤. 

### 2.2 Probing Locations

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” LLMì˜ ì–´ëŠ layer($l$)ì—ì„œ ì–´ëŠ token position $p$ì˜ hidden stateê°€ semantics entropyë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ”ì§€ ì—°êµ¬í–ˆë‹¤. ì´ë•Œ ë‘ ê°€ì§€ ìœ„ì¹˜ê°€ ì£¼ìš”í•˜ê²Œ ì‚¬ìš©ëœë‹¤.

- TBG (Token Before Generating): ì…ë ¥ ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ token
- SLT (Second Last Token): ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” ì‘ë‹µì—ì„œ ë§ˆì§€ë§‰ tokenì˜ ì§ì „ token

(TBGëŠ” ëª¨ë¸ì´ generationì„ ë§Œë“¤ê¸° ì „ì— ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê³„ì‚°ë¹„ìš©ì„ ë” ì ˆê°í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.)

## 3. Experiment

### 3.1 Task

- short-form task:
  - ëª¨ë¸ì—ê²Œ ê°€ëŠ¥í•œ í•œ ì§§ê²Œ ë‹µë³€í•˜ë„ë¡ ìš”ì²­í•œë‹¤.
  - ìƒì„± ëª¨ë¸ë¡œëŠ” Llama-2 7B, 70B, Mistral 7B, Phi-3 Minië¥¼ ì‚¬ìš©í•˜ê³  DeBERTaLargeë¡œ entailment(í•¨ì˜)ë¥¼ ì˜ˆì¸¡í•œë‹¤.
- long-form task:
  - ì™„ì „í•œ ë¬¸ì¥ì„ ìƒì„±í•˜ë„ë¡ ìš”ì²­í•˜ì—¬ ê¸´ ë‹µë³€ì„ ë°˜í™˜í•˜ë„ë¡ ìš”ì²­í•œë‹¤.
  - ìƒì„± ëª¨ë¸ë¡œëŠ” Llama-2-70B, Llama-3-70Bë¥¼ ì‚¬ìš©í•˜ê³  GPT-3.5ë¥¼ í†µí•´ ì˜ë¯¸ë¡ ì  ë¶ˆí™•ì‹¤ì„±ì„ ì˜ˆì¸¡í•œë‹¤.

short-form settingì˜ ê²½ìš° SQuAD F1 ìŠ¤ì½”ì–´ë¡œ ëª¨ë¸ ì •í™•ë„ë¥¼ í‰ê°€í•˜ê³ , long-form settingì˜ ê²½ìš° GPT-4ë¥¼ ì‚¬ìš©í•´ entailment(í•¨ì˜)ë¥¼ ì˜ˆì¸¡í•œë‹¤.

### 3.2 Linear Probe

- scikit-learnì˜  logistic regression modelì„ ì‚¬ìš©.
  - regularization: default hyperparameters for L2
  - optimizer: LBFGS

### 3.3 Evaluation

í‰ê°€ì—ëŠ” ë‘ ê°€ì§€ ê¸°ì¤€ì´ ì ìš©ëœë‹¤.

1. Semantic Entropyë¥¼ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€
2. model hallucinationì„ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€

ë‘ ê¸°ì¤€ ëª¨ë‘ area under the receiver operating characteristic curve (AUROC)ê°’ìœ¼ë¡œ í‰ê°€í•˜ë©°, AUROCê°€ ë†’ì„ìˆ˜ë¡ ì˜ë¯¸ì  ë¶ˆí™•ì‹¤ì„±ì„ ì˜ íŒŒì•…í•œë‹¤ëŠ” ê²ƒì´ë‹¤.

## 4.  LLM Hidden States Implicitly Capture Semantic Entropy

### 4.1 Hidden States Capture Semantic Entropy

modelì˜ hidden stateê°€ semantic entropyë¥¼ ì˜ í¬ì°©í•˜ëŠ”ê°€? 

-> ì¼ë°˜ì ìœ¼ë¡œ, ëª¨ë¸ì˜ í›„ë°˜ë¶€ ë ˆì´ì–´ì—ì„œ AUROC ê°’ì´ ì¦ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ í™•ì¸ëœë‹¤. ì´ëŠ” **ëª¨ë¸ í›„ë°˜ë¶€ layerê°€ semantic entropyë¥¼ ì˜ íŒŒì•…**í•œë‹¤ëŠ”ê²ƒì„ ì˜ë¯¸í•œë‹¤.

<center><img width="600" src="https://github.com/user-attachments/assets/38dbc1ab-af8e-43a2-aa47-4835d448e76d"></center>
<center><em style="color:gray;">paper</em></center><br>

### 4.2 Semantic Entropy Can Be Predicted Before Generating

outputì„ ìƒì„±í•˜ê¸° ì „ì— semantic entropyë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”ê°€?

-> ê²°ë¡ : ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ í‘œëŠ” TBG location, ì¦‰ outputì„ ìƒì„±í•˜ê¸° ì§ì „ ì…ë ¥ ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ tokenì„ ì²˜ë¦¬í•  ë•Œì˜ AUROC scoreì´ë‹¤. ë³´ë©´ ì „ë°˜ì ìœ¼ë¡œ ë†’ì€ ì ìˆ˜ê°€ ë‚˜ì™€ í•´ë‹¹ ì‹œì ì—ì„œë„ semantic entropyë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ í™•ì¸ëœë‹¤.

<center><img width="600" src="https://github.com/user-attachments/assets/7a0b7620-c672-40bc-bdf0-6a5183cdaace"></center>
<center><em style="color:gray;">paper</em></center><br>

### 4.3 Long-Form Generations and Layer Dynamics

long form generation taskì— ëŒ€í•´ì„œ semantic entropyë¥¼ ì–¼ë§ˆë‚˜ ì˜ í¬ì°©í•˜ëŠ”ê°€?

-> long form generation taskë¥¼ ì²˜ë¦¬í•  ë•Œ modelì˜ ì¤‘ê°„ layerì—ì„œ AUROC scoreê°€ ë†’ì•„ì§€ëŠ” ê²½í–¥ì´ ë‚˜íƒ€ë‚¬ë‹¤. ë…¼ë¬¸ì˜ ì €ìëŠ” ë§ˆì§€ë§‰ layerì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ modelì´ ì˜ë¯¸ì ì¸ ê²ƒë³´ë‹¤ ë¬¸ë²•ì , ì–´íœ˜ì  ìš”ì†Œì— ë” ê³ ë ¤í•˜ì—¬ ë‹¤ìŒ tokenì„ ì˜ˆì¸¡í•˜ê¸° ë•Œë¬¸ì´ë¼ê³  ê¸°ìˆ í•˜ì˜€ë‹¤.

### 4.4 Counterfactual Context Addition Experiment

contextê°€ modelì˜ semantic entropyì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?

-> ê²°ë¡ : ë¯¸ì¹œë‹¤. 

- context ì¶”ê°€ ì „: ë¬¸ë§¥ ì—†ì´ ì…ë ¥ì— ëŒ€í•œ outputì„ ìƒì„±í•  ë•Œ SEPì˜ semantic entropyê°€ ë§¤ìš° ë†’ì•„ì§€ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤. semantic entropyê°€ ë†’ë‹¤ëŠ” ê²ƒì€ ë‹µì„ ìƒì„±í•  ë•Œ ì˜ë¯¸ì  ë¶ˆí™•ì‹¤ì„±ì´ ë†’ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.
- context ì¶”ê°€ í›„: ë¬¸ë§¥ ì¶”ê°€ í›„ì—ëŠ” modelì˜ accuracyë„ ì¶”ê°€ ì „ì¸ 26%ë³´ë‹¤ í˜„ì €íˆ ë†’ì€ 78%ë¡œ ì¸¡ì •ë˜ì—ˆê³  ê·¸ì™€ ë™ì‹œì— SEPê°€ ì˜ˆì¸¡í•œ semantic entropyë„ ê°ì†Œí•˜ì˜€ë‹¤.

ìœ„ì™€ ê°™ì€ ê²°ê³¼ëŠ” ëª¨ë¸ì—ê²Œ ë‹µë³€ ì‹œ ë„ì›€ì´ ë˜ëŠ” ë¬¸ë§¥ì„ ì¶”ê°€í•´ì£¼ë©´ ì˜ë¯¸ì  ë¶ˆí™•ì‹¤ì„±ì´ ì¤„ì–´ë“ ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. (RAGì´ hallucination ê°ì†Œì— ì˜í–¥ì„ ì¤€ë‹¤ëŠ” ì¦ê±°ê°€ ë  ìˆ˜ ìˆê² ë‹¤.)

<center><img width="200" src="https://github.com/user-attachments/assets/7a2af6b1-7334-4d11-82e4-0fcda6202444"></center>
<center><em style="color:gray;">paper</em></center><br>


# Reference

> Detecting hallucinations in large language models using semantic entropy <br>
> [https://www.nature.com/articles/s41586-024-07421-0](https://www.nature.com/articles/s41586-024-07421-0)<br>
> [https://github.com/jlko/semantic_uncertainty](https://github.com/jlko/semantic_uncertainty)<br>
> [https://github.com/jlko/long_hallucinations](https://github.com/jlko/long_hallucinations)<br>
> Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs




