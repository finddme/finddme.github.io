---
title: "MoE, MoA  (작성 중)"
category: LLM / Multimodal
tag: Multimodal
---







* 목차
{:toc}











# MoE(Mixtures of Experts)

MoE는 모든 네트워크가 활성화되는 dense model과는 달리 "experts"라고 불리는 여러 specialized subnetwork를 사용하여 입력에 따라 관련된 expert만 활성화시켜 학습과 추론을 떠 빠르고 효율적으로 만든다.

MoE model의 핵심 요소는 아래 두 가지이다:

## core component

- **Sparse MoE Layers** : transformer architecture의 dense feed-forward network layer를 대체하는 layer이다. (dense feed-forward network layer 여러개라고 볼 수 있음.)각 MoE layer는 expert이며, 입력에 대해 이들 중 일부 expert만 활성화된다.
- **Gate Network or Router** : 이는 어떤 token이 어떤 expert에 의해 처리될지 결정하는 요소이다. 입력의 각 부분이 가장 적합한 expert에 의해 처리되도록 한다.

아래 이미지에서 왼쪽은 일반적인 GPT 계열 모델의 구조이고, 오른쪽은 GPT 계열 모델 구조에서 feedforward network가 Router + Exterts로 대체된 구조이다. 오른쪽 이미지를 보면 현재 time step에서의 token은 Router를 통해 Expert2와 Expert4이 활성화되어 이를 통과하고 있다.

> **GPT 계열의 모델들은 모두 Transformers의 Decoder 구조를 기반으로 한다. 아래 그림에서는 layer normalization 수행 시점이 Decoder 구조와는 다르게 각 block 앞에 위치해 있다. 이는 GPT3부터 변경된 구조로, 최근 나오는 많은 모델들이 이 구조를 따르고 있다.**
>> [GPT3 : Language Models are Few-Shot Learners](https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/#gpt3--language-models-are-few-shot-learners)

<center><img width="400" src="https://github.com/finddme/finddme.github.io/assets/53667002/a8ea1d45-8af2-4fe3-b064-e637a9d0bd49"></center>
<center><em style="color:gray;">Illustrated by the author</em></center><br>

하지만 위와 같이 복잡해진 구조로 인해 두 가지 문제점이 발생한다.
- MoE를 통해 모델을 병합한 후의 fine-tuning이 어렵다. tuning하는 동안 expert usage의 균형을 맞추어 적절히 학습시켜야 하는데 이것이 쉬운 일이 아니다.
- 추론 시 많은 Memory가 요구된다. 각 expert들이 포함된 모델을 memory에 올려야 하기 때문에 VRAM 용량이 많이 필요하다.

## essential parameter

- **Number of experts**:
  - `num_local_experts`
  - 해당 parameter는 expert 수를 결정하는 parameter이다. 예를 들어 Mixtral은 8개의 expert를 가지고 있다.
  - expert가 많을수록 모델 크기가 커져 memory 요구량이 많아진다.

- **Number of experts/token**

# MoA(Mixture-of-Agents)
