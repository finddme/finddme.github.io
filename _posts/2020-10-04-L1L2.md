---
title: L1, L2 (Norm, Loss, Regularization)
category: Machine Learning, Deep Learning
tag: Machine Learning, Deep Learning
---

## 1. Norm

Norm이란, vector의 크기(magnitude) 혹은 길이(length)를 측정하는 방법(함수)이다:

\begin{matrix}
Norm=\lVert x \rVert_p=(\sum^{n}_ {i=1}{\left\vert x_i \right\vert}^p)^{1/p}
\end{matrix}

vector의 길이를 구하는 방식으로는 L1 norm, L2 norm 그리고 Max norm이 있다.

### 1.1 L1 norm

L1 norm은 Manhattan norm혹은 Taxicab norm이라고도 불린다. L1 norm은 위 공식에서 $p$값이 1인 것으로, $x$값은 모두 절댓값을 취하도록 하고 모두 더해주면 된다:

\begin{matrix}
L_1 norm=\lVert x \rVert_{1}=\sum^{n}_ {i=1}{\left\vert x_i \right\vert}
\end{matrix}

예를 들어

<img width="93" alt="2021-03-20 (1)" src="https://user-images.githubusercontent.com/53667002/111892760-95765680-8a41-11eb-8e54-0e35ec6baaea.png">

위와 같은 vector가 있을 때 L1 norm은 $\lVert x \rVert_{1}=3+-1+5+2=11$이다.

L1 norm은 L1 Regularization이나 Computer Vision에서 많이 사용된다.

### 1.2 L2 norm

L2 norm은 $p$가 2일 경우이다. L2 norm은 Euclidean Distance로도 불리는데, 말 그대로 n차원의 유클리드 공간(좌표평면)에서의 vector 크기를 직선으로 계산하는 함수이다, 즉 원점에서 vector까지의 직선거리(최단거리)를 계산하는 함수이다(피타고라스와 같다):

\begin{matrix}
L_2 norm=\lVert x \rVert_{2}=(\sum^{n}_ {i=1}{\left\vert x_i \right\vert}^2)^{1/2}=\sqrt{\sum^{n}_ {i=1}{\left\vert x_i \right\vert}}
\end{matrix}

L1과 L2 norm을 그림으로 표현하자면 아래와 같다. 아래 그림에서 초록색이 L2이고 나머지가 L1이다.

<center><img width="205" alt="2021-03-20 (2)" src="https://user-images.githubusercontent.com/53667002/111892868-6d3b2780-8a42-11eb-94a9-3bb203278577.png"></center>

## 1.3 Max norm(supremum norm)

Max norm은 $p=\infty$인 경우로, vector성분의 절댓값 중 가장 큰 값을 구한다.

\begin{matrix}
Max\,norm=\lVert x \rVert_{\infty}=\lim_{n \to \infty}(\sum^{n}_ {i=1}{\left\vert x_i \right\vert}^p)^{1/p}=max\{\left\vert x_1 \right\vert, \left\vert x_2 \right\vert, ..., \left\vert x_n \right\vert\}
\end{matrix}

## 2. Loss

loss(손실값)를 통해 weight parameter가 적절한 feature를 잘 찾고 있는지 확인할 수 있다. 모델은 loss를 최소화하는 방향으로 학습되어야 한다.

### 2.1 L1 Loss
L1 norm을 기반으로 만들어진 L1 loss는 LAD(Least Absolute Deviations), LAE(Least Absolute Errors), LAR(Least Absolute Residual)등으로 불린다. L1 loss는 실제 target 값($y_i$)과 예측 target 값($f(x_i)$)의 오차에 절댓값을 취하여 오차의 합을 최소화하는 방향으로 loss를 구한다.

\begin{matrix}
L_1 loss=\sum^{n}_ {i=1}{\left\vert y_i-f(x_i) \right\vert}
\end{matrix}

### 2.2 L2 Loss

L2 loss는 LSE(Least Squares Error)로도 불린다. L1 loss와 다른 점은 실제 target 값과 예측 target값의 오차를 제곱한 값들의 합으로 loss를 구한다.

\begin{matrix}
L_2 loss=\sum^{n}_ {i=1}{(y_i-f(x_i))}^2
\end{matrix}

### 2.3 L1, L2 loss차이

(1) Robustness  
Robustness는 loss function이 noise나 outlier(이상치)에 얼마나 영향을 받는지에 대한 것이다. 즉, 이상한 데이터가 입력되었을 때 얼마나 견고함을 유지할 수 있는지에 대한 것이다. 위 수식에서 알 수 있듯이 noise가 들어왔을 때 L2 loss에서는 noise값도 제곱하여 계산되기 때문에 단순히 절댓값을 씌우는 L1보다 outlier의 영향이 커진다. 따라서 L2가 L1보다 Robustness에 대해 취약한 반면 이를 이용하여 이상치를 발견해야 하는 경우에 유용하게 사용될 수 있다.

(2) Stability  
Stability는 모델의 안정성에 관한 것이다. 안정적인 모델일수록 일관된 결과값을 낸다. L1보다는 L2가 안정적이다. 이에 따라 L1은 다양한 결과를 낼 수 있고, L2는 일관적인 결과를 낼 수 있다. 필요에 따라 골라서 사용하면 된다.

## 3. Regularization
Regularization(정규화, 일반화)은 말 그대로 모델의 overfitting을 줄여 어떠한 데이터에도 일반화된 예측을 할 수 있도록 하는 것이다. 즉, 모델을 일반화시키는 것이다. Regulariztion 에는 Data augmentation, Dropout, Batch Normalization, Model ensemble 등 여러가지 방법들이 있는데, L1과 L2 Regularization을 그 중 매우 low한 개념에 속한다. L1, L2 Regularization모두 L1, L2 loss에 Regularization Term(정규화 요소)을 붙여 모델이 overfit되지 않도록 패널티를 주는 것이다.

L1 Regularization은 불필요한 feature에 대한 weight를 0으로 만들어 해당 weight가 힘을 가질 수 없도록 만든다. 반면 L2 Regularization은 불필요한 feature에 대한 weight를 0에 가깝게 만들어 linear model들의 성능 개선시키는 것에 도움을 준다.

